\section{Supplemental Information}
\label{supp_sec:info}

\subsection{Cre/structure combinations in $\mathcal D$}
\label{supp_sec:data}

This section describes the abundances of leaf and cre line combinations in our dataset.
Users of the connectivity matrices who are interested in a particular cre line or structure can see the quantity and type of data used to compute and evaluate that connectivity.

\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/CB centroid densityoct12.png}
    \label{fig:my_label}
\end{figure}
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/TH centroid densityoct12.png}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/CTXsp centroid densityoct12.png}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/P centroid densityoct12.png}
    \label{fig:my_label}
\end{figure}
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/PAL centroid densityoct12.png} 
    \label{fig:my_label}
\end{figure}
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/Isocortex centroid densityoct12.png}
    \label{fig:iso_count}
\end{figure}
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/MB centroid densityoct12.png} 
    \label{fig:my_label}
\end{figure}
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/MY centroid densityoct12.png} 
    \label{fig:my_label}
\end{figure}
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/HPF centroid densityoct12.png} 
    \label{fig:my_label}
\end{figure}
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/OLF centroid densityoct12.png} 
    \label{fig:my_label}
\end{figure}
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/STR centroid densityoct12.png} 
    \label{fig:my_label}
\end{figure}
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{figs/HY centroid densityoct12.png} 
    \label{fig:my_label}
\end{figure}

\newpage

\subsection{Distances between structures}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../analyses/paper/figures/distances_leafs.png} 
    \caption{Distance between structures.  Short-range connections are masked in red}
    \label{fig:dist_bw_str}
\end{figure}

\subsection{Model evaluation}
\label{supp_sec:model-evaluation}

\begin{table}[H]
\small
\begin{tabular}{lrr}
\toprule
{} &  Total &  Cre-Leaf \\
\midrule
Isocortex &     36 &         4 \\
OLF       &      7 &         2 \\
HPF       &    122 &        62 \\
CTXsp     &     85 &        41 \\
STR       &   1128 &       732 \\
PAL       &     68 &        18 \\
TH        &     46 &         7 \\
HY        &     35 &        17 \\
MB        &     33 &         8 \\
P         &     30 &        11 \\
MY        &     78 &        45 \\
CB        &     83 &        29 \\
\bottomrule
\end{tabular}
\caption{Number of experiments available to evaluate models in leave-one-out cross validation. 
Models that rely on a finer granularity of modeling have less data available to validate with.} 
\label{tab:eval_size}
\end{table}

\newpage 
\section{Supplemental methods}
\label{supp_sec:methods}

This section consists of additional information on preprocessing of the neural connectivity data, estimation of connectivity, and matrix factorization.

\subsection{Data preprocessing}
\label{supp_sec:dp}

Several data prepreprocessing steps take place prior to evaluations of the connectivity matrices.
These steps are described in Algorithm \ref{alg:preprocess}.
The arguments of this normalization process - injection signals $x(i)$, projection signals $y(i)$, injection fraction $F(i)$, and data quality mask $q(i)$ - were downloaded using the Allen SDK. %and injection fraction $F(i)$ 
The injections and projection signals $\in \mathcal B \times \mathbb [0,1]$ were segmented manually in histological analysis.
The projection signal gives the proportion of pixels within the voxel displaying fluorescence, and the injection signal gives the proportion of pixels within the histologically-selected injection subset displaying fluorescence.
The injection fraction $\in \mathcal B \times \mathbb [0,1]$ gives the proportion of pixels within each voxel in the injection subset.
Finally, the data quality mask $\in \mathcal B \times \mathbb \{0,1\}$ gives the voxels that have valid data.

%We also have a map $A: \mathbb R \to \mathbb R^{|S|}$ where $S$ is the number of structures that takes the average value for voxels in that structure
Our preprocessing makes use of the above ingredients, as well as several other essential steps.
First, we compute the weighted injection centroid
\begin{eqnarray*}
c(i) &= \sum_{l \in \mathcal B} x(i) l(v) 
\end{eqnarray*}
Given a regionalization $\mathcal R$, we also have access to a regionalization map as $R: \mathcal B  \to \mathcal R $ which induces a map of connectivities 
\begin{eqnarray*}
R_*: \mathcal F &\to \mathcal R \times \mathbb R^+ \\
(v, y) &\mapsto \sum_{v' \in R} y'  \text{ for } (v,y') \text{ s.t. } R \ni v.
\end{eqnarray*}
This map depends on the choice of regionalization; we regionalize at the leaf level.
We also can restrict a signal to a individual structure
\begin{eqnarray*}
S_* :  \mathcal F &\to  \mathcal F \\
 (v, y) &= \begin{cases} 
 (v, y) \text{ if } v \in S \\
 (v, 0) \text{ otherwise }
 \end{cases}
\end{eqnarray*}
Finally, given a vector or array $a$, we have the $L1$ normalization map
\begin{eqnarray*}
n: a &\mapsto \frac{a}{\sum_{j = 1}^p a_j}
\end{eqnarray*}

\begin{algorithm}[H]
\floatname{algorithm}{\textsc{Preprocess}}%{\preprocess}
%\renewcommand{\thealgorithm}{}
\begin{algorithmic}
\caption{{\bf Input} Injection $x(i)$, Projection $y(i)$, Injection centroid $c(i) \in \mathbb R^3$, injection fraction $F(i)$, data quality mask $q(i)$}
\State Injection fraction $x_F(i) \gets x(i) \odot F(i)$
\State Data-quality censor $y_M (i) \gets  \odot y(i) \odot q(i) , x_M(i) \gets x_F(i) \odot F(i)$
\State Restrict injection $x_M(i)  \odot S(i)$.
\State Compute centroid $c(i)$ from $x_M(i) $
\State Regionalize $y_S (i) \gets R_*(  y_M(i))$
\State Normalize $\tilde y(i) \gets n(Y_S(i))$
 \State {\bf Output}$\tilde y(i)$, $c(i)$ )  
\end{algorithmic}
\label{alg:preprocess}
\end{algorithm}



%The data-quality censor is established by \skcomment{fill}
%We find the $l2$ norm not appropriate in a few ways 1) there is not a clear inner product structure on the connectome.
%doesn't matter if we normalize prior to regionalization as long as we don't correct for density of the target region.
%which, due to normalization, can be rewritten as simply the $l2$ loss $\|y - \hat y\|.$  Note that, due to normalization, $\|y - \hat y\| = \langle y - \hat y , y - \hat y \rangle = 2 - 2 \langle y, \hat y\rangle $, and, applying normalization again, $ \langle y, \hat y\rangle $ is the vector cosine of $\hat y$ and $y$.

\newpage
\subsection{Estimators}
\label{supp_sec:estimators}
Our estimators model a connectivity vector $f (v,s)  \in \mathbb R_{\geq}^T$, and so we may write
\begin{eqnarray*}
f (v,s,t) = f (v,t)[t].
\end{eqnarray*}
Thus, for the remainder of this section, we will discuss only $f (v,s)$.

\subsubsection{Centroid-based Nadaraya-Watson}

In the Nadaraya-Watson approach of \citet{Knox2019-ot}, the injection is considered only through its centroid $c(i) := c(x(i))$, and the projection is considered regionalized.
%The prediction for a given region is then given by the integral of predictions over that region, which is computed as a sum over voxels.
That is,
\begin{eqnarray*}
f_*({\mathcal D}_i) = \{c(i), y_{\mathcal T}(i)\}.
\end{eqnarray*}
Since the injection is considered only by its centroid, this model only generates predictions for particular locations $c$, and the prediction for a structure $s$ is given by integrating over locations within the structure
\begin{eqnarray*}
\label{eq:regionalize}
f^* (\hat f (f_*(\mathcal D))) (v,s) = \sum_{l_{s_j} \in s} \hat f (f_*(\mathcal D)) (v,l_{s_j}),
\end{eqnarray*}
This $\hat f$ is the Nadaraya-Watson estimator
\begin{eqnarray*}
\hat f_{NW}( c(I) , y_{\mathcal T}(I) ) (l) :=  \sum_{i \in I} \frac{ \omega_{c(i) l}}{\sum_{i \in I} \omega_{c(i), l}} y_{\mathcal T}(i)
\end{eqnarray*}
where $\omega_{c(i) l } = \exp( - \gamma d( l , c(i))^2 )$ and $d$ is the Euclidean distance between centroid $c(i)$ and voxel with position $l$.

Several facets of the estimator are visible here. %$f_{NW}^{\gamma}$ is the Nadaraya-Watson estimator with smoothing given by inverse-bandwidth $\gamma$.
A smaller $\gamma$ corresponds to a greater amount of smoothing, and index set $I \subseteq  \{1:n\}$ indicates which experiments to use to generate the prediction.
Fitting $\gamma$ via empirical risk minimization therefore bridges between $1$-nearest neighbor prediction and averaging of all experiments in $I$.
In \citet{Knox2019-ot}, $I$ consisted of experiments sharing the same brain division.
Restricting of index set to only include experiments with the same neuron class gives the class-specific Cre-NW model.

\newpage
\subsubsection{The expected-loss estimator}

\label{supp_sec:el}

The response induced by each of the Cre-lines is effected by both the injection location and the targeted cell types.
Since Cre-lines that target similar cell classes are therefore expected to induce similar projections, and including similar Cre-lines in the Nadaraya-Watson estimator increases the effective sample size, we introduce an estimator that assigns a predictive weight to each training point that depends both on its centroid-distance and Cre-line.
This weight is determined by the expected prediction error of each of the two feature types, as determined by cross-validation.
These weights are then utilized in a Nadaraya-Watson estimator in a final prediction step.
Estimating $\hat f(v, c)$ shares the advantage of fine-scale spatial resolution with \citet{Knox2019-ot}, but in addition enables us to model a particular cell-class $v$.

We formalize Cre-line behavior as the average regionalized projection of a Cre-line in a given structure (i.e. leaf).
This vectorization of categorical information is known as \textbf{target encoding}.
We define a \textbf{Cre-distance} in a leaf to be the distance between the target-encoded projections of two Cre-lines.
The relative predictive accuracy of Cre-distance and centroid distance is determined by fitting a surface of projection distance as a function of Cre-distance and centroid distance. 

In mathematical terms, our full feature set consists of the centroid coordinates and the target-encoded means of the combinations of virus type and injection-centroid structure.
That is, 
\begin{eqnarray*}
f_*({\mathcal D}_i) = \{c(i) , \bar y_{\mathcal T} ( {I_v} \cap I_s), y_{\mathcal T}(i) \}.
\end{eqnarray*}
$f^*$ is defined as in \eqref{eq:regionalize}.
The expected loss estimator is then 
\begin{eqnarray*}
\hat f_{EL} (c, c(i),v, y_{\mathcal T} (I_v \cap I_s)) =  \sum_{i \in I} \frac{ \nu {(c(i) , c, v(i), v)}}{\sum_{i \in I} \nu {(c(x_i) , c, v_i, v) }} r(y_i)
\end{eqnarray*}
where
\begin{eqnarray*}
\nu_i = \exp (- \gamma g( d(c, c(x_i))^2, d(\bar r (v), \bar r (v_i))^2))
\end{eqnarray*}
Note that $g$ must be a concave, non-decreasing function of its arguments with with $g(0,0) = 0$, then $g$ defines a metric on the product of the metric spaces defined by experiment centroid and target-encoded cre-line, and $\hat f_{EL}$ is a Nadaraya-Watson estimator. 
A derivation of this fact is given in Appendix \ref{supp_sec:el}, and we therefore use shape-constrained B-splines to estimate $g$.

%This contrasts with the models in \citet{Knox2019-ot} and \citet{Oh2014-kh}, where connectivity was directly estimated by $\hat f$ a function of $S$ without an integral.
%Since we wish to weight similar points more highly, we weight distances in cre-space and centroid-space by how well they predict the response variable.
%Due to the fact that the cre-space is defined with respect to the response variable, points are removed from their cre-mean prior to cross-validation.
%Figure shows an example of such an estimated $g$.

%In the low-sample size setting, it is worth utilizing points from similar groups.

\begin{algorithm}[H]
\floatname{algorithm}{\textsc{EL}}%{\preprocess}
%\renewcommand{\thealgorithm}{}
\begin{algorithmic}
\caption{{\bf Input} Projection $y_{\mathcal T}(I_s)$, Injection centroids $c(I_s) \in \mathbb R^3$, Cell-classes $v(I_s),$ $g$, location $l$, cell-class $v$}
\State Get structures $s(1:n) = r(c(1:n))$, $s = r(c)$
\State Target encode $v(1:n)$ and $v$ with $n(r(y(1:n)))$
\State 
%\State (Optional Cross-validation) Estimate expected losses $ \hat f (\|l - c(i')\|_2^2, \|\bar y_{\mathcal T}(v, s\setminus i) - \bar y_{\mathcal T}(v, s)\|_2^2)$
\State Estimate expected losses $X = [g (\|l - c(i')\|_2^2, \|\bar y_{\mathcal T}(v, s) - \bar y_{\mathcal T}(v(i'), s)\|_2^2) : i' \in ( I_s )]$
\State Predict $\widehat y_{\mathcal T} = NW(X, y_{\mathcal T}(I_s)$
 \State {\bf Output}$\tilde y(i)$, $c(i)$ )  
\end{algorithmic}
\label{alg:preprocess}
\end{algorithm}

\newpage

%\begin{algorithmic}
%\label{alg:model}
%\begin{algorithm}{Injection centroids $c(1:n)$, normalized projections $n(r(y(1:n)))$, viruses $v(1:n)$. target centroid $c$, target virus $v$}
%\STATE Get structures $s(1:n) = r(c(1:n))$, $s = r(c)$
%\STATE Target encode $v(1:n)$ and $v$ with $n(r(y(1:n)))$
%\STATE Estimate expected loss $l_{ii'} = \hat f (\|c(i) - c(i')\|_2^2, \|t(v(i)) - t(v(i'))\|_2^2)$
% \RETURN $\tilde y(i)$ (optional $\tilde x(i)$ )
%\end{algorithm}
%\end{algorithmic}


\begin{figure}[H]
%\begin{adjustbox}{width=.75\columnwidth,center}
\begin{comment}
\subfloat[]{
\begin{tikzpicture}[
        execute at begin picture={
          \useasboundingbox (-4,-1) rectangle (4,1);
         }
         ,
  node distance = 8mm and 1mm,
job/.style args = {#1/#2}{
    rectangle, rounded corners, draw=black, very thick,
    minimum height=3em, text width=14em, align=center,
    label={[font=\sffamily,anchor=north]above:\textit{#1}},
    label={[font=\sffamily,anchor=south]below:\textbf{#2}},
    node contents ={}},
    arr/.style = {semithick, -Stealth}
                    ]
\node (n1)  [job=Predict ${\hat y_r \in \mathbb R_{\geq 0}^{|T|}}$/${c, v, s(c)}$];
%\node (n21) [job=Voxel /$c$,below left=of n1.south];
%\node (n22) [job= Target encode discrete covariates/ ${\mu_{s,v}} $,below right=of n1.south];
%\node (n31) [job=Get centroid distances/${d_{cen} = \{\|c(i') - c\|_2^2 \}} $,below =of n21.south];
%\node (n32) [job= Get cre distances/ ${d_{cre} =\{\|\mu_{s,v}(i) - \mu_{s,v}\|_2^2  } $,below =of n22.south];

%\node (n4)  [job=Get weights/${w  = g_M({d_{cen}, d_{cre}})}$,below right=of n31.south];
%\node (n5)  [job= Predict/{$\hat y_r = \widehat{NW}(w)$},below=of n4];
%
\coordinate[below=4mm of n1.south] (aux1);
%\coordinate[above=4mm of n4.north] (aux2);
%
%\draw[arr]  (n1) -- (aux1) -| (n21);
%\draw[arr]  (aux1) -| (n22);
%\draw[arr]  (n22) -- (n32);
%\draw[arr]  (n21) -- (n31);
%\draw[arr]  (n31) |- (aux2)
%            (n32) |- (aux2) -- (n4);
%\draw[arr]  (n4)  -- (n5);
   \end{tikzpicture}
   }
\newline
\end{comment}
\begin{comment}
\begin{tabular}[t]{cc}
\subfloat[]{
    \includegraphics[width = 2.1in]{figs/figsforpres/315_summary_scatter.png}
    \caption{Isocortex loss distribution}
    \label{fig:my_label} 
    }
    &
    \subfloat[]{
    \includegraphics[width = 2.1in]{figs/figsforpres/315_summary_surface.png}
    \caption{$\hat g$ fit to expected-loss using shape-constrained splines}
    \label{fig:my_label}
    %\end{figure}
    }
    \end{tabular}
\end{comment}
 %  \end{subfigure}
% \end{adjustbox}
 \caption{The Expected-Loss estimator}
 \end{figure}


\newpage
%\subsubsection{The expected-loss estimator}
\paragraph{Justification of shape constraint}

The shape-constrained expected-loss estimator introduced in this paper is, to our knowledge, novel.
It should be considered an alternative method to the classic weighted kernel method.
While we do not attempt a detailed theoretical study of this estimator, we do establish the need for the shape constraint in our spline estimator.
Though this fact is probably well known, we prove a (slightly stronger) version here for completeness.

Given a collection of metric spaces $X_1, ... X_n$ with metrics $d_1 ... d_n$ (e.g. $d_{centroid}, d_cre$), and a function $f: (X_1 \times X_1) ... \times (X_n  \times X_n) = g(d_1(X_1 \times X_1),... d_n(X_n \times X_n))$, then then $f$ is a metric iff $g$ is concave, non-decreasing and $g(d) = 0 \Longleftrightarrow d = 0$.

We first show $g$ satisfying the above properties implies that $f$ is a metric.
\begin{itemize}
    \item The first property of a metric is that $f(x,x') = 0 \Longleftrightarrow x = x'$.  The left implication: $x = x' \implies f(x_1, x_1', ... x_n, x_n') = g(0,....,0)$, since $d$ are metrics.  Then, since $g(0) = 0$, we have that $f(x,x') = 0$. The right implication: $f(x,x') = 0 \implies  d = 0 \implies x = x'$ since $d$ are metrics.
    \item The second property of a metric is that $f(x,x') = f(x',x)$. This follows immediately from the symmetry of the $d_i$, i.e. $f(x,x') = f(x_1, x_1', ... x_n, x_n') = g(d_1(x_1, x_1'), ... d_n(x_n, x_n')) = g(d_1(x_1', x_1), ... d_n(x_n', x_n)) =  f(x_1', x_1, ... x_n', x_n) = f(x',x)$.
    \item The third property of a metric is the triangle inequality: $f(x, x') \leq f(x, x^*) +  f(x^*, x') $.  To show this is satisfied for such a $g$, we first note that $f(x,x') = g(d(x,x')) \leq g(d(x, x^*) + d(x^*, x')) $ since g is non-decreasing and by the triangle inequality of $d$. Then, since $g$ is concave, $g(d(x, x^*) + d(x^*, x')) \leq g(d(x, x^*)) + g(d(x^*, x')) = f(x,x^*) + f(x^*, x')$.
    %or $ f(x_1, x_1', ... x_n, x_n') \leq f(x_1, x_1^*, ... x_n, x_n^*) + f(x_1', x_1^*, ... x_n', x_n^*)$.  To show this is satisfied for such a $g$, we first note $f(x_1, x_1', ... x_n, x_n') = g(d_1(x_1, x_1'),... d_n(x_n ,x_n')) \leq g(d_1(x_1, x_1*) + d_1(x_1', x_1*) ,... , d_n(x_n ,x_n*) + d_n(x_n' ,x_n*))$ since g is non-decreasing and by the triangle inequality on d. Then $g(d_1(x_1, x_1*) + d_1(x_1', x_1*) , ... , d_n(x_n , x_n*) + d_n(x_n' ,x_n*)) < f(x_1, x_1*, ... x_n, x_n*) + f(x_1', x_1*, ... x_n', x_n*)$ since $g$ is concave.
    
    %Lemma: h(a+ b) < h(a) + h(b) for a concave increasing function h.  
%Pf: h(a+b) - h(b) < h(a) - h(0) since rate of increase is decreasing.
\end{itemize}

We then show that $f$ being a metric implies that $g$ satisfies the above properties.
\begin{itemize}
    \item The first property is that $g(d) = 0 \Longleftrightarrow d = 0$. We first show the right implication: $g(d) = 0$, and $g(d) = f(x,x')$, so $x = x'$ (since $f$ is a metric), so $d = 0$. We then show the left implication: $d = 0 \implies x = x'$, since $d$ is a metric, so $f(x,x') = 0,$ since $f$ is a metric, and thus $g(d) = 0$.
    %\item 
    %$f$ is a metric, so $f(x,x') = 0 \Longleftrightarrow x = x'$.  Then, since $f(x,x') = g(d (x,x') )$ which $\implies g(0,\dotsc 0) = 0$ since $g$ is increasing.
    \item The second property is that $g$ is non-decreasing. We proceed by contradiction.
    Suppose g is decreasing in argument $d_1$ in some region $[l, u]$ with $0 < l< u$.
    Then $g(d_1(0, l), 0) \geq g(d_1(0, 0), 0) + g(d_1(0, u), 0) = g(d_1(0, u),0)$, which violates the triangle inequality on f. Thus, decreasing $g$ means that $f$ is not a metric, so $f$ a metric implies non-decreasing $g$.
    \item The final property is that $g$ is concave. We proceed by contradiction. Suppose $g$ is strictly convex. Then there exist vectors $d, d'$ such that $g(d + d')  < g(d) + g(d')$.  Assume that $d$ and $d'$ only are non-zero in the first position, and $d = d(0, x), d' = d(0,x')$.  Then, $f(0,x) + f(0,x') <  f(0,x+ x')$, which violates the triangle inequality on $f$.  Therefore, $g$ must be concave.
\end{itemize}

\newpage

\subsection{Establishing a lower detection limit}
\label{supp_sec:methods_lower}

The lower detection limit of our approach is a complicated consequence of our experimental and analytical protocols.
For example, the Nadaraya-Watson estimator is likely to generate many small false positive connections, since the projection of even a single experiment within the source region to a target will cause a non-zero connectivity in the Nadaraya-Watson weighted average.
On the other hand, the complexities of the experimental protocol itself and the image analysis and alignment can also cause spurious signals.
Therefore, it is of interest to establish a lower-detection threshold below which we have very little power-to-predict, and set estimated connectivities below this threshold to zero.
This should make our estimated connectivities more accurate, especially in the biologically-important sense of sparsity.

We establish this limit with respect to the sum of Type 1 and Type 2 errors
\begin{eqnarray*}
\iota = \sum_{i \in \mathcal E} 1_{y_{\mathcal T}(i) = 0}^T 1_{\hat f(v(i),c(i)) > \tau} + 1_{y_{\mathcal T}(i) > 0}^T 1_{\hat f(v(i),c(i)) < \tau}  .
\end{eqnarray*}

We then select the $\tau$ that minimizes $\iota$.
Results for this approach are given in Supplemental Section \ref{supp:exp_lower}.

\newpage

\subsection{Decomposing the connectivity matrix}
\label{supp_sec:matrix_factor_methods}

We utilize non-negative matrix factorization (NMF) to analyze the principal signals in our connectivity matrix.
Here, we review this approach as applied to decomposition of the distal elements of the estimated connectivity matrix $\widehat {\mathcal C}$ to identify $q$ connectivity archetypes.
Aside from the NMF program itself, the key elements are selection of the number of archetypes $q$ and stabilization of the tendency of NMF to give random results over different initialization. 

\subsubsection{Non-negative matrix factorization}

Given a matrix $X \in \mathbb R_{\geq 0}^{a \times b}$ and a desired latent space dimension $q$, the non-negative matrix factorization is
\begin{eqnarray*}
\textsc{NMF}(\mathcal V, \lambda, q) = \arg \min_{W, H} \frac{1}{2}\| 1_{M} \odot \mathcal C - WH\|_2^2  + \lambda  (\|H \|_1 + \|W \|_1) .
\end{eqnarray*}
%NMF creates a useful decomposition since $X$ is in the positive orthant, and PCA cannot not apply.
%There is no orthogonality without sparsity.
We note the existence of NMF with alternative norms for certain marginal distributions, but leave utilization of this approach for future work \citep{Brunet2004-gi}.

The mask $1_M \in \{0,1\}^{S \times T}$ serves two purposes.
First, it enables computation of the NMF objective while excluding self and nearby connections.
These connections are both strong and linearly independent, and so would unduly influence the $NMF$ reconstruction error over more biologically interesting or cell-type dependent long-range connections.
Second, it enables cross-validation based selection of the number of retained components.

\subsubsection{Cross-validating NMF}

Cross-validation for NMF is somewhat standard but not entirely well-known, and so we review it here.
In summary, a NMF model is first fit on a reduced data set, and an evaluation set is held out.
After random masking of the evaluation set, the loss of the learned model is then evaluated on the basis of successful reconstruction of the held-out values.
This procedure is performed repeatedly, with replicates of random masks at each tested dimensionality $q$.
This determines the point past which additional hidden units provide no reconstructive value.

The differentiating feature of cross-validation for $NMF$ compared with supervised learning is the random masking of the matrix $\mathcal C$.
Cross-validation for supervised learning generally leaves out entire observations, but this is insufficient for our situation.
%where $e(\mathcal C)$ is a map that encodes $\mathcal C$ in a learned representation, and $d$ is the decoding reconstruction map.
This is because, given $W$, our $H$ is the solution of a regularized non-negative least squares optimization problem
\begin{align*}
H := e_W(X) = \arg \min_{\beta \in \mathbb R_{\geq 0}^{q \times T}} \|X - W \beta\|_2^2 + \|\beta\|_1.
\end{align*}
The negative effects of an overfit model can therefore be optimized away from on the evaluation set.

The standard solution is to generate uniformly random masks $1_{M(p)} \in \mathbb R^{S \times T}$ where
\begin{align*}
1_{M(p)} (s,t) \sim \text{Bernoulli(p)}.
\end{align*}
Our cross-validation error is then
\begin{align*}
\epsilon_q &= \frac{1}{R} \sum_{r = 1}^R (\|1_{M(p)_r^C} \odot X - \widehat d_q(\widehat e_q (1_{M(p)_r^C} \odot X ))\|_2^2 
\end{align*}
where
\begin{align*}
\widehat d_q, \widehat e_q &= \widehat{\text{ NMF}}(1_{M(p)_r} \odot X, q)).
\end{align*}
Here, $1_{M(p)_r}^C$ is the binary complement of $1_{M(p)_r}$.

Theoretically, the optimum number of components is then
\begin{align*}
    \widehat q = \arg \min_q \epsilon_q.
\end{align*}
However, the low decrease in error at higher values of $q$ will motivate us to empirically select a slightly smaller number of components.

\subsubsection{Stabilizing NMF}

The NMF program is non-convex, and, empirically, individual replicates will not converge to the same optima.
One solution therefore is to run multiple replicates of the NMF algorithm and cluster the resulting vectors.
This approach raises the questions of how many clusters to use, and how to deal with stochasticity in the clustering algorithm itself.
We address this issue through the notion of clustering stability \citep{Von_Luxburg2010-lu}.

The clustering stability approach is to generate $L$ replicas of k-cluster partitions $\{C_{kl} : l \in 1 \dots L\}$ and then compute the average dissimilarity between clusterings
\begin{align*}
\xi_k &= \frac{2}{L(L - 1)} \sum_{l = 1}^{L} \sum_{l'= 1}^{l}  d(C_{kl}, C_{kl'}).
\end{align*}
Then, the optimum number of clusters is 
\begin{align*}
\hat k &= \arg \min_k \xi_k.
\end{align*}
A review of this approach is found in \citet{Von_Luxburg2010-qe}.
Intuitively, archetype vectors that cluster together frequently over clustering replicates indicate the presence of a stable clustering.
For $d$, we utilize the adjusted Rand Index - a simple dissimilarity measure between clusterings.
Note that we expect to select slightly more than the $q$ components suggested by cross-validation, since archetype vectors which appear in one NMF replicate generally should appear in others.
We then select the $q$ clusters with the most archetype vectors - the most stable NMF results - and take the median of each cluster to create a sparse representative archetype \citet{Wu2016-gg, Kotliar2019-yj}.
Experimental results for these cross-validation and stability selection approaches are given in Supplemental Section \ref{supp_sec:matrix_factor_results}.

\newpage

\section{Supplemental Experiments}
\label{supp_sec:exp}

\subsection{Establishing a lower limit of detection}
\label{supp:exp_lower}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../figures/Threshold}
    \label{fig:threshold}
    \caption{$\tau$ at different limits of detection.}
\end{figure}

\newpage

\subsection{Loss subsets}
\label{supp_sec:loss_subsets}

We report model accuracies for our $EL$ model by neuron class and structure.
These expand upon the results in Table \ref{tab:crossvalidation} and give more specific information about the quality of our estimates. 

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../paper/figures/lossdetails_0618_512.png} 
    \label{fig:distances}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../paper/figures/lossdetails_0618_703.png} 
    \label{fig:distances}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../paper/figures/lossdetails_0618_1089.png} 
    \label{fig:distances}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../paper/figures/lossdetails_0618_1097.png} 
    \label{fig:distances}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../paper/figures/lossdetails_0618_315.png} 
    \label{fig:distances}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../paper/figures/lossdetails_0618_313.png} 
    \label{fig:distances}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../paper/figures/lossdetails_0618_354.png} 
    \label{fig:distances}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../paper/figures/lossdetails_0618_698.png} 
    \label{fig:distances}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../paper/figures/lossdetails_0618_771.png} 
    \label{fig:distances}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../paper/figures/lossdetails_0618_477.png} 
    \label{fig:distances}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width = 7in]{../../paper/figures/lossdetails_0618_549.png} 
    \label{fig:distances}
    \caption{}
\end{figure}

\newpage

\subsection{Matrix Factorization}
\label{supp_sec:matrix_factor_results}

We give additional results on the generation of the archetypal connectome patterns.
These consist of cross-validation selection of $q$, the number of latent components, stability analysis, and visualization of the reconstructed wild-type connectivity.

\subsubsection{Cross-validation}

We set $\alpha = 0.002$ and run Program \ref{eq:nmf} on $\mathcal C_{wt}$.
We use a random mask with $p = .3$ to evaluate prediction accuracy of models trained on the unmasked data on the masked data.
To account for stochasticity in the NMF algorithm, we run $R = 8$ replicates at each potential dimension $q$.
This selects $\hat q = 60$.
\skcomment{Can run longer experient to show larger elbow. Note that training error also increases at high $q$ due to difficulty training model}.

\begin{figure}[H]
    \centering
    \includegraphics[width = 5in]{figs/test_train_0603.png} 
    \label{fig:train_test}
    \caption{Train and test error using NMF decomposition.}
\end{figure}

\newpage

\subsubsection{Stability}

For the purposes of visualization and interpretability, we restrict to a $q = 15$ component model.
To address the instability of the NMF algorithm in identifying components, we $k-means$ cluster components over $R = 10$ replicates with $k \in \{10,15,20, 25, 30\}$.
Since the clustering is itself unstable, we repeat the clustering $25$ times and select the $k$ with the largest Rand index.

\begin{table}[H]
\begin{tabular}{lrrrrr}
\toprule
{} &          0 &          1 &          2 &         3 &          4 \\
\midrule
q          &  10 &  15 &  20 &  25 &  30 \\
Rand index &   0.685081 &   0.789262 &   0.921578 &   \textbf{0.94548} &   0.914799 \\
\bottomrule
\end{tabular}
\end{table}

Since $k$-means is most stable at $k=25$, we cluster the $qR = 150$ components into $25$ clusters and select the $15$ clusters appearing in the most replicates.
\begin{figure}[H]
    \centering
    \includegraphics[width = 5in]{figs/nmfcluster_0620.png} 
    \label{fig:distances}
    \caption{Stability of NMF results across replicates. 
    Replicate and NMF component are shown on rows.
    Components that are in the top $15$ are also indicated.}
\end{figure}
These are the components whose medians are plotted in Figure \ref{fig:H}.


\newpage

\subsubsection{Reconstructed connectivity from archetypes}
As a simple heuristic validation of our archetypes, we plot the reconstructed wild-type connectivity.

\begin{figure}[H]
    \centering
    \includegraphics[width = 5in]{../../analyses/paper/figures/conn_leafs_recon_0617.png} 
    \label{fig:distances}
    \caption{Reconstruction of $\mathcal C$ from $H$ and $W$ with $q=15$ in Figure \ref{nmf_results}.}
\end{figure}


\newpage

\section{Glossary of symbols}

\begin{table}
\begin{tabular}{c|c}
Symbol & Meaning \\
$q$ & Number of components of latent space \\
$\mathcal S$ & Set of source structures \\
$\mathcal T$ & Set of target structures \\
$S$ & $|\mathcal S|$ \\ 
$T$ & $|\mathcal T|$ \\ 
$\mathcal C$ & Connectivity \\
$R$ & Number of replicates \\
$r$ & A replicate index \\
$\mathcal R$ & Set of regions
\end{tabular}
\end{table}






%Recall that in supervised learning
%Recall that in a supervised learning model $y \sim f(x)$.
%Standard cross-validation removes elements of $X$, fits $f$, and then uses the $f$ learned from part of the data to predict $Y$.
%A good $f$ will have low error on the training data, and also low error on the test data, indicating that it has not overfit.
%Although there is no assumed dichotomy between $X$ and $Y$ in unsupervised learning, for techniques like autoencoders, the above paradigm still applies, i.e., one can still hold out values of $X$.
%We can then estimate 
%\begin{align*}
%\arg \min_{d,e} \widehat E(l(X, d_{X^C}(e_{X^C}(X)))) = \sum_{r=  1}^R l(X_r, d_{X^{C_r}}(e_{X^{C_r}}(X_r)))
%\end{align*}
%over $R$ random samples of rows of $X$.
%However, i
%http://alexhwilliams.info/itsneuronalblog/2018/02/26/crossval/



%\comment{ 
%The limit-of-detection problem is common in a variety of scientific fields, but statistical methodology is not 
%We utilize the zero-inflated adjusted Kendall-$\tau$ statistic

%\begin{align*}
%    \tau_0 = p_{11}^2 \tau_11 + 2(p_{00} p_{11} - p_{10} p_{01}),
%\end{align*}
%where \citep{Pimentel2009KendallsTA, Albasi2018-iv}
%}

%\section{Data}\label{sec:supp_structure_cre}

%\begin{figure}
%    \centering
%    \includegraphics[width = 7in]{figs/CB centroid densityoct12.png}
%    \caption{Caption}
%    \label{fig:my_label}
%\end{figure}



%\begin{table}[]
%    \centering
%    \begin{tabular}{c|c}
%         &  \\
%         & 
%    \end{tabular}
%    \caption{Caption}
%    \label{tab:cross-validation}
%\end{table}

%However, for alternate models, such as the major-structure divided model from \citet{Knox2019-ot}, the potential evaluation set is larger. In order to compare between methods, we therefore restrict to the smallest set of evaluation indices, which is to say, virus-leaf combinations that are present at least twice.  This means that in some cases, our training set exceeds our evaluation set in size. 
