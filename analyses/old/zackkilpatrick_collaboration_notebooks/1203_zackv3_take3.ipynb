{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.decomposition.cdnmf_fast module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.decomposition. Anything that cannot be imported from sklearn.decomposition is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "import pickle\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import math\n",
    "\n",
    "workingdirectory = os.popen('git rev-parse --show-toplevel').read()[:-1]\n",
    "sys.path.append(workingdirectory)\n",
    "os.chdir(workingdirectory)\n",
    "\n",
    "import allensdk.core.json_utilities as ju\n",
    "from allensdk.core.mouse_connectivity_cache import MouseConnectivityCache\n",
    "\n",
    "from mcmodels.core import VoxelModelCache\n",
    "from mcmodels.core.utils import get_structure_id,get_ordered_summary_structures\n",
    "from mcmodels.core.connectivity_data import get_connectivity_data\n",
    "from mcmodels.models.crossvalidation import get_nwloocv_predictions_multimodel_merge_dists\n",
    "from mcmodels.utils import nonzero_unique #, unionize\n",
    "from mcmodels.models.crossvalidation import get_best_hyperparameters,get_loss_best_hyp,get_loss#get_loocv_predictions,get_loss#get_best_hyperparameters,get_loss_best_hyp,get_loocv_predictions,get_loss\n",
    "from mcmodels.core.utils import get_leaves_ontologicalorder, get_indices, get_indices2,get_eval_indices,screen_index_matrices,screen_index_matrices2,screen_index_matrices3#get_cre_status,get_minorstructure_dictionary,get_leaves_ontologicalorder\n",
    "from mcmodels.core.utils import get_indices_2ormore\n",
    "from mcmodels.regressors import NadarayaWatson\n",
    "from mcmodels.core.plotting import plot_loss_surface,plot_loss_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation/ccf_2017\n"
     ]
    }
   ],
   "source": [
    "#read data\n",
    "TOP_DIR = '/Users/samsonkoelle/alleninstitute/mcm_2020/mcm_updated/'\n",
    "INPUT_JSON = os.path.join(TOP_DIR, 'input_011520.json')\n",
    "EXPERIMENTS_EXCLUDE_JSON = os.path.join(TOP_DIR, 'experiments_exclude.json')\n",
    "FILE_DIR = '/Users/samsonkoelle/alleninstitute/mcm_2020/mcm_updated/'\n",
    "OUTPUT_DIR = os.path.join(FILE_DIR, 'output')\n",
    "\n",
    "input_data = ju.read(INPUT_JSON)\n",
    "manifest_file = input_data.get('manifest_file')\n",
    "manifest_file = os.path.join(TOP_DIR, manifest_file)\n",
    "experiments_exclude = ju.read(EXPERIMENTS_EXCLUDE_JSON)\n",
    "\n",
    "#its unclear why the hyperparameters are loaded from the output directory\n",
    "cache = VoxelModelCache(manifest_file=manifest_file)\n",
    "major_structures = input_data.get('structures')\n",
    "major_structure_ids = [get_structure_id(cache, s) for s in major_structures]\n",
    "data_info = pd.read_excel('/Users/samsonkoelle/alleninstitute/Whole Brain Cre Image Series_curation only.xlsx', 'all datasets curated_070919pull')\n",
    "data_info.set_index(\"id\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_ordered_summary_structures(mcc):\n",
    "    # TODO : replace with json of wanted structures\n",
    "\n",
    "    \"\"\"Returns structure ids of summary structures - fiber tracts (and 934)\"\"\"\n",
    "    #ss_regions = mcc.get_structure_tree().get_structures_by_set_id([687527945])\n",
    "    ss_regions = mcc.get_structure_tree().get_structures_by_set_id([167587189])\n",
    "\n",
    "    # 934 not in 100 micron!!!!! (dont want fiber tracts)\n",
    "    ids, orders = [], []\n",
    "    for region in ss_regions:\n",
    "        if region[\"id\"] not in [934, 1009]:\n",
    "            ids.append(region[\"id\"])\n",
    "            orders.append(region[\"graph_order\"])\n",
    "\n",
    "    # return ids sorted by graph order\n",
    "    ids = np.asarray(ids)\n",
    "    return ids[np.argsort(orders)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ontological_order = get_ordered_summary_structures(cache)\n",
    "\n",
    "mcc = MouseConnectivityCache(manifest_file = '../connectivity/mouse_connectivity_manifest.json')\n",
    "st = mcc.get_structure_tree()\n",
    "ai_map = st.get_id_acronym_map()\n",
    "ia_map = {value: key for key, value in ai_map.items()}\n",
    "\n",
    "#regionalize voxel model: compare with regional model\n",
    "#regional parameters\n",
    "cre = None\n",
    "eid_set=None\n",
    "high_res=False\n",
    "threshold_injection = False\n",
    "\n",
    "COARSE_STRUCTURE_SET_ID = 2#3#2#167587189#3#2(old)\n",
    "DEFAULT_STRUCTURE_SET_IDS = tuple([COARSE_STRUCTURE_SET_ID])\n",
    "tree = cache.get_structure_tree()\n",
    "default_structures = tree.get_structures_by_set_id(DEFAULT_STRUCTURE_SET_IDS)\n",
    "default_structure_ids = [st['id'] for st in default_structures if st['id'] != 934]\n",
    "#cre= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MDRNv', 'MDRNd', 'MDRN')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia_map[1107], ia_map[1098], ia_map[395] #'MDRNd', 'MDRN' were summary structures in the old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315, array([277, 278, 279]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ontological_order), np.where(np.isin(ontological_order, np.asarray([1107, 1098, 395])))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontological_order = get_ordered_summary_structures(cache)\n",
    "ontological_order.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_ordered_summary_structuresold(mcc):\n",
    "    # TODO : replace with json of wanted structures\n",
    "\n",
    "    \"\"\"Returns structure ids of summary structures - fiber tracts (and 934)\"\"\"\n",
    "    #ss_regions = mcc.get_structure_tree().get_structures_by_set_id([687527945])\n",
    "    ss_regions = mcc.get_structure_tree().get_structures_by_set_id([687527945])\n",
    "\n",
    "    # 934 not in 100 micron!!!!! (dont want fiber tracts)\n",
    "    ids, orders = [], []\n",
    "    for region in ss_regions:\n",
    "        if region[\"id\"] not in [934, 1009]:\n",
    "            ids.append(region[\"id\"])\n",
    "            orders.append(region[\"graph_order\"])\n",
    "\n",
    "    # return ids sorted by graph order\n",
    "    ids = np.asarray(ids)\n",
    "    return ids[np.argsort(orders)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_ss = get_ordered_summary_structuresold(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_aligned_ids(list1,list2):\n",
    "    output = np.empty(len(list1), dtype = int)\n",
    "    for i in range(len(list1)):\n",
    "        output[ i] = np.intersect1d(st.ancestor_ids([list1[i]]), list2)[0]\n",
    "    return(output)\n",
    "eval_cres = ['C57BL/6J']\n",
    "ontological_order_majors = get_aligned_ids(new,major_structure_ids)\n",
    "#ontological_order_leaves_summary = get_aligned_ids(ontological_order_leaves,ontological_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = [ia_map[ontological_order_majors[i] ]for i in range(len(ontological_order_majors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       58,        75,       395,     10671, 484682470, 484682508,\n",
       "       549009211, 549009215, 549009219, 549009223, 549009227, 560581559,\n",
       "       560581563, 563807435, 563807439, 576073699, 576073704, 589508447,\n",
       "       589508451, 589508455, 599626923, 599626927, 606826663, 607344830])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [ia_map[new[i] ] for i in range(len(new))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['MT', 'MB'],\n",
       "       ['DT', 'MB'],\n",
       "       ['MDRN', 'MY'],\n",
       "       ['ME', 'HY'],\n",
       "       ['ProS', 'HPF'],\n",
       "       ['APr', 'HPF'],\n",
       "       ['MA3', 'MB'],\n",
       "       ['P5', 'P'],\n",
       "       ['Acs5', 'P'],\n",
       "       ['PC5', 'P'],\n",
       "       ['I5', 'P'],\n",
       "       ['Xi', 'TH'],\n",
       "       ['PIL', 'TH'],\n",
       "       ['PoT', 'TH'],\n",
       "       ['IntG', 'TH'],\n",
       "       ['VMPO', 'HY'],\n",
       "       ['PeF', 'HY'],\n",
       "       ['HATA', 'HPF'],\n",
       "       ['Pa5', 'MY'],\n",
       "       ['VeCB', 'CB'],\n",
       "       ['SCO', 'MB'],\n",
       "       ['PDTg', 'P'],\n",
       "       ['Pa4', 'MB'],\n",
       "       ['PN', 'MB']], dtype='<U4')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([a,b]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[395, 370, 354, 1065, 343, 8, 997]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.ancestor_ids([395])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MY'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia_map[354]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reference_space = cache.get_reference_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ReferenceSpace in module allensdk.core.reference_space object:\n",
      "\n",
      "class ReferenceSpace(builtins.object)\n",
      " |  ReferenceSpace(structure_tree, annotation, resolution)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, structure_tree, annotation, resolution)\n",
      " |      Handles brain structures in a 3d reference space\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      structure_tree : StructureTree\n",
      " |          Defines the heirarchy and properties of the brain structures.\n",
      " |      annotation : numpy ndarray\n",
      " |          3d volume whose elements are structure ids.\n",
      " |      resolution : length-3 tuple of numeric\n",
      " |          Resolution of annotation voxels along each dimension.\n",
      " |  \n",
      " |  check_coverage(self, structure_ids, domain_mask)\n",
      " |      Determines whether a spatial domain is completely covered by \n",
      " |      structures in a set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      structure_ids : list of int \n",
      " |          Specifies the set of structures to check.\n",
      " |      domain_mask : numpy ndarray\n",
      " |          Same shape as annotation. 1 inside the mask, 0 out. Specifies \n",
      " |          spatial domain.\n",
      " |          \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy ndarray : \n",
      " |          1 where voxels are missing from the candidate, 0 where the \n",
      " |          candidate exceeds the domain\n",
      " |  \n",
      " |  direct_voxel_counts(self)\n",
      " |      Determines the number of voxels directly assigned to one or more \n",
      " |      structures.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict : \n",
      " |          Keys are structure ids, values are the number of voxels directly \n",
      " |          assigned to those structures.\n",
      " |  \n",
      " |  downsample(self, target_resolution)\n",
      " |      Obtain a smaller reference space by downsampling\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      target_resolution : tuple of numeric\n",
      " |          Resolution in microns of the output space.\n",
      " |      interpolator : string\n",
      " |          Method used to interpolate the volume. Currently only 'nearest' \n",
      " |          is supported\n",
      " |          \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ReferenceSpace : \n",
      " |          A new ReferenceSpace with the same structure tree and a \n",
      " |          downsampled annotation.\n",
      " |  \n",
      " |  export_itksnap_labels(self, id_type=<class 'numpy.uint16'>, label_description_kwargs=None)\n",
      " |      Produces itksnap labels, remapping large ids if needed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      id_type : np.integer, optional\n",
      " |          Used to determine the type of the output annotation and whether ids need to be remapped to smaller values.\n",
      " |      label_description_kwargs : dict, optional\n",
      " |          Keyword arguments passed to StructureTree.export_label_description\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      np.ndarray : \n",
      " |          Annotation volume, remapped if needed\n",
      " |      pd.DataFrame\n",
      " |          label_description dataframe\n",
      " |  \n",
      " |  get_slice_image(self, axis, position, cmap=None)\n",
      " |      Produce a AxBx3 RGB image from a slice in the annotation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : int\n",
      " |          Along which to slice the annotation volume. 0 is coronal, 1 is \n",
      " |          horizontal, and 2 is sagittal.\n",
      " |      position : int \n",
      " |          In microns. Take the slice from this far along the specified axis.\n",
      " |      cmap : dict, optional\n",
      " |          Keys are structure ids, values are rgb triplets. Defaults to \n",
      " |          structure rgb_triplets. \n",
      " |          \n",
      " |      Returns\n",
      " |      -------\n",
      " |      np.ndarray : \n",
      " |          RGB image array. \n",
      " |          \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you assign a custom colormap, make sure that you take care of the \n",
      " |      background in addition to the structures.\n",
      " |  \n",
      " |  make_structure_mask(self, structure_ids, direct_only=False)\n",
      " |      Return an indicator array for one or more structures\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      structure_ids : list of int\n",
      " |          Make a mask that indicates the union of these structures' voxels\n",
      " |      direct_only : bool, optional\n",
      " |          If True, only include voxels directly assigned to a structure in \n",
      " |          the mask. Otherwise include voxels assigned to descendants.\n",
      " |          \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy ndarray :\n",
      " |          Same shape as annotation. 1 inside mask, 0 outside.\n",
      " |  \n",
      " |  many_structure_masks(self, structure_ids, output_cb=None, direct_only=False)\n",
      " |      Build one or more structure masks and do something with them\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      structure_ids : list of int\n",
      " |          Specify structures to be masked\n",
      " |      output_cb : function, optional\n",
      " |          Must have the following signature: output_cb(structure_id, fn). \n",
      " |          On each requested id, fn will be curried to make a mask for that \n",
      " |          id. Defaults to returning the structure id and mask.\n",
      " |      direct_only : bool, optional\n",
      " |          If True, only include voxels directly assigned to a structure in \n",
      " |          the mask. Otherwise include voxels assigned to descendants.\n",
      " |          \n",
      " |      Yields\n",
      " |      -------\n",
      " |      Return values of output_cb called on each structure_id, structure_mask \n",
      " |      pair.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      output_cb is called on every yield, so any side-effects (such as \n",
      " |      writing to a file) will be carried out regardless of what you do with \n",
      " |      the return values. You do actually have to iterate through the output, \n",
      " |      though.\n",
      " |  \n",
      " |  remove_unassigned(self, update_self=True)\n",
      " |      Obtains a structure tree consisting only of structures that have \n",
      " |      at least one voxel in the annotation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      update_self : bool, optional\n",
      " |          If True, the contained structure tree will be replaced,\n",
      " |          \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of dict : \n",
      " |          elements are filtered structures\n",
      " |  \n",
      " |  total_voxel_counts(self)\n",
      " |      Determines the number of voxels assigned to a structure or its \n",
      " |      descendants\n",
      " |          \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict : \n",
      " |          Keys are structure ids, values are the number of voxels assigned \n",
      " |          to structures' descendants.\n",
      " |  \n",
      " |  validate_structures(self, structure_ids, domain_mask)\n",
      " |      Determines whether a set of structures produces an exact and \n",
      " |      nonoverlapping tiling of a spatial domain\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      structure_ids : list of int \n",
      " |          Specifies the set of structures to check.\n",
      " |      domain_mask : numpy ndarray\n",
      " |         Same shape as annotation. 1 inside the mask, 0 out. Specifies \n",
      " |         spatial domain.\n",
      " |         \n",
      " |      Returns\n",
      " |      -------\n",
      " |      set : \n",
      " |          Ids of structures that are the ancestors of other structures in \n",
      " |          the supplied set.\n",
      " |      numpy ndarray : \n",
      " |          Indicator for missing voxels.\n",
      " |  \n",
      " |  write_itksnap_labels(self, annotation_path, label_path, **kwargs)\n",
      " |      Generate a label file (nrrd) and a label_description file (csv) for use with ITKSnap\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      annotation_path : str\n",
      " |          write generated label file here\n",
      " |      label_path : str\n",
      " |          write generated label_description file here\n",
      " |      **kwargs : \n",
      " |          will be passed to self.export_itksnap_labels\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  check_and_write(base_dir, structure_id, fn)\n",
      " |      A many_structure_masks callback that writes the mask to a nrrd file \n",
      " |      if the file does not already exist.\n",
      " |  \n",
      " |  return_mask_cb(structure_id, fn)\n",
      " |      A basic callback for many_structure_masks\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  direct_voxel_map\n",
      " |  \n",
      " |  total_voxel_map\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(reference_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = np.setdiff1d(ontological_order,old_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1098, 1107])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.setdiff1d(old_ss, ontological_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(old_ss == 395)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#due to redundancy in ccfv3, remove 'MDRNv', 'MDRNd' from summary structures (include as leafs of MDRN)\n",
    "ontological_order[277]\n",
    "ontological_order = np.setdiff1d(ontological_order, [1098, 1107])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COARSE_STRUCTURE_SET_ID = 167587189#2#167587189#3#2(old)\n",
    "# DEFAULT_STRUCTURE_SET_IDS = tuple([COARSE_STRUCTURE_SET_ID])\n",
    "# tree = cache.get_structure_tree()\n",
    "# default_structures = tree.get_structures_by_set_id(DEFAULT_STRUCTURE_SET_IDS)\n",
    "# print(len(default_structures))#, default_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mcmodels.core.base import VoxelData#, RegionalData\n",
    "from mcmodels.utils import unionize #nonzero_unique,\n",
    "import numpy as np\n",
    "\n",
    "class ModelData(object):\n",
    "\n",
    "    def __init__(self, cache, structure_id, structure_set_id = 167587189):\n",
    "        self.cache = cache\n",
    "        self.structure_id = structure_id\n",
    "        self.structure_set_id = structure_set_id\n",
    "\n",
    "    def get_structure_id(self, acronym):\n",
    "        structure_tree = self.cache.get_structure_tree()\n",
    "        return structure_tree.get_structures_by_acronym([acronym])[0]['id']\n",
    "\n",
    "    def get_experiment_ids(self, eid_set=None, experiments_exclude=[], cre=None):\n",
    "        \"\"\"gets model data from ...\"\"\"\n",
    "\n",
    "        # get experiments\n",
    "        experiments = self.cache.get_experiments(\n",
    "            # injection_structure_ids=[self.structure_id], cre=False)\n",
    "            injection_structure_ids=[self.structure_id], cre=cre)\n",
    "        experiment_ids = [e['id'] for e in experiments]\n",
    "\n",
    "        # exclude bad, restrict to eid_set\n",
    "        eid_set = experiment_ids if eid_set is None else eid_set\n",
    "        return set(experiment_ids) & set(eid_set) - set(experiments_exclude)\n",
    "\n",
    "    def get_voxel_data(self, **kwargs):\n",
    "        print('here')\n",
    "        # eid_set = kwargs.pop('eid_set')\n",
    "        experiments_exclude = kwargs.pop('experiments_exclude')\n",
    "        injection_hemisphere_id = kwargs.pop('injection_hemisphere_id')\n",
    "        print(injection_hemisphere_id)\n",
    "        cre = kwargs.pop('cre')\n",
    "        experiment_ids = self.get_experiment_ids(experiments_exclude=experiments_exclude, cre=cre)\n",
    "        # projection_hemisphere_id = kwargs.pop('projection_hemisphere_id'3)\n",
    "        data = VoxelData(self.cache, injection_structure_ids=[self.structure_id],\n",
    "                         # injection_hemisphere_id=2, flip_experiments = True)\n",
    "                         injection_hemisphere_id=injection_hemisphere_id, flip_experiments=True)  # 2 + flip flips 1s into 2s\n",
    "        data.get_experiment_data(experiment_ids)\n",
    "        data.experiment_ids = experiment_ids\n",
    "        return data\n",
    "\n",
    "    def get_regional_data_fromvoxel(self, **kwargs):\n",
    "        data = self.get_voxel_data(**kwargs)\n",
    "\n",
    "    def get_regional_data(self, high_res=False, threshold_injection=True, **kwargs):\n",
    "        def get_summary_structure_ids():\n",
    "            structure_tree = self.cache.get_structure_tree()\n",
    "            structures = structure_tree.get_structures_by_set_id([self.structure_set_id])\n",
    "\n",
    "            return [s['id'] for s in structures if s['id'] not in (934, 1009)]\n",
    "\n",
    "        def get_injection_regions(region_set):\n",
    "            \"\"\"Return regions in region_set if descend from structure_id\"\"\"\n",
    "            st = self.cache.get_structure_tree()\n",
    "            return [r for r in region_set\n",
    "                    if st.structure_descends_from(r, self.structure_id)]\n",
    "\n",
    "        projection_hemisphere_id = kwargs.pop('projection_hemisphere_id', 3)\n",
    "\n",
    "        # get summary structures\n",
    "        region_set = get_summary_structure_ids()\n",
    "        injection_set = get_injection_regions(region_set)\n",
    "\n",
    "        # get experiments\n",
    "        experiment_ids = self.get_experiment_ids(**kwargs)\n",
    "\n",
    "        container = RegionalData if high_res else VoxelData\n",
    "        container_kwargs = dict(injection_structure_ids=injection_set,\n",
    "                                projection_structure_ids=region_set,\n",
    "                                injection_hemisphere_id=2,\n",
    "                                projection_hemisphere_id=projection_hemisphere_id,\n",
    "                                normalized_injection=True,\n",
    "                                normalized_projection=True,\n",
    "                                flip_experiments=True)\n",
    "\n",
    "        data = container(self.cache, **container_kwargs)\n",
    "        data.get_experiment_data(experiment_ids)\n",
    "\n",
    "        # get model data\n",
    "        if not high_res:\n",
    "            data.injections = unionize(\n",
    "                data.injections, data.injection_mask.get_key(\n",
    "                    structure_ids=injection_set, hemisphere_id=2))\n",
    "            data.projections = unionize(\n",
    "                data.projections, data.projection_mask.get_key(\n",
    "                    structure_ids=region_set, hemisphere_id=projection_hemisphere_id))\n",
    "\n",
    "        # threshold injection\n",
    "        # NOTE\n",
    "        if threshold_injection:\n",
    "            pct = np.percentile(data.injections[data.injections.nonzero()], 5)\n",
    "            data.injections[data.injections < pct] = 0\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method get_reference_space in module allensdk.core.reference_space_cache:\n",
      "\n",
      "get_reference_space(structure_file_name=None, annotation_file_name=None) method of mcmodels.core.voxel_model_cache.VoxelModelCache instance\n",
      "    Build a ReferenceSpace from this cache's annotation volume and \n",
      "    structure tree. The ReferenceSpace does operations that relate brain \n",
      "    structures to spatial domains.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    \n",
      "    structure_file_name: string\n",
      "        File name to save/read the structures table.  If file_name is None,\n",
      "        the file_name will be pulled out of the manifest.  If caching\n",
      "        is disabled, no file will be saved. Default is None.\n",
      "        \n",
      "    annotation_file_name: string\n",
      "        File name to store the annotation volume.  If it already exists,\n",
      "        it will be read from this file.  If file_name is None, the\n",
      "        file_name will be pulled out of the manifest.  Default is None.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cache.get_reference_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "703\n",
      "1089\n",
      "1097\n",
      "315\n",
      "313\n",
      "354\n",
      "698\n",
      "771\n",
      "803\n",
      "477\n",
      "549\n"
     ]
    }
   ],
   "source": [
    "connectivity_data = get_connectivity_data(cache, major_structure_ids, experiments_exclude, remove_injection = False)\n",
    "\n",
    "connectivity_data.get_injection_hemisphere_ids()\n",
    "connectivity_data.align()\n",
    "connectivity_data.get_centroids()\n",
    "connectivity_data.get_data_matrices(default_structure_ids)\n",
    "connectivity_data.get_crelines(data_info)\n",
    "with open('data/info/leafs.pickle', 'rb') as handle:\n",
    "    leafs = pickle.load(handle)\n",
    "    \n",
    "connectivity_data.ai_map = ai_map\n",
    "connectivity_data.get_summarystructures(data_info)\n",
    "connectivity_data.leafs = leafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#major division segregation is legacy code but convenient for fast cross validation in major division model\n",
    "#experiments_minor_structures = get_summarystructure_dictionary(connectivity_data, data_info)\n",
    "#get leaves in ontological order.  Where leafs don't exist, uses summary structure\n",
    "ontological_order_leaves = get_leaves_ontologicalorder(connectivity_data, ontological_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(564,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontological_order_leaves.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Key isn't affected by which experiment we choose. This allows default masking to be inherited from the AllenSDK.\n",
    "sid0 = list(connectivity_data.structure_datas.keys())[0]\n",
    "eid0 = list(connectivity_data.structure_datas[sid0].experiment_datas.keys())[0]\n",
    "#Identify keys denoting which voxels correspond to which structure in the ipsi and contra targets.\n",
    "contra_targetkey = connectivity_data.structure_datas[sid0].projection_mask.get_key(structure_ids=ontological_order_leaves, hemisphere_id=1)\n",
    "ipsi_targetkey = connectivity_data.structure_datas[sid0].projection_mask.get_key(structure_ids=ontological_order_leaves, hemisphere_id=2)\n",
    "\n",
    "#contra_targetkey = connectivity_data.structure_datas[sid0].projection_mask.get_key(structure_ids=ontological_order, hemisphere_id=1)\n",
    "#ipsi_targetkey = connectivity_data.structure_datas[sid0].projection_mask.get_key(structure_ids=ontological_order, hemisphere_id=2)\n",
    "#get average intensities of projection structures given ipsi and contra keys\n",
    "#source_key = ontological_order #only relevant here when injection needs to be unionized, but currently a required argument\n",
    "ipsi_target_regions, ipsi_target_counts = nonzero_unique(ipsi_targetkey, return_counts=True)\n",
    "contra_target_regions, contra_target_counts = nonzero_unique(contra_targetkey, return_counts=True)\n",
    "\n",
    "target_order = lambda x: np.array(ontological_order_leaves)[np.isin(ontological_order_leaves, x)]\n",
    "permutation = lambda x: np.argsort(np.argsort(target_order(x)))\n",
    "targ_ids = np.concatenate([ipsi_target_regions[permutation(ipsi_target_regions)],\n",
    "                           contra_target_regions[permutation(contra_target_regions)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(564,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipsi_target_regions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(566,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontological_order_leaves.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448962,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipsi_targetkey.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connectivity_data.get_regionalized_normalized_data(ontological_order, ipsi_targetkey, contra_targetkey)\n",
    "connectivity_data.get_creleaf_combos()\n",
    "connectivity_data.leaf2_index_matrices = get_indices_2ormore(connectivity_data.leafs)\n",
    "connectivity_data.creleaf2_index_matrices = get_indices_2ormore(connectivity_data.creleaf_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_cre_distances(projections, means_cast, sids, cres):\n",
    "    nsamp = cres.shape[0]\n",
    "    credist = np.empty((nsamp,nsamp))\n",
    "    credist[:] = np.nan\n",
    "    for i in range(nsamp):\n",
    "        #print(i)\n",
    "        meani = means_cast[sids[i]][cres[i]]#means_cast.loc[tuple([cres[i], sids[i]])]\n",
    "        for j in range(nsamp):\n",
    "            meanj = means_cast[sids[j]][cres[j]]\n",
    "            if sids[j] == sids[i]:\n",
    "                credist[i,j]  = np.linalg.norm(meani - meanj)#**2\n",
    "    return(credist)\n",
    "def get_loss_surface_cv(projections, centroids, cres, sids,fraction, gamma = 100000):\n",
    "    means_cast = get_means(projections, cres, sids)\n",
    "    cre_distances_cv = get_cre_distances_cv(projections, means_cast, sids, cres)\n",
    "    surface = get_surface_from_distances(projections,centroids,cre_distances_cv, fraction, gamma)\n",
    "    surface.cre_distances_cv = cre_distances_cv\n",
    "    return(surface)\n",
    "\n",
    "def get_surface_from_distances(projections,centroids,cre_distances, fraction, gamma = 100000):\n",
    "    \n",
    "    nsamp = centroids.shape[0]\n",
    "    pairs = np.asarray(np.where(~np.isnan(cre_distances))).transpose() #not all cres will have distances, e.g. if not in same leaf\n",
    "    ngp = pairs.shape[0]\n",
    "    \n",
    "    coordinates = np.zeros((ngp,2))\n",
    "    projection_distances = np.zeros((ngp,1))\n",
    "    for i in range(ngp):\n",
    "        coordinates[i,0] = np.linalg.norm(centroids[pairs[i][0]] - centroids[pairs[i][1]])**2\n",
    "        coordinates[i,1] = cre_distances[pairs[i][0]][pairs[i][1]]\n",
    "        projection_distances[i] = np.linalg.norm(projections[pairs[i][0]] - projections[pairs[i][1]])**2\n",
    "    coordinates_normed = coordinates / np.linalg.norm(coordinates, axis = 0)#**2\n",
    "\n",
    "    surface = NadarayaWatson(kernel='rbf',  gamma  = gamma)\n",
    "    randos = random.sample(list(range(ngp)), math.floor(ngp * fraction))\n",
    "    surface.fit(coordinates_normed[randos], projection_distances[randos])\n",
    "    surface.coordinates_normed = coordinates_normed\n",
    "    surface.norms = np.linalg.norm(coordinates, axis = 0)\n",
    "    surface.projection_distances = projection_distances\n",
    "    return(surface)\n",
    "\n",
    "def get_means(projections, cres, sids):\n",
    "    cre_means = {}\n",
    "    cre_types = np.unique(cres)\n",
    "    sid_types = np.unique(sids)\n",
    "    for i in range(len(sid_types)):\n",
    "        cre_means[sid_types[i]] = {}\n",
    "        sid_inds = np.where(sids == sid_types[i])[0]\n",
    "        for j in range(len(cre_types)): \n",
    "            cre_inds = np.where(cres == cre_types[j])[0]\n",
    "            cre_means[sid_types[i]][cre_types[j]] = np.mean(projections[np.intersect1d(sid_inds, cre_inds)], axis = 0 )\n",
    "    return(cre_means)\n",
    "\n",
    "def get_cre_distances_cv(proj, means_cast, sids,cres):\n",
    "    nsamp = cres.shape[0]\n",
    "    credist = np.empty((nsamp,nsamp))\n",
    "    credist[:] = np.nan\n",
    "    for i in range(nsamp):\n",
    "        #print(i)\n",
    "        meani = meani = means_cast[sids[i]][cres[i]]\n",
    "        ls = np.where(leafs[sid] == leafs[sid][i])[0]\n",
    "        crs = np.where(cres == cres[i])[0]\n",
    "        ncr = len(np.intersect1d(ls, crs))\n",
    "        \n",
    "        meanloocvi = meani\n",
    "        if ncr > 1:\n",
    "            meanloocvi = (ncr * meani ) / (ncr - 1) -   (1/ ncr)* proj[i] #results[reg[i]][tuple([cs[reg_cre_ind[j],1], cs[reg_cre_ind[k],1]])]\n",
    "        else:\n",
    "            meanloocvi = np.zeros(proj[i].shape[0])\n",
    "            meanloocvi[:] = np.nan\n",
    "            \n",
    "        for j in range(nsamp):\n",
    "            meanj = means_cast[sids[j]][cres[j]]\n",
    "            if sids[j] == sids[i]:\n",
    "                credist[j,i]  = np.linalg.norm(meanloocvi - meanj)\n",
    "\n",
    "    return(credist)\n",
    "\n",
    "def get_embedding_cv(surface, dists, cre_distances_cv):\n",
    "    \n",
    "    ntrain = dists.shape[0]\n",
    "    norms = surface.norms\n",
    "    leaf_pairs = np.asarray(np.where(~np.isnan(cre_distances_cv))).transpose()\n",
    "    nlp = leaf_pairs.shape[0]\n",
    "    \n",
    "    losses = np.zeros((ntrain, ntrain))\n",
    "    for i in range(nlp):\n",
    "        d_ij = dists[leaf_pairs[i][0]][leaf_pairs[i][1]] / norms[0]\n",
    "        p_ij = cre_distances_cv[leaf_pairs[i][0]][leaf_pairs[i][1]] / norms[1]\n",
    "        losses[leaf_pairs[i][0]][leaf_pairs[i][1]] = surface.predict(np.asarray([[d_ij, p_ij]]))\n",
    "      \n",
    "    losses[np.where(losses == 0)] = np.nan\n",
    "    \n",
    "    return(losses)\n",
    "\n",
    "\n",
    "from mcmodels.core import Mask\n",
    "\n",
    "\n",
    "def get_embedding(surface, dists, cres = None, cre = None, means = None):\n",
    "    \n",
    "    ntrain = dists.shape[0]\n",
    "    neval = dists.shape[1]\n",
    "    norms = surface.norms\n",
    "    #cnorm = surface.cnorm\n",
    "    \n",
    "    cre_deezy = np.zeros((ntrain))\n",
    "    \n",
    "    for i in range(ntrain):\n",
    "        cre_deezy[i] = np.linalg.norm(means[cres[i]] - means[cre])\n",
    "    \n",
    "    losses = np.zeros((ntrain, neval))\n",
    "    for i in range(ntrain):\n",
    "        for j in range(neval):\n",
    "            d_ij = dists[i,j] / norms[0]\n",
    "            p_i = cre_deezy[i] / norms[1]\n",
    "            losses[i,j] = surface.predict(np.asarray([[d_ij, p_i]]))\n",
    "            \n",
    "    return(losses)\n",
    "\n",
    "\n",
    "\n",
    "def get_nw_predictions(projections, dists, gamma):\n",
    "  \n",
    "     \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    neval = dists.shape[1]\n",
    "    #nexp = centroids.shape[0]\n",
    "    predictions = np.zeros((neval, projections.shape[1]))\n",
    "    predictions[:] = np.nan\n",
    "    \n",
    "    #print(model_index_val.shape, eval_index_val.shape)\n",
    "    #weights = np.exp(- dists / gamma)#np.exp(-dists[model_index_val] / gamma) #get_weights(centroids, gamma)\n",
    "    for i in range(neval):\n",
    "        dists_i = dists[:,i] - np.min(dists[:,i])\n",
    "        #dists_i = dists[i,:] - np.min(dists[i,:])\n",
    "        weights_i = np.exp(- dists_i * gamma)\n",
    "        weights_i = np.asarray(weights_i, dtype=np.float32)\n",
    "        weights_i[np.isnan(weights_i)] = 0.\n",
    "        weights_i = weights_i / np.sum(weights_i)\n",
    "        predictions[i] = np.dot(weights_i, projections)\n",
    "        \n",
    "    return(predictions) \n",
    "\n",
    "def get_connectivity_matrices2_nw(connectivity_data, gamma_dict, cres, structures, model_ordering, source_ordering, target_ordering, eval_cres, cre_model = True):\n",
    "    \n",
    "    nsource = len(source_ordering)\n",
    "    #n#target = len(target_ordering)\n",
    "    ncre = len(eval_cres)\n",
    "\n",
    "    ipsi_target_regions = connectivity_data.ipsi_target_regions\n",
    "    contra_target_regions = connectivity_data.contra_target_regions                               \n",
    "    ipsi_indices= np.asarray([])\n",
    "    contra_indices = np.asarray([])\n",
    "    for iy in target_ordering: \n",
    "        ipsi_indices = np.concatenate([ipsi_indices, np.where(ipsi_target_regions==iy)[0]] )\n",
    "        contra_indices = np.concatenate([contra_indices, np.where(contra_target_regions==iy)[0]] )\n",
    "    ipsi_indices = np.asarray(ipsi_indices, dtype = int)   \n",
    "    contra_indices = np.asarray(contra_indices, dtype = int)    \n",
    "    reorder = np.concatenate([ipsi_indices, len(ipsi_indices) + contra_indices])  \n",
    "    ntarget = len(reorder)\n",
    "      \n",
    "    connectivity = np.zeros((ncre, nsource, ntarget))\n",
    "    connectivity[:] = np.nan\n",
    "    #structure_major_dictionary = connectivity_data.structure_major_dictionary\n",
    "    for c in range(ncre):\n",
    "        for i in range(nsource):\n",
    "            print(i,source_ordering[i])\n",
    "            sid = model_ordering[i,0]#structure_major_dictionary[source_ordering[i]]\n",
    "            gamma = gamma_dict[sid]\n",
    "            connectivity[c,i] = get_region_prediction2(cache,\n",
    "                                                      connectivity_data.structure_datas[sid],\n",
    "                                                      exp_structures = structures[sid],\n",
    "                                                      model_region = model_ordering[i,1],\n",
    "                                                      prediction_region= source_ordering[i],\n",
    "                                                      cre = eval_cres[c],\n",
    "                                                      gamma = gamma_dict[sid],\n",
    "                                                      cre_model = cre_model)\n",
    "                                                      \n",
    "    connectivity = connectivity[:,:,reorder]                                                  \n",
    "                                                      \n",
    "    return(connectivity)\n",
    "\n",
    "\n",
    "def get_region_prediction2(cache, structure_data,  exp_structures, model_region, prediction_region, cre, gamma, surface = None, cre_model = False):\n",
    "    \n",
    "    model_experiments = np.where(exp_structures == model_region)[0]\n",
    "    nexp = len(model_experiments)\n",
    "    \n",
    "    cres = structure_data.crelines[model_experiments]\n",
    "    mask = Mask.from_cache(cache,structure_ids=[prediction_region],hemisphere_id=2)\n",
    "\n",
    "#     if surface != None and cre_model != True:\n",
    "#         means = get_means(projections,cres, np.repeat(model_region,nexp))\n",
    "        \n",
    "#         losses = get_embedding(surface, pairwise_distances(centroids, mask.coordinates)**2, cres, cre, means[model_region])\n",
    "#         predictions = get_nw_predictions(projections, losses, gamma)\n",
    "#         output = np.mean(predictions, axis = 0)\n",
    "        \n",
    "    if surface != None and cre_model == True:\n",
    "        projections = structure_data.reg_proj_norm[model_experiments]\n",
    "        centroids = structure_data.centroids[model_experiments]\n",
    "        means = get_means(projections, cres, np.repeat(model_region,nexp))\n",
    "        #print(means)\n",
    "        print(prediction_region, model_region,means.keys())\n",
    "        #supposed to check if theres a cre in that region\n",
    "        if np.isin(model_region, list(means.keys())):\n",
    "            if np.isin(cre, np.asarray(list(means[model_region].keys()))):\n",
    "                losses = get_embedding(surface, pairwise_distances(centroids, mask.coordinates)**2, cres, cre, means[model_region])\n",
    "                predictions = get_nw_predictions(projections, losses, gamma)\n",
    "                output = np.mean(predictions, axis = 0)\n",
    "            else:\n",
    "                output = np.zeros(projections.shape[1])\n",
    "                output[:] = np.nan\n",
    "        else:\n",
    "            output = np.zeros(projections.shape[1])\n",
    "            output[:] = np.nan           \n",
    "#     if surface == None and cre_model != True:\n",
    "#         means = get_means(projections, cres, [model_region])\n",
    "#         predictions = means[cres]\n",
    "#         output = np.mean(predictions, axis = 0)\n",
    "        \n",
    "    if surface == None and cre_model == True:\n",
    "        \n",
    "        \n",
    "        modelcre_experiments = np.where(cres == cre)[0]\n",
    "        if len(modelcre_experiments)>0:\n",
    "            projections = structure_data.reg_proj_norm[model_experiments][modelcre_experiments]\n",
    "            centroids = structure_data.centroids[model_experiments][modelcre_experiments]\n",
    "            #print((pairwise_distances(centroids, mask.coordinates)**2).shape)\n",
    "            predictions = get_nw_predictions(projections, pairwise_distances(centroids, mask.coordinates)**2, gamma)\n",
    "            output = np.mean(predictions, axis = 0)\n",
    "        else:\n",
    "            output = np.zeros(structure_data.reg_proj_norm.shape[1])\n",
    "            output[:] = np.nan  \n",
    "            \n",
    "    if surface == None and cre_model == False:\n",
    "        \n",
    "        if len(model_experiments)>0:\n",
    "            projections = structure_data.reg_proj_norm[model_experiments]\n",
    "            centroids = structure_data.centroids[model_experiments]\n",
    "            predictions = get_nw_predictions(projections, pairwise_distances(centroids, mask.coordinates)**2, gamma)\n",
    "            output = np.mean(predictions, axis = 0)\n",
    "        else:\n",
    "            output = np.zeros(structure_data.reg_proj_norm.shape[1])\n",
    "            output[:] = np.nan  \n",
    "        \n",
    "    return(output)\n",
    "\n",
    "\n",
    "#good one\n",
    "#model_ordering_leafs,model_region_structures are len n exps, which model to use\n",
    "#model_experiments_leaf,model_experiments_structure are inds of \n",
    "def get_connectivity_matrices3(connectivity_data, surfaces, cres, structures_smooth,structures_surface, model_ordering_leafs, model_ordering_surfaces, source_ordering, target_ordering, eval_cres):\n",
    "    \n",
    "    nsource = len(source_ordering)\n",
    "    #n#target = len(target_ordering)\n",
    "    ncre = len(eval_cres)\n",
    "\n",
    "    ipsi_target_regions = connectivity_data.ipsi_target_regions\n",
    "    contra_target_regions = connectivity_data.contra_target_regions                               \n",
    "    ipsi_indices= np.asarray([])\n",
    "    contra_indices = np.asarray([])\n",
    "    for iy in target_ordering: \n",
    "        ipsi_indices = np.concatenate([ipsi_indices, np.where(ipsi_target_regions==iy)[0]] )\n",
    "        contra_indices = np.concatenate([contra_indices, np.where(contra_target_regions==iy)[0]] )\n",
    "    ipsi_indices = np.asarray(ipsi_indices, dtype = int)   \n",
    "    contra_indices = np.asarray(contra_indices, dtype = int)    \n",
    "    reorder = np.concatenate([ipsi_indices, len(ipsi_indices) + contra_indices])  \n",
    "    ntarget = len(reorder)\n",
    "      \n",
    "    connectivity = np.zeros((ncre, nsource, ntarget))\n",
    "    connectivity[:] = np.nan\n",
    "    #structure_major_dictionary = connectivity_data.structure_major_dictionary\n",
    "    for c in range(ncre):\n",
    "        for i in range(nsource):\n",
    "            print(i,source_ordering[i])\n",
    "            sid = model_ordering_leafs[i,0]#structure_major_dictionary[source_ordering[i]]\n",
    "            gamma = surfaces[sid].gamma#gamma_dict[sid]\n",
    "            connectivity[c,i] = get_region_prediction3(cache,\n",
    "                                                      connectivity_data.structure_datas[sid],\n",
    "                                                      #structures_sid = structures[sid],\n",
    "                                                       structures_surface_sid =structures_surface[sid],\n",
    "                                                       structures_smooth_sid= structures_smooth[sid],\n",
    "                                                      model_region_leaf = model_ordering_leafs[i,1],\n",
    "                                                       model_region_surface = model_ordering_surfaces[i,1],\n",
    "                                                      prediction_region= source_ordering[i],\n",
    "                                                      cre = eval_cres[c],\n",
    "                                                      gamma = surfaces[sid].bestgamma,\n",
    "                                                      surface = surfaces[sid],\n",
    "                                                      cre_model = True)\n",
    "                                                      \n",
    "    connectivity = connectivity[:,:,reorder]                                                  \n",
    "                                                      \n",
    "    return(connectivity)\n",
    "\n",
    "def get_region_prediction3(cache, structure_data,  structures_surface_sid,structures_smooth_sid, model_region_surface, model_region_leaf, prediction_region, cre, gamma, surface = None, cre_model = False):\n",
    "    \n",
    "\n",
    "    model_experiments_leaf = np.where(structures_smooth_sid == model_region_leaf)[0]\n",
    "    model_experiments_surface = np.where(structures_surface_sid == model_region_surface)[0]\n",
    "    \n",
    "    nexp_surface = len(model_experiments_surface)\n",
    "    \n",
    "    cres_surface = structure_data.crelines[model_experiments_surface]\n",
    "    mask = Mask.from_cache(cache,structure_ids=[prediction_region],hemisphere_id=2)\n",
    "    if surface != None and cre_model == True:\n",
    "        projections = structure_data.reg_proj_norm[model_experiments_leaf]\n",
    "        projections_surface = structure_data.reg_proj_norm[model_experiments_surface]\n",
    "        centroids = structure_data.centroids[model_experiments_leaf]#model_region_leaf\n",
    "        means = get_means(projections_surface, cres_surface, np.repeat(model_region_surface,nexp_surface))\n",
    "        #supposed to check if theres a cre in that region\n",
    "        #if np.isin(model_region_surface, list(means.keys())):\n",
    "        if centroids.shape[0]>0:\n",
    "            if np.isin(cre, np.asarray(list(means[model_region_surface].keys()))):\n",
    "                losses = get_embedding(surface, pairwise_distances(centroids, mask.coordinates)**2, cres_surface, cre, means[model_region_surface])\n",
    "                predictions = get_nw_predictions(projections, losses, gamma)\n",
    "                output = np.mean(predictions, axis = 0)\n",
    "            else:\n",
    "                output = np.zeros(projections.shape[1])\n",
    "                output[:] = np.nan\n",
    "        else:\n",
    "            output = np.zeros(projections.shape[1])\n",
    "            output[:] = np.nan\n",
    "    if surface == None and cre_model == True:\n",
    "        \n",
    "        \n",
    "        modelcre_experiments = np.where(cres == cre)[0]\n",
    "        if len(modelcre_experiments)>0:\n",
    "            projections = structure_data.reg_proj_norm[model_experiments][modelcre_experiments]\n",
    "            centroids = structure_data.centroids[model_experiments][modelcre_experiments]\n",
    "            #print((pairwise_distances(centroids, mask.coordinates)**2).shape)\n",
    "            predictions = get_nw_predictions(projections, pairwise_distances(centroids, mask.coordinates)**2, gamma)\n",
    "            output = np.mean(predictions, axis = 0)\n",
    "        else:\n",
    "            output = np.zeros(structure_data.reg_proj_norm.shape[1])\n",
    "            output[:] = np.nan  \n",
    "            \n",
    "    if surface == None and cre_model == False:\n",
    "        \n",
    "        if len(model_experiments)>0:\n",
    "            projections = structure_data.reg_proj_norm[model_experiments]\n",
    "            centroids = structure_data.centroids[model_experiments]\n",
    "            predictions = get_nw_predictions(projections, pairwise_distances(centroids, mask.coordinates)**2, gamma)\n",
    "            output = np.mean(predictions, axis = 0)\n",
    "        else:\n",
    "            output = np.zeros(structure_data.reg_proj_norm.shape[1])\n",
    "            output[:] = np.nan  \n",
    "        \n",
    "    return(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_structures = {}\n",
    "\n",
    "for sid in major_structure_ids:\n",
    "    summary_structures[sid] = connectivity_data.structure_datas[sid].summary_structures\n",
    "    \n",
    "connectivity_data.summary_structures = summary_structures#get_indices_2ormore(connectivity_data.leafs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wrong dist (not remove)... try to recreate old result\n",
    "frac_learn = np.ones(12)\n",
    "frac_learn[4] = .1\n",
    "sls = {}\n",
    "gammas_surface = 100000 * np.ones(12)\n",
    "gammas_surface[0] = 10\n",
    "gammas_surface[1] = 10\n",
    "gammas_surface[2] = 1000\n",
    "gammas_surface[3] = 100\n",
    "gammas_surface[4] = 20000\n",
    "gammas_surface[3] = 100\n",
    "gammas_surface[5] = 100\n",
    "gammas_surface[6] = 10\n",
    "gammas_surface[7] = 100\n",
    "gammas_surface[8] = 100\n",
    "gammas_surface[9] = 25\n",
    "gammas_surface[10] = 1000\n",
    "gammas_surface[11] = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "for m in range(12):\n",
    "    print(m)\n",
    "    sid = major_structure_ids[m]\n",
    "    connectivity_data.structure_datas[sid].loss_surface_cv_str = get_loss_surface_cv(connectivity_data.structure_datas[sid].reg_proj_norm,\n",
    "                                                                                                 connectivity_data.structure_datas[sid].centroids,\n",
    "                                                                                                 connectivity_data.creline[sid],\n",
    "                                                                                                 connectivity_data.summary_structures[sid],\n",
    "                                                                                                 frac_learn[m],\n",
    "                                                                                                 gamma = gammas_surface[m])\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for m in range(len(list(connectivity_data.structure_datas.keys()))):\n",
    "    sid = major_structure_ids[m]\n",
    "    print(m)\n",
    "    connectivity_data.structure_datas[sid].smoothed_losses_str = get_embedding_cv(surface = connectivity_data.structure_datas[sid].loss_surface_cv_str,\n",
    "                                                                              dists = pairwise_distances(connectivity_data.structure_datas[sid].centroids)**2,\n",
    "                                                                              cre_distances_cv = connectivity_data.structure_datas[sid].loss_surface_cv_str.cre_distances_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Crossval:\n",
    "    \n",
    "    def __init__(self):\n",
    "        2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connectivity_data.creleaf2_evalindices = get_eval_indices(connectivity_data.creleaf2_index_matrices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/alleninstitute/sambranch/mouse_connectivity_models/mcmodels/models/crossvalidation.py:107: RuntimeWarning: overflow encountered in exp\n",
      "  weights_i = np.exp(-dists_i * gamma)  # weights[i,:] / np.nansum(weights[i,:][otherindices])\n"
     ]
    }
   ],
   "source": [
    "twostage_leaf_creleaf2 = Crossval()\n",
    "\n",
    "#gammas = np.asarray([0.1,.5,1,2,10,20,40,100,1000,10000])\n",
    "gammas = np.asarray([0.001,0.01,0.1,1,10,100,1000,10000])\n",
    "#gammas = np.asarray([0.1,.5,1,2,10,20,40])\n",
    "loocvpredictions = {}\n",
    "reg_proj_norm= {}\n",
    "for sid in np.asarray(list(connectivity_data.structure_datas.keys())):\n",
    "    reg_proj_norm[sid ] = connectivity_data.structure_datas[sid].reg_proj_norm\n",
    "    loocvpredictions[sid] = get_nwloocv_predictions_multimodel_merge_dists(connectivity_data.structure_datas[sid].reg_proj_norm, \n",
    "                                                                                       connectivity_data.structure_datas[sid].smoothed_losses_str,#np.ones(losses.shape),#(losses - np.nanmin(losses))**2,#**6, \n",
    "                                                                                       #pds,                                       \n",
    "                                                                                       #(sls[sid] - np.nanmin(sls[sid]))**.5,\n",
    "                                                                                       gammas, \n",
    "                                                                                       connectivity_data.leaf2_index_matrices[sid], \n",
    "                                                                                       connectivity_data.leaf2_index_matrices[sid])                                  \n",
    "                                                                                       #screened_eval_indices[sid])                                  \n",
    "                                                                                       #indices_leaf2ormore[sid])\n",
    "            \n",
    "    \n",
    "    \n",
    "a= [list(range(len(gammas)))]\n",
    "keys = np.asarray(list(itertools.product(*a)))\n",
    "\n",
    "twostage_leaf_creleaf2.loocvpredictions = loocvpredictions\n",
    "twostage_leaf_creleaf2.losses = get_loss(reg_proj_norm, twostage_leaf_creleaf2.loocvpredictions,pred_ind = connectivity_data.creleaf2_evalindices, true_ind = connectivity_data.creleaf2_evalindices,keys = keys)\n",
    "twostage_leaf_creleaf2.bestgamma  = get_best_hyperparameters(twostage_leaf_creleaf2.losses,keys)\n",
    "twostage_leaf_creleaf2.meanloss = get_loss_best_hyp(twostage_leaf_creleaf2.losses, twostage_leaf_creleaf2.bestgamma)\n",
    "\n",
    "#meanloss_nw_leaf_leaf2wt\n",
    "#meanloss_nw_wtleaf_leaf2wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.03855325, 0.60238711, 0.10212501, 0.21615291, 0.15733546,\n",
       "        0.14594234, 0.16171284, 0.05002459, 0.2573415 , 0.14731652,\n",
       "        0.04591791, 0.30162975]), array([[5],\n",
       "        [0],\n",
       "        [5],\n",
       "        [5],\n",
       "        [5],\n",
       "        [4],\n",
       "        [5],\n",
       "        [5],\n",
       "        [4],\n",
       "        [4],\n",
       "        [5],\n",
       "        [4]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twostage_leaf_creleaf2.meanloss,twostage_leaf_creleaf2.bestgamma \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crelist = ['C57BL/6J']\n",
    "eval_cres = ['C57BL/6J']\n",
    "def get_aligned_ids(list1,list2):\n",
    "    output = np.empty(len(list1), dtype = int)\n",
    "    for i in range(len(ontological_order_leaves)):\n",
    "        output[ i] = np.intersect1d(st.ancestor_ids([list1[i]]), list2)[0]\n",
    "    return(output)\n",
    "eval_cres = ['C57BL/6J']\n",
    "ontological_order_leaves_majors = get_aligned_ids(ontological_order_leaves,major_structure_ids)\n",
    "ontological_order_leaves_summary = get_aligned_ids(ontological_order_leaves,ontological_order)\n",
    "\n",
    "\n",
    "# model_ordering_leaf = np.asarray([ontological_order_leaves_majors,ontological_order_leaves]).transpose()\n",
    "# model_ordering_summary = np.asarray([ontological_order_leaves_majors,ontological_order_leaves_summary]).transpose()\n",
    "model_ordering_leafs = np.asarray([ontological_order_leaves_majors,ontological_order_leaves]).transpose()\n",
    "model_ordering_summaries = np.asarray([ontological_order_leaves_majors,ontological_order_leaves_summary]).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surfaces_str = {}\n",
    "#surfaces_leaf = {}\n",
    "for m in range(12):\n",
    "    sid = major_structure_ids[m]\n",
    "    surfaces_str[sid] = connectivity_data.structure_datas[sid].loss_surface_cv_str\n",
    "    surfaces_str[sid].bestgamma =  gammas[twostage_leaf_creleaf2.bestgamma[m]]#gs[m]#gammas[twostage_str_creleaf2.bestgamma[m]]\n",
    "    \n",
    "for sid in major_structure_ids:\n",
    "    connectivity_data.structure_datas[sid].crelines = connectivity_data.creline[sid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 231.0\n",
      "1 955.0\n",
      "2 963.0\n",
      "3 238.0\n",
      "4 246.0\n",
      "5 250.0\n"
     ]
    }
   ],
   "source": [
    "#connectivity_data\n",
    "surfaces = surfaces_str\n",
    "cres = crelist\n",
    "structures_smooth = leafs\n",
    "structures_surface = summary_structures\n",
    "model_ordering_leafs = np.asarray([ontological_order_leaves_majors,ontological_order_leaves]).transpose()\n",
    "model_ordering_surfaces = np.asarray([ontological_order_leaves_majors,ontological_order_leaves_summary]).transpose()\n",
    "model_ordering_leafs = model_ordering_leafs[117:123]\n",
    "model_ordering_surfaces = model_ordering_summaries[117:123]\n",
    "eval_cres = eval_cres\n",
    "source_ordering = ontological_order_leaves[117:123]\n",
    "target_ordering= ontological_order_leaves\n",
    "connectivity_matrices = get_connectivity_matrices3(connectivity_data, surfaces, cres, structures_smooth,structures_surface, model_ordering_leafs, model_ordering_surfaces, source_ordering, target_ordering, eval_cres)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen_010719_5",
   "language": "python",
   "name": "allen_010719_5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
