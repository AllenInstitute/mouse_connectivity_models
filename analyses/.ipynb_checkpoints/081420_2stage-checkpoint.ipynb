{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "import pickle\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "workingdirectory = os.popen('git rev-parse --show-toplevel').read()[:-1]\n",
    "sys.path.append(workingdirectory)\n",
    "os.chdir(workingdirectory)\n",
    "\n",
    "import allensdk.core.json_utilities as ju\n",
    "from allensdk.core.mouse_connectivity_cache import MouseConnectivityCache\n",
    "\n",
    "from mcmodels.core import Mask,ModelData,VoxelModelCache\n",
    "from mcmodels.core.utils import get_structure_id, get_ordered_summary_structures,get_minorstructures,get_loss_paper\n",
    "from mcmodels.utils import nonzero_unique, unionize\n",
    "from mcmodels.core.experiment import get_voxeldata_msvd\n",
    "from mcmodels.models.crossvalidation import get_best_hyperparameters,get_loss_best_hyp,get_loocv_predictions,get_loss\n",
    "from mcmodels.core.utils import get_cre_status,get_minorstructure_dictionary,get_leaves_ontologicalorder\n",
    "from mcmodels.core.utils import get_regionalized_normalized_data\n",
    "from mcmodels.core.utils import get_connectivity\n",
    "from mcmodels.core.utils import get_ontological_order_leaf\n",
    "from mcmodels.core.utils import get_nw_loocv,get_wt_inds\n",
    "from mcmodels.core.utils import get_countvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "TOP_DIR = '/Users/samsonkoelle/alleninstitute/mcm_2020/mcm_updated/'\n",
    "INPUT_JSON = os.path.join(TOP_DIR, 'input_011520.json')\n",
    "EXPERIMENTS_EXCLUDE_JSON = os.path.join(TOP_DIR, 'experiments_exclude.json')\n",
    "FILE_DIR = '/Users/samsonkoelle/alleninstitute/mcm_2020/mcm_updated/'\n",
    "OUTPUT_DIR = os.path.join(FILE_DIR, 'output')\n",
    "\n",
    "input_data = ju.read(INPUT_JSON)\n",
    "manifest_file = input_data.get('manifest_file')\n",
    "manifest_file = os.path.join(TOP_DIR, manifest_file)\n",
    "experiments_exclude = ju.read(EXPERIMENTS_EXCLUDE_JSON)\n",
    "\n",
    "#its unclear why the hyperparameters are loaded from the output directory\n",
    "cache = VoxelModelCache(manifest_file=manifest_file)\n",
    "major_structures = input_data.get('structures')\n",
    "major_structure_ids = [get_structure_id(cache, s) for s in major_structures]\n",
    "data_info = pd.read_excel('/Users/samsonkoelle/alleninstitute/Whole Brain Cre Image Series_curation only.xlsx', 'all datasets curated_070919pull')\n",
    "data_info.set_index(\"id\", inplace=True)\n",
    "ontological_order = get_ordered_summary_structures(cache)\n",
    "\n",
    "mcc = MouseConnectivityCache(manifest_file = '../connectivity/mouse_connectivity_manifest.json')\n",
    "st = mcc.get_structure_tree()\n",
    "ai_map = st.get_id_acronym_map()\n",
    "ia_map = {value: key for key, value in ai_map.items()}\n",
    "\n",
    "#regionalize voxel model: compare with regional model\n",
    "#regional parameters\n",
    "cre = None\n",
    "eid_set=None\n",
    "high_res=False\n",
    "threshold_injection = False\n",
    "\n",
    "COARSE_STRUCTURE_SET_ID = 2\n",
    "DEFAULT_STRUCTURE_SET_IDS = tuple([COARSE_STRUCTURE_SET_ID])\n",
    "tree = cache.get_structure_tree()\n",
    "default_structures = tree.get_structures_by_set_id(DEFAULT_STRUCTURE_SET_IDS)\n",
    "default_structure_ids = [st['id'] for st in default_structures if st['id'] != 934]\n",
    "#cre= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "None\n",
      "703\n",
      "None\n",
      "1089\n",
      "None\n",
      "1097\n",
      "None\n",
      "315\n",
      "None\n",
      "313\n",
      "None\n",
      "354\n",
      "None\n",
      "698\n",
      "None\n",
      "771\n",
      "None\n",
      "803\n",
      "None\n",
      "477\n",
      "None\n",
      "549\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "msvds = {}\n",
    "#gammas = np.asarray([0.1])\n",
    "for sid in major_structure_ids:\n",
    "    print(sid)\n",
    "    voxel_data = ModelData(cache, sid)\n",
    "    print(cre)\n",
    "    experiment_ids = voxel_data.get_experiment_ids(experiments_exclude=experiments_exclude, cre=cre)\n",
    "    experiment_ids = np.asarray(list(experiment_ids))    \n",
    "    msvd = get_voxeldata_msvd(cache, sid,experiments_exclude,default_structure_ids,cre)\n",
    "    #msvd.l2losses, msvd.paperlosses,msvd.normspredict,msvd.normtrue = single_region_cv(msvd, gammas)\n",
    "    msvds[sid]  = msvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from mcmodels.regressors.nonparametric.nadaraya_watson import get_weights\n",
    "\n",
    "def get_best_hyperparameters(losses, keys):\n",
    "    \n",
    "    major_structure_ids = np.asarray(list(losses.keys()))\n",
    "    nms = len(major_structure_ids)\n",
    "    nkey = keys.shape[1]\n",
    "    output = np.empty((nms, nkey))\n",
    "    for m in range(nms):\n",
    "        print(m)\n",
    "        sid = major_structure_ids[m]\n",
    "        lvec = np.asarray([np.nanmean(losses[sid][key]) for key in keys])\n",
    "        if np.any(~np.isnan(lvec)):\n",
    "            output[m] = keys[np.nanargmin(lvec)]\n",
    "        # if len(np.where(np.isnan(np.nanmean(losses[sid][:,:], axis = 1)))[0]) < losses[sid].shape[0]:\n",
    "        #    output[m] = np.nanargmin(np.nanmean(losses[sid][:,:], axis = 1))\n",
    "\n",
    "    output = np.asarray(output, dtype=int)\n",
    "    return(output)\n",
    "\n",
    "#get where we actually modelled\n",
    "def get_eval_indices(eval_index_matrices):\n",
    "    eval_indices = {}\n",
    "    major_structure_ids = np.asarray(list(eval_index_matrices.keys()))\n",
    "    for sid in major_structure_ids:\n",
    "        eval_indices[sid] = np.where(eval_index_matrices[sid].sum(axis = 0) > 0)[0]\n",
    "    return(eval_indices)    \n",
    "\n",
    "\n",
    "\n",
    "def get_weights(eval_centroids, model_centroids, gamma):\n",
    "    weights = pairwise_kernels(X=eval_centroids, Y=model_centroids, metric='rbf', gamma=gamma, filter_params=True)\n",
    "    return (weights)\n",
    "\n",
    "\n",
    "def get_indices(ids):\n",
    "\n",
    "    ids_unique = np.unique(ids)\n",
    "    output = np.zeros((len(ids_unique), len(ids)), dtype = int)\n",
    "    for i in range(len(ids_unique)):\n",
    "        output[i,np.where(ids == ids_unique[i])[0] ] = 1\n",
    "    return(output)\n",
    "\n",
    "#get indices of firstlist in firstlisttest in categories defined by secondlist\n",
    "def get_indices2(firstlist, firstlisttest, secondlist):\n",
    "    \n",
    "    sl_unique = np.unique(secondlist)\n",
    "    output = np.zeros((len(sl_unique), len(secondlist)), dtype = int)\n",
    "    for i in range(len(sl_unique)):\n",
    "        output[i,np.intersect1d(np.where(np.isin(firstlist,firstlisttest))[0], np.where(secondlist == sl_unique[i])[0])] = 1\n",
    "    return(output)\n",
    "\n",
    "#nmodels = nleafs\n",
    "#populate each with experiments that share summary structure\n",
    "def get_indices_summaryinleaf(summarylist , leaflist):\n",
    "    \n",
    "    nexp = len(leaflist)\n",
    "    leaf_unique = np.unique(leaflist)\n",
    "    output = np.zeros((len(leaf_unique), nexp), dtype = int)\n",
    "    \n",
    "    for i in range(len(leaf_unique)):\n",
    "        \n",
    "        summary = summarylist[np.where(leaflist == leaf_unique[i])[0]][0]\n",
    "        output[i,np.where(summarylist == summary)[0]] = 1\n",
    "        \n",
    "        \n",
    "    return(output)\n",
    "\n",
    "#get predictions at all eval_indices using model_indices\n",
    "#if an eval_indices is also a model indice, leave it out of the model\n",
    "#if a model index is not an eval index, it never gets left out\n",
    "def get_nwloocv_predictions_singlemodel(projections, centroids, gamma, model_indices, eval_indices):\n",
    "  \n",
    "    eval_index_val = np.where(eval_indices == 1)[0]\n",
    "    model_index_val = np.where(model_indices == 1)[0]\n",
    "    \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    \n",
    "    nmod_ind = len(model_index_val)\n",
    "    neval = len(eval_index_val)\n",
    "    #nexp = centroids.shape[0]\n",
    "    predictions = np.empty(projections.shape)\n",
    "    #print(model_index_val.shape, eval_index_val.shape)\n",
    "\n",
    "    if len(model_index_val) > 0 and  len(eval_index_val) > 0:\n",
    "        weights = pairwise_kernels(centroids[model_index_val], centroids[eval_index_val], metric='rbf', gamma=gamma, filter_params=True) #get_weights(centroids, gamma)\n",
    "        for i in range(neval):\n",
    "            matchindex = np.where(model_index_val == eval_index_val[i])[0]\n",
    "            otherindices = np.setdiff1d(np.asarray(list(range(nmod_ind))), matchindex)         \n",
    "            #this order of operations is the fastest I found\n",
    "            weights_i = weights[:,i] / weights[:,i][otherindices].sum()\n",
    "            weights_i[matchindex] = 0\n",
    "            weights_i = np.asarray(weights_i, dtype=np.float32)\n",
    "            pred = np.dot(weights_i, projections[model_index_val])\n",
    "            predictions[eval_index_val[i]] = pred\n",
    "\n",
    "        \n",
    "    return(predictions)    \n",
    "\n",
    "def get_nwloocv_predictions_multimodel(projections, centroids, gammas, model_index_matrix, eval_index_matrix):\n",
    "    \n",
    "\n",
    "    \n",
    "    ntargets = projections.shape[1]\n",
    "    nexp = projections.shape[0]\n",
    "    nmodels = model_index_matrix.shape[0]\n",
    "    ngammas = len(gammas)\n",
    "    \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    predictions = np.empty((nmodels, ngammas, nexp, ntargets))\n",
    "    \n",
    "    \n",
    "    for m in range(nmodels):\n",
    "        #print('m', m, len(np.where(model_index_matrix[m] ==1)[0]))\n",
    "        predictions[m] = np.asarray([get_nwloocv_predictions_singlemodel(projections, centroids, gammas[g], model_index_matrix[m], eval_index_matrix[m]) for g in range(ngammas)])\n",
    "    \n",
    "    return(predictions)  \n",
    "\n",
    "def combine_predictions(predictions, eval_index_matrix):\n",
    "    \n",
    "    nmodels, ngammas, nexp, ntargets = predictions.shape\n",
    "    combined_predictions = np.empty((ngammas, nexp, ntargets))\n",
    "    for m in range(nmodels):\n",
    "        combined_predictions[:,np.where(eval_index_matrix[m] == 1)[0]] = predictions[m][:,np.where(eval_index_matrix[m] == 1)[0]]\n",
    "        \n",
    "    return(combined_predictions)\n",
    "\n",
    "def get_nwloocv_predictions_multimodel_merge(projections, centroids, gammas, model_index_matrix, eval_index_matrix):\n",
    "    \n",
    "    predictions_unmerged = get_nwloocv_predictions_multimodel(projections, centroids, gammas, model_index_matrix, eval_index_matrix)\n",
    "    print(predictions_unmerged.shape)\n",
    "    predictions_merged = combine_predictions(predictions_unmerged, eval_index_matrix)\n",
    "    \n",
    "    return(predictions_merged)\n",
    "\n",
    "#we should not pass model_index_matrices that are identical to eval_index_matrices and have only 1 element per model\n",
    "#although in principal we could do automatically in the cross validation code \n",
    "#we would rather do it explicitly to ensure identical indexing b/w experiments\n",
    "#if we only have 1 model index we will remove the model index from eval indices\n",
    "def screen_indices(model_indices, eval_indices):\n",
    "    \n",
    "    eval_indices2 = eval_indices.copy()\n",
    "    mod_loc = np.where(model_indices == 1)[0]\n",
    "    if len(mod_loc) == 1:\n",
    "        eval_indices2[mod_loc] = 0\n",
    "    return(eval_indices2)\n",
    "\n",
    "#this could result in an empty eval index i.e. certain indices having no prediction.  catch later\n",
    "#can merge (sum) the index matrix to see where predictions are actually generated\n",
    "def screen_index_matrices(model_index_matrices, eval_index_matrices):\n",
    "    \n",
    "    #alter eval_indices to remove model index in cases where there is only one experiment in the model\n",
    "    \n",
    "    nmodels = model_index_matrices.shape[0]\n",
    "    eval_index_matrices2 = eval_index_matrices.copy()\n",
    "    for m in range(nmodels):\n",
    "        eval_index_matrices2[m] = screen_indices(model_index_matrices[m], eval_index_matrices[m])\n",
    "    \n",
    "    return(eval_index_matrices2)\n",
    "\n",
    "#need code for removing experiments that have no model\n",
    "#this can happen when the model set is a subset of the evaluation set.\n",
    "#we will therefore generate predictions for a subset\n",
    "#given a leaf is included, the eval set is the same\n",
    "#however, we want to remove evals in leaves we don't have a wt for... of course one could say we are doing worse...\n",
    "#but we also have a fewer number of models\n",
    "#model_index_matrices are the indices of the leafs'\n",
    "#indices_wtinleaf are the wild types\n",
    "#need to make sure we dont have leafs with only 1 experiment\n",
    "\n",
    "\n",
    "def screen_index_matrices2(model_index_matrices, eval_index_matrices):\n",
    "    \n",
    "    #alter model and eval matrices to be nonzero only when there are at least two experiments in the model\n",
    "    #it can be useful for when model_index_matrices is a subset of eval_index_matrices\n",
    "    #nmodels = model_index_matrices.shape[0]\n",
    "    include_per_model = model_index_matrices.sum(axis= 1)\n",
    "    to_exclude = np.where(include_per_model <= 1)[0]\n",
    "    #to_include = np.where(include_per_model > 0)[0]\n",
    "    \n",
    "    model_index_matrices2 = model_index_matrices.copy()\n",
    "    eval_index_matrices2 = eval_index_matrices.copy()\n",
    "    model_index_matrices2[to_exclude] = 0\n",
    "    eval_index_matrices2[to_exclude] = 0\n",
    "    \n",
    "    return(model_index_matrices2, eval_index_matrices2)\n",
    "\n",
    "def screen_index_matrices3(model_index_matrices, eval_index_matrices):\n",
    "    \n",
    "    #alter model and eval matrices to be nonzero only when there are at least one experiments in the model\n",
    "    #it can be useful for when model_index_matrices is a subset of eval_index_matrices\n",
    "    #nmodels = model_index_matrices.shape[0]\n",
    "    include_per_model = model_index_matrices.sum(axis= 1)\n",
    "    to_exclude = np.where(include_per_model < 1)[0]\n",
    "    #to_include = np.where(include_per_model > 0)[0]\n",
    "    \n",
    "    model_index_matrices2 = model_index_matrices.copy()\n",
    "    eval_index_matrices2 = eval_index_matrices.copy()\n",
    "    model_index_matrices2[to_exclude] = 0\n",
    "    eval_index_matrices2[to_exclude] = 0\n",
    "    \n",
    "    to_remove = np.where(include_per_model == 1)[0]\n",
    "    eval_index_matrices2[to_remove] = 0\n",
    "    \n",
    "    return(model_index_matrices2, eval_index_matrices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices_leaf = {}\n",
    "indices_wtinleaf = {}\n",
    "indices_wtleaf = {}\n",
    "indices_summary = {}\n",
    "indices_summaryinleaf = {}\n",
    "indices_major = {}\n",
    "indices_majorinleaf = {}\n",
    "indices_leaf2ormore = {}\n",
    "indices_wtinleaf2ormore = {}\n",
    "indices_leaf_reduced = {}\n",
    "indices_wtinleaf_reduced = {}\n",
    "indices_wt_leaf2ormore = {}\n",
    "indices_leaf2ormore_wt = {}\n",
    "indices_wt1ormore_leaf2ormore = {}\n",
    "indices_leaf2ormore_wt1ormore = {}\n",
    "for sid in major_structure_ids:\n",
    "    \n",
    "    #wt_leaf on leaf\n",
    "\n",
    "    #get the indices of experiments sharing leafs (nmodels is number of leafs)\n",
    "    indices_leaf[sid] = get_indices(leafs[sid]) #eval_indices\n",
    "    #indices_creleaf = get_indices(leafs[sid])\n",
    "    \n",
    "    #get the indices of the wts in the leaf (nmodels is number of leafs)\n",
    "    indices_wtinleaf[sid] = get_indices2(creline[sid], np.asarray(['C57BL/6J']),leafs[sid]) #model_indices\n",
    "    \n",
    "    #get indices of experiments sharing summary structure x cre combination (nmodel is number of cre x leaf combinations)\n",
    "    #indices_wtleaf[sid] = get_indices(creleafs_merged[sid])\n",
    "    \n",
    "    #get indices of experiments sharing summary structure(nmodel is number of summary structures)\n",
    "    indices_summary[sid] = get_indices(experiments_minor_structures[sid])\n",
    "    \n",
    "    #get indices of experiments sharing major structure(nmodel is number of summary structures)\n",
    "    indices_major[sid] = np.ones((1,experiments_minor_structures[sid].shape[0]))\n",
    "    \n",
    "    #get indices of experiments sharing same major structure as a leaf (nmodel is number of leafs)\n",
    "    indices_majorinleaf[sid] = np.ones((len(np.unique(leafs[sid])),experiments_minor_structures[sid].shape[0]))#get_indices2(np.ones(len(leafs[sid])), np.asarray([1]),leafs[sid]) #model_indices\n",
    "    \n",
    "    #get indices of experiments in same summary structure as a leaf (nmodel is number of leafs)\n",
    "    indices_summaryinleaf[sid] = get_indices_summaryinleaf(experiments_minor_structures[sid], leafs[sid])\n",
    "    \n",
    "    #evaluate models on leafs\n",
    "    #model_indices, eval_indices = indices_majorinleaf, indices_leaf\n",
    "    #model_indices, eval_indices = indices_summaryinleaf, indices_leaf\n",
    "    #this is the most restrictive of these 3, so eval_indices_leaf2ormore is the smallest eval set\n",
    "    indices_leaf2ormore[sid] = screen_index_matrices(indices_leaf[sid], indices_leaf[sid])\n",
    "    \n",
    "    #get indices of wt in leafs with 2 or more experiments, and experiments in those leafs\n",
    "    #indices_leaf2ormore_wt is a subset of indices_leaf2ormore since leafs need 2 wts\n",
    "    #honestly ould have just 1 experimnet as long as multiple leafs\n",
    "    #indices_wt_leaf2ormore should be the same as indices_wtinleaf2ormore\n",
    "    #indices_wt_leaf2ormore[sid], indices_leaf2ormore_wt[sid] \n",
    "    indices_wt1ormore_leaf2ormore[sid], indices_leaf2ormore_wt1ormore[sid] = screen_index_matrices3(indices_wtinleaf[sid], indices_leaf2ormore[sid])\n",
    "    \n",
    "    indices_wtinleaf2ormore[sid] = screen_index_matrices(indices_wtinleaf[sid], indices_wtinleaf[sid])\n",
    "    \n",
    "    #need to find explicitly so can be used in other experiments\n",
    "    #how do we line up with leaf model...\n",
    "    #reduced modelset.  also\n",
    "    #indices_wtinleaf_reduced, indices_leaf_reduced = screen_index_matrices2(indices_wtinleaf[sid], indices_leaf[sid])\n",
    "    indices_leaf_reduced[sid], indices_wtinleaf_reduced[sid]  = screen_index_matrices2( indices_leaf2ormore[sid],indices_wtinleaf[sid])\n",
    "\n",
    "\n",
    "    #if i want to only use indices_leaf_reduced as target in a different experiment, do i need to eliminate  \n",
    "    #\n",
    "    \n",
    "    \n",
    "    #leaf on leaf\n",
    "    #eval_indices = indices_leaf\n",
    "    #model_indices = indices_leaf\n",
    "\n",
    "    #leaf on wt_leaf\n",
    "    #model_indices = indices_leaf\n",
    "    #eval_indices = indices_wtinleaf\n",
    "\n",
    "    #summary on summary\n",
    "    #model_indices = get_indices(experiments_minor_structures[sid])\n",
    "    #eval_indices = get_indices(experiments_minor_structures[sid])\n",
    "\n",
    "    #creleaf precise (nmodels is number of creleaf combinations)\n",
    "    \n",
    "    #indices_wtleaf = get_indices(creleafs[sid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dictionaries of creline and leaf by experiment\n",
    "creline = get_cre_status(data_info, msvds)\n",
    "with open('data/info/leafs.pickle', 'rb') as handle:\n",
    "    leafs = pickle.load(handle)\n",
    "    \n",
    "#get dictionary of minor structures for each experiment in each major division\n",
    "#major division segregation is legacy code but convenient for fast cross validation in major division model\n",
    "experiments_minor_structures = get_minorstructure_dictionary(msvds, data_info)\n",
    "\n",
    "#get leaves in ontological order.  Where leafs don't exist, uses summary structure\n",
    "ontological_order_leaves = get_leaves_ontologicalorder(msvd, ontological_order)\n",
    "\n",
    "#Key isn't affected by which experiment we choose. This allows default masking to be inherited from the AllenSDK.\n",
    "key = list(msvd.experiments.keys())[0]\n",
    "\n",
    "#Identify keys denoting which voxels correspond to which structure in the ipsi and contra targets.\n",
    "#contra_targetkey = msvd.experiments[list(msvd.experiments.keys())[0]].projection_mask.get_key(structure_ids=ontological_order_leaves, hemisphere_id=1)\n",
    "#ipsi_targetkey = msvd.experiments[list(msvd.experiments.keys())[0]].projection_mask.get_key(structure_ids=ontological_order_leaves, hemisphere_id=2)\n",
    "contra_targetkey = msvd.experiments[key].projection_mask.get_key(structure_ids=ontological_order, hemisphere_id=1)\n",
    "ipsi_targetkey = msvd.experiments[key].projection_mask.get_key(structure_ids=ontological_order, hemisphere_id=2)\n",
    "\n",
    "#get average intensities of projection structures given ipsi and contra keys\n",
    "source_key = ontological_order_leaves #only relevant here when injection needs to be unionized, but currently a required argument\n",
    "msvds = get_regionalized_normalized_data(msvds,cache, source_key,ipsi_targetkey,contra_targetkey)\n",
    "\n",
    "#wt_2ormore = get_wt_inds(creline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'majorsum_homonormed2x_0812_leaf2'\n",
    "expfolder = workingdirectory +   '/data/' + name\n",
    "#os.mkdir(expfolder)\n",
    "#loss.to_csv(expfolder + 'losses')\n",
    "\n",
    "#msvds_save = {}\n",
    "for sid in major_structure_ids:\n",
    "    #np.save(expfolder +'/'+ str(sid) + 'reg_proj_vcount_norm_renorm',msvds[sid].reg_proj_vcount_norm_renorm)\n",
    "    msvds[sid].loocv_predictions_all = np.load(expfolder +'/'+ str(sid) + 'loocv_predictions_all.npy')\n",
    "\n",
    "    \n",
    "\n",
    "eval_indices_leaf2 = get_eval_indices(indices_leaf2ormore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_proj_vcount_norm_renorms = {}\n",
    "homo_leafs = {}\n",
    "for sid in major_structure_ids:\n",
    "    reg_proj_vcount_norm_renorms[sid ] = msvds[sid].reg_proj_vcount_norm_renorm\n",
    "    homo_leafs[sid] = msvds[sid].loocv_predictions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rediduals(homo_leafs, reg_proj_vcount_norm_renorms):\n",
    "    \n",
    "    residuals = {}\n",
    "    major_structure_ids = np.asarray(list(homo_leafs.keys()))\n",
    "    for sid in major_structure_ids:\n",
    "        residuals[sid] = np.squeeze(homo_leafs[sid]) - reg_proj_vcount_norm_renorms[sid]\n",
    "    return(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 577)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals[sid].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = get_rediduals(homo_leafs, reg_proj_vcount_norm_renorms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 5, 36, 577)\n",
      "(5, 5, 7, 577)\n",
      "(18, 5, 122, 577)\n",
      "(21, 5, 85, 577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: overflow encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: overflow encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: overflow encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119, 5, 1128, 577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 5, 68, 577)\n",
      "(19, 5, 46, 577)\n",
      "(9, 5, 35, 577)\n",
      "(13, 5, 33, 577)\n",
      "(7, 5, 30, 577)\n",
      "(8, 5, 78, 577)\n",
      "(29, 5, 83, 577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "gammas = np.asarray([0.1,.5,1,2,10])\n",
    "for sid in major_structure_ids:\n",
    " \n",
    "    \n",
    "    \n",
    "    msvds[sid].loocv_predictions_residuals = get_nwloocv_predictions_multimodel_merge(residuals[sid], \n",
    "                                                                                       msvds[sid].centroids, \n",
    "                                                                                       gammas, \n",
    "                                                                                       indices_leaf2ormore[sid], \n",
    "                                                                                       indices_leaf2ormore[sid])\n",
    "\n",
    "#     msvds[sid].loocv_predictions_wtleaf_wtleaf2 = get_nwloocv_predictions_multimodel_merge(msvds[sid].reg_proj_vcount_norm_renorm, \n",
    "#                                                                                        msvds[sid].centroids, \n",
    "#                                                                                        gammas, \n",
    "#                                                                                        indices_wtinleaf2ormore[sid], \n",
    "#                                                                                         indices_wtinleaf2ormore[sid])\n",
    "\n",
    "    \n",
    "    #predict all leafs using cre in that leaf \n",
    "    #indices_leaf2ormore is a reduced set of indices_leaf so that we dont have the evaluator be the only predictor\n",
    "    #if we are evaluating leaf2ormore with cres we wont have this issue any more than already.\n",
    "    #however, we have to not evaluate when there are no cres in the leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nw_residuals = {}\n",
    "cor_pred = {}\n",
    "homo_leafs_cor = {}\n",
    "for sid in major_structure_ids:\n",
    "    #reg_proj_vcount_norm_renorms[sid ] = msvds[sid].reg_proj_vcount_norm_renorm\n",
    "    nw_residuals[sid] =  msvds[sid].loocv_predictions_residuals\n",
    "    cor_pred[sid] = nw_residuals[sid] + homo_leafs[sid]\n",
    "    nfact = np.swapaxes(np.swapaxes(np.asarray([np.linalg.norm(cor_pred[sid], axis = 2)]*cor_pred[sid].shape[2])**(-1),1,2), 0,2)\n",
    "    #nwloocv_wtleaf_leaf2[sid]  =  msvds[sid].loocv_predictions_wtleaf_leaf2\n",
    "    homo_leafs_cor[sid] = cor_pred[sid] * nfact\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a= [list(range(5))]\n",
    "keys = np.asarray(list(itertools.product(*a)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices_leaf = {}\n",
    "indices_leaf2ormore = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sid in major_structure_ids:\n",
    "    \n",
    "    #wt_leaf on leaf\n",
    "\n",
    "    #get the indices of experiments sharing leafs (nmodels is number of leafs)\n",
    "    indices_leaf[sid] = get_indices(leafs[sid]) #eval_indices\n",
    "    #indices_creleaf = get_indices(leafs[sid])\n",
    "      #evaluate models on leafs\n",
    "    #model_indices, eval_indices = indices_majorinleaf, indices_leaf\n",
    "    #model_indices, eval_indices = indices_summaryinleaf, indices_leaf\n",
    "    #this is the most restrictive of these 3, so eval_indices_leaf2ormore is the smallest eval set\n",
    "    indices_leaf2ormore[sid] = screen_index_matrices(indices_leaf[sid], indices_leaf[sid])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "eval_indices_leaf2ormore = get_eval_indices(indices_leaf2ormore)\n",
    "losses_leaf_leaf2 = get_loss(reg_proj_vcount_norm_renorms, homo_leafs_cor,pred_ind = eval_indices_leaf2ormore, true_ind = eval_indices_leaf2ormore,keys = keys)\n",
    "best_gamma_leaf_leaf2 = get_best_hyperparameters(losses_leaf_leaf2,keys)\n",
    "meanloss_nw_leaf_leaf2 = get_loss_best_hyp(losses_leaf_leaf2, best_gamma_leaf_leaf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45111214, 0.10055048, 0.46353552, 0.29852924, 0.42926426,\n",
       "       0.5279106 , 0.4321668 , 0.51323082, 0.19049975, 0.40648581,\n",
       "       0.7094603 , 0.34155919])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanloss_nw_leaf_leaf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(meanloss_nw_leaf_leaf2[[4,7,2,1,10,9,11,3,5,8,6,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1128, 577)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo_leafs_cor[315].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nw_residuals is a best guess of the residual using nw\n",
    "#add these residuals to our homo predictions and renormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3559ef31a339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindices_leaf2ormore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 512"
     ]
    }
   ],
   "source": [
    "indices_leaf2ormore[sid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen_010719_5",
   "language": "python",
   "name": "allen_010719_5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
