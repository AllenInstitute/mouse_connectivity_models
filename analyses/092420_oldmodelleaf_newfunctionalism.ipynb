{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "import pickle\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import math\n",
    "\n",
    "workingdirectory = os.popen('git rev-parse --show-toplevel').read()[:-1]\n",
    "sys.path.append(workingdirectory)\n",
    "os.chdir(workingdirectory)\n",
    "\n",
    "import allensdk.core.json_utilities as ju\n",
    "from allensdk.core.mouse_connectivity_cache import MouseConnectivityCache\n",
    "\n",
    "from mcmodels.regressors import NadarayaWatson\n",
    "from mcmodels.core import Mask,ModelData,VoxelModelCache\n",
    "from mcmodels.core.utils import get_structure_id, get_ordered_summary_structures,get_minorstructures,get_loss_paper\n",
    "from mcmodels.utils import nonzero_unique, unionize\n",
    "from mcmodels.core.experiment import get_voxeldata_msvd\n",
    "from mcmodels.models.crossvalidation import get_best_hyperparameters,get_loss_best_hyp,get_loocv_predictions,get_loss\n",
    "from mcmodels.core.utils import get_cre_status,get_minorstructure_dictionary,get_leaves_ontologicalorder\n",
    "from mcmodels.core.utils import get_regionalized_normalized_data\n",
    "from mcmodels.core.utils import get_connectivity\n",
    "from mcmodels.core.utils import get_ontological_order_leaf\n",
    "from mcmodels.core.utils import get_nw_loocv,get_wt_inds\n",
    "from mcmodels.core.utils import get_countvec\n",
    "from mcmodels.core.utils import get_injection_hemisphere_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "TOP_DIR = '/Users/samsonkoelle/alleninstitute/mcm_2020/mcm_updated/'\n",
    "INPUT_JSON = os.path.join(TOP_DIR, 'input_011520.json')\n",
    "EXPERIMENTS_EXCLUDE_JSON = os.path.join(TOP_DIR, 'experiments_exclude.json')\n",
    "FILE_DIR = '/Users/samsonkoelle/alleninstitute/mcm_2020/mcm_updated/'\n",
    "OUTPUT_DIR = os.path.join(FILE_DIR, 'output')\n",
    "\n",
    "input_data = ju.read(INPUT_JSON)\n",
    "manifest_file = input_data.get('manifest_file')\n",
    "manifest_file = os.path.join(TOP_DIR, manifest_file)\n",
    "experiments_exclude = ju.read(EXPERIMENTS_EXCLUDE_JSON)\n",
    "\n",
    "#its unclear why the hyperparameters are loaded from the output directory\n",
    "cache = VoxelModelCache(manifest_file=manifest_file)\n",
    "major_structures = input_data.get('structures')\n",
    "major_structure_ids = [get_structure_id(cache, s) for s in major_structures]\n",
    "data_info = pd.read_excel('/Users/samsonkoelle/alleninstitute/Whole Brain Cre Image Series_curation only.xlsx', 'all datasets curated_070919pull')\n",
    "data_info.set_index(\"id\", inplace=True)\n",
    "ontological_order = get_ordered_summary_structures(cache)\n",
    "\n",
    "mcc = MouseConnectivityCache(manifest_file = '../connectivity/mouse_connectivity_manifest.json')\n",
    "st = mcc.get_structure_tree()\n",
    "ai_map = st.get_id_acronym_map()\n",
    "ia_map = {value: key for key, value in ai_map.items()}\n",
    "\n",
    "#regionalize voxel model: compare with regional model\n",
    "#regional parameters\n",
    "cre = None\n",
    "eid_set=None\n",
    "high_res=False\n",
    "threshold_injection = False\n",
    "\n",
    "COARSE_STRUCTURE_SET_ID = 2\n",
    "DEFAULT_STRUCTURE_SET_IDS = tuple([COARSE_STRUCTURE_SET_ID])\n",
    "tree = cache.get_structure_tree()\n",
    "default_structures = tree.get_structures_by_set_id(DEFAULT_STRUCTURE_SET_IDS)\n",
    "default_structure_ids = [st['id'] for st in default_structures if st['id'] != 934]\n",
    "#cre= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ccf_data(cache, experiment_id):\n",
    "\n",
    "    eid_data = ExperimentData(experiment_id)\n",
    "    eid_data.data_quality_mask = cache.get_data_mask(experiment_id)[0]\n",
    "    eid_data.injection_signal = cache.get_injection_density(experiment_id)[0]\n",
    "    eid_data.injection_fraction = cache.get_injection_fraction(experiment_id)[0]\n",
    "    eid_data.projection_signal = cache.get_projection_density(experiment_id)[0]\n",
    "    return(eid_data)\n",
    "#     return {\n",
    "#         \"data_quality_mask\" : cache.get_data_mask(experiment_id)[0],\n",
    "#         \"injection_signal\" : cache.get_injection_density(experiment_id)[0],\n",
    "#         \"injection_fraction\" : cache.get_injection_fraction(experiment_id)[0],\n",
    "#         \"projection_signal\" : cache.get_projection_density(experiment_id)[0]\n",
    "#     }\n",
    "\n",
    "def get_connectivity_data(cache, structure_ids, experiments_exclude, remove_injection = False):\n",
    "\n",
    "    connectivity_data = ConnectivityData()\n",
    "    for sid in structure_ids:\n",
    "        print(sid)\n",
    "        sid_data = StructureData(sid)\n",
    "        #deprecated language\n",
    "        model_data = ModelData(cache, sid)\n",
    "        sid_data.eids = model_data.get_experiment_ids(experiments_exclude=experiments_exclude, cre=None)\n",
    "        for eid in sid_data.eids:\n",
    "            \n",
    "            eid_data = get_ccf_data(cache, eid)#ExperimentData(eid)\n",
    "            eid_data.data_mask_tolerance = .5\n",
    "            #ccf_data = get_ccf_data(cache, eid)\n",
    "            eid_data.injection_signal_true = eid_data.injection_signal * eid_data.injection_fraction\n",
    "            if remove_injection == True:\n",
    "                pass #remove injection fraction from projection\n",
    "            #injection_signal should = projection_signal in some locations (nonzero)\n",
    "            #why do we use partial?\n",
    "            #mask_func = partial(_mask_data_volume,data_mask=eid_data.data_mask,tolerance=eid_data.data_mask_tolerance)\n",
    "            eid_data.injection_qmasked  = _mask_data_volume(eid_data.injection_signal_true,eid_data.data_quality_mask,eid_data.data_mask_tolerance)\n",
    "            eid_data.projection_qmasked  = _mask_data_volume(eid_data.projection_signal,eid_data.data_quality_mask,eid_data.data_mask_tolerance) #mask_func(eid_data.projection_signal) \n",
    "            #eid_data.centroid = compute_centroid(eid_data.injection_qmasked)\n",
    "            sid_data.experiment_datas[eid] = eid_data\n",
    "        connectivity_data.structure_datas[sid] = sid_data\n",
    "    return(connectivity_data)\n",
    "\n",
    "def get_data_matrices(connectivity_data):\n",
    "    \n",
    "    structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "    for sid in structure_ids:\n",
    "        experiment_ids = np.asarray(list(connectivity_data.structure_datas[sid].experiment_datas.keys()))\n",
    "        connectivity_data.structure_datas[sid].injection_mask = Mask.from_cache(cache,structure_ids=[sid],hemisphere_id=2)\n",
    "        connectivity_data.structure_datas[sid].projection_mask = Mask.from_cache(cache,structure_ids=default_structure_ids, hemisphere_id=3)\n",
    "        for eid in experiment_ids:\n",
    "            connectivity_data.structure_datas[sid].experiment_datas[eid].injection_vec = connectivity_data.structure_datas[sid].injection_mask.mask_volume(connectivity_data.structure_datas[sid].experiment_datas[eid].injection_qmasked)\n",
    "            connectivity_data.structure_datas[sid].experiment_datas[eid].projection_vec = connectivity_data.structure_datas[sid].projection_mask.mask_volume(connectivity_data.structure_datas[sid].experiment_datas[eid].projection_qmasked)\n",
    "        connectivity_data.structure_datas[sid].injections = np.asarray([connectivity_data.structure_datas[sid].experiment_datas[eid].injection_vec for eid in connectivity_data.structure_datas[sid].eids])\n",
    "        connectivity_data.structure_datas[sid].projections = np.asarray([connectivity_data.structure_datas[sid].experiment_datas[eid].projection_vec for eid in connectivity_data.structure_datas[sid].eids])\n",
    "        connectivity_data.structure_datas[sid].centroids = np.asarray([connectivity_data.structure_datas[sid].experiment_datas[eid].centroid for eid in connectivity_data.structure_datas[sid].eids])\n",
    "        \n",
    "        \n",
    "    return(connectivity_data)\n",
    "\n",
    "\n",
    "def get_regionalized_normalized_data(connectivity_data, cache, source_order, ipsi_key, contra_key): #experiments_minor_structures):\n",
    "    '''\n",
    "    :param msvds: Class dictionary holding data\n",
    "    :param cache: AllenSDK cache\n",
    "    :param source_order: Source key (tautologically ipsilateral due to hemisphere mirroring)\n",
    "    :param ipsi_key: Ipsilateral target key\n",
    "    :param contra_key:  Contralateral target key\n",
    "    :return: msvds: Class dictionary holding average data\n",
    "    '''\n",
    "    major_structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "    for sid in major_structure_ids:\n",
    "        # print()\n",
    "        structure_data = connectivity_data.structure_datas[sid]\n",
    "        #nexp = msvd.projections.shape[0]\n",
    "\n",
    "        #minor_structures = np.unique(experiments_minor_structures[sid])\n",
    "        #nmins = len(minor_structures)\n",
    "\n",
    "        projections = structure_data.projections\n",
    "        ipsi_proj = unionize(projections, ipsi_key)\n",
    "        contra_proj = unionize(projections, contra_key)\n",
    "        reg_proj = np.hstack([ipsi_proj, contra_proj])\n",
    "        structure_data.reg_proj = reg_proj\n",
    "\n",
    "        ipsi_target_regions, ipsi_target_counts = nonzero_unique(ipsi_key, return_counts=True)\n",
    "        contra_target_regions, contra_target_counts = nonzero_unique(contra_key, return_counts=True)\n",
    "        target_counts = np.concatenate([ipsi_target_counts, contra_target_counts])\n",
    "        reg_proj_vcount_norm = np.divide(reg_proj, target_counts[np.newaxis, :])\n",
    "        structure_data.reg_proj_vcount_norm = reg_proj_vcount_norm\n",
    "        structure_data.reg_proj_vcount_norm_renorm = reg_proj_vcount_norm / np.expand_dims(np.linalg.norm(reg_proj_vcount_norm, axis=1), 1)\n",
    "        structure_data.reg_proj_norm = reg_proj / np.expand_dims(np.linalg.norm(reg_proj, axis = 1),1)\n",
    "        #structure_data.reg_proj_norm_vcount_norm = np.divide(structure_data.reg_proj_norm, target_counts[np.newaxis, :]) # / np.expand_dims(np.linalg.norm(reg_proj_vcount_norm, axis=1), 1)\n",
    "                \n",
    "        source_mask = Mask.from_cache(cache, structure_ids=[sid], hemisphere_id=2)\n",
    "        source_key = source_mask.get_key(structure_ids=source_order)\n",
    "        source_regions, source_counts = nonzero_unique(source_key, return_counts=True)\n",
    "\n",
    "        injections = structure_data.injections\n",
    "        reg_ipsi_inj = unionize(injections, source_key)\n",
    "        structure_data.reg_inj = reg_ipsi_inj\n",
    "        reg_inj_vcount_norm = np.divide(reg_ipsi_inj, source_counts[np.newaxis, :])\n",
    "        structure_data.reg_inj_vcount_norm = reg_inj_vcount_norm\n",
    "        \n",
    "        structure_data.reg_proj_vcount_norm_injnorm = reg_proj_vcount_norm / np.expand_dims(np.linalg.norm(reg_inj_vcount_norm, axis=1), 1)\n",
    "        connectivity_data.structure_datas[sid] = structure_data\n",
    "        #msvd.reg_proj_vcountnorm_totalnorm =\n",
    "    connectivity_data.ipsi_target_regions = ipsi_target_regions\n",
    "    connectivity_data.contra_target_regions = contra_target_regions        \n",
    "    connectivity_data.target_regions = np.concatenate([ipsi_target_regions, contra_target_regions])\n",
    "    return (connectivity_data)\n",
    "\n",
    "# def get_normalized_data(connectivity_data, normalization = None):\n",
    "\n",
    "#     structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "#     for sid in structure_ids:\n",
    "#         if normalization == None:\n",
    "#             pass\n",
    "#         if normalization == 'injection':\n",
    "#             nc = np.sum(connectivity_data.structure_datas[sid].injections, axis = 1)\n",
    "#             connectivity_data.structure_datas[sid].projection_normed = connectivity_data.structure_datas[sid].projections / nc\n",
    "#         if normalization == 'total':\n",
    "#             nc = np.sum(conn_data[sid].projections, axis = 1)\n",
    "#             connectivity_data.structure_datas[sid].projection_normed = connectivity_data.structure_datas[sid].projections / nc\n",
    "\n",
    "#     return(connectivity_data)\n",
    "\n",
    "\n",
    "def compute_centroid(injection_density):\n",
    "    \"\"\"Computes centroid in index coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    injection_density : array, shape (x_ccf, y_ccf, z_ccf)\n",
    "        injection_density data volume.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        centroid of injection_density in index coordinates.\n",
    "    \"\"\"\n",
    "    nnz = injection_density.nonzero()\n",
    "    coords = np.vstack(nnz)\n",
    "\n",
    "    return np.dot(coords, injection_density[nnz]) / injection_density.sum()\n",
    "\n",
    "def get_cre_status(data_info, connectivity_data):\n",
    "    major_structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "    exps = np.asarray(data_info.index.values , dtype = np.int)\n",
    "    creline = {}\n",
    "    for sid in major_structure_ids:\n",
    "        experiment_ids = np.asarray(list(connectivity_data.structure_datas[sid].experiment_datas.keys()))\n",
    "        nexp = len(experiment_ids)\n",
    "        creline[sid] = np.zeros(nexp, dtype = object)\n",
    "        for i in range(len(experiment_ids)):\n",
    "            index = np.where(exps == experiment_ids[i])[0][0]\n",
    "            creline[sid][i] = data_info['transgenic-line'].iloc[index]\n",
    "    return(creline)\n",
    "\n",
    "\n",
    "class ConnectivityData():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.structure_datas = {}    \n",
    "    \n",
    "    def get_injection_hemisphere_ids(self):\n",
    "        \n",
    "        structure_datas = self.structure_datas\n",
    "        \n",
    "        \n",
    "        for sid in list(structure_datas.keys()):\n",
    "            structure_datas[sid].get_injection_hemisphere_ids()\n",
    "            \n",
    "        self.structure_datas = structure_datas\n",
    "        \n",
    "        \n",
    "    def align(self):\n",
    "        \n",
    "        structure_datas= self.structure_datas\n",
    "        \n",
    "        for sid in list(structure_datas.keys()):\n",
    "            structure_datas[sid].align()\n",
    "        self.structure_datas = structure_datas\n",
    "\n",
    "    def get_centroids(self):\n",
    "        \n",
    "        structure_datas = self.structure_datas\n",
    "        \n",
    "        for sid in list(structure_datas.keys()):\n",
    "            structure_datas[sid].get_centroids()\n",
    "            \n",
    "        self.structure_datas = structure_datas\n",
    "        \n",
    "class StructureData():\n",
    "    \n",
    "    def __init__(self, sid):\n",
    "        self.experiment_datas = {}\n",
    "        \n",
    "    def get_injection_hemisphere_ids(self):   \n",
    "        \n",
    "        experiment_datas = self.experiment_datas\n",
    "        \n",
    "        for eid in list(experiment_datas.keys()):\n",
    "             experiment_datas[eid].injection_hemisphere_id = get_injection_hemisphere_id(experiment_datas[eid].injection_qmasked, majority=True)\n",
    "        self.experiment_datas = experiment_datas\n",
    "        \n",
    "    def align(self):\n",
    "        \n",
    "        experiment_datas = self.experiment_datas\n",
    "        \n",
    "        for eid in list(experiment_datas.keys()):\n",
    "            if experiment_datas[eid].injection_hemisphere_id == 1:\n",
    "                experiment_datas[eid].flip()\n",
    "        self.experiment_datas = experiment_datas\n",
    "\n",
    "    def get_centroids(self):\n",
    "        \n",
    "        experiment_datas = self.experiment_datas\n",
    "        \n",
    "        for eid in list(experiment_datas.keys()):\n",
    "            experiment_datas[eid].centroid = compute_centroid(experiment_datas[eid].injection_qmasked)\n",
    "        self.experiment_datas = experiment_datas        \n",
    "        \n",
    "class ExperimentData():\n",
    "    \n",
    "    def __init__(self, eid):\n",
    "        2+2\n",
    "    \n",
    "    #def get_injection_hemisphere():\n",
    "        \n",
    "        \n",
    "    def flip(self):\n",
    "        \"\"\"Reflects experiment along midline.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self - flipped experiment\n",
    "        \"\"\"\n",
    "\n",
    "        self.injection_qmasked = self.injection_qmasked[..., ::-1]\n",
    "        self.projection_qmasked = self.projection_qmasked[..., ::-1]\n",
    "        self.injection_signal = self.injection_signal[..., ::-1]\n",
    "        self.projection_signal = self.projection_signal[..., ::-1]\n",
    "        self.injection_signal_true = self.injection_signal_true[..., ::-1]\n",
    "        #self.projection_signal_true = self.projection_signal_true[..., ::-1]\n",
    "        self.injection_fraction = self.injection_fraction[..., ::-1]\n",
    "        self.data_quality_mask = self.data_quality_mask[...,::-1]\n",
    "        \n",
    "        #return self        \n",
    "\n",
    "def _mask_data_volume(data_volume, data_mask, tolerance=0.0):\n",
    "    \"\"\"Masks a given data volume in place.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_volume : array, shape (x_ccf, y_ccf, z_ccf)\n",
    "        Data volume to be masked.\n",
    "\n",
    "    data_mask : array, shape (x_ccf, y_ccf, z_ccf)\n",
    "        data_mask for given experiment (values in [0,1])\n",
    "        See allensdk.core.mouse_connectivity_cache for more info.\n",
    "\n",
    "    tolerance : float, optional (default=0.0)\n",
    "        tolerance with which to define bad voxels in data_mask.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_volume\n",
    "        data_volume parameter masked in place.\n",
    "\n",
    "    \"\"\"\n",
    "    if data_volume.shape != data_mask.shape:\n",
    "        raise ValueError(\"data_volume (%s) and data_mask (%s) must be the same \"\n",
    "                         \"shape!\" % (data_volume.shape, data_mask.shape))\n",
    "\n",
    "    # mask data volume\n",
    "    data_volume[data_mask < tolerance] = 0.0\n",
    "\n",
    "    return data_volume\n",
    "\n",
    "\n",
    "def get_summarystructure_dictionary(connectivity_data, data_info):\n",
    "    summarystructure_dictionary = {}\n",
    "    major_structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "    for sid in major_structure_ids:\n",
    "        eids = np.asarray(list(connectivity_data.structure_datas[sid].experiment_datas.keys()))\n",
    "        summarystructure_dictionary[sid] = get_minorstructures(eids, data_info)\n",
    "    return (summarystructure_dictionary)\n",
    "\n",
    "\n",
    "def get_leaves_ontologicalorder(connectivity_data, ontological_order):\n",
    "    '''\n",
    "\n",
    "    :param msvd:\n",
    "    :param ontological_order:\n",
    "    :return: The leaf order associated with the 'ontological_order' of summary structures\n",
    "    '''\n",
    "    sid0 = list(connectivity_data.structure_datas.keys())[0]\n",
    "    #eid0 = list(connectivity_data.structure_datas[sid0].experiment_datas.keys())[0]\n",
    "    levs = connectivity_data.structure_datas[sid0].projection_mask.reference_space.structure_tree.child_ids(\n",
    "        ontological_order)\n",
    "    flat_list = np.asarray([item for sublist in levs for item in sublist])\n",
    "\n",
    "    nss = len(levs)\n",
    "    leavves = np.asarray([])\n",
    "    for i in range(nss):\n",
    "        if len(levs[i]) > 0:\n",
    "            leavves = np.append(leavves, levs[i])\n",
    "        else:\n",
    "            leavves = np.append(leavves, ontological_order[i])\n",
    "    return (leavves)\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from mcmodels.regressors.nonparametric.nadaraya_watson import get_weights\n",
    "\n",
    "def get_best_hyperparameters(losses, keys):\n",
    "    \n",
    "    major_structure_ids = np.asarray(list(losses.keys()))\n",
    "    nms = len(major_structure_ids)\n",
    "    nkey = keys.shape[1]\n",
    "    output = np.empty((nms, nkey))\n",
    "    for m in range(nms):\n",
    "        print(m)\n",
    "        sid = major_structure_ids[m]\n",
    "        lvec = np.asarray([np.nanmean(losses[sid][key]) for key in keys])\n",
    "        if np.any(~np.isnan(lvec)):\n",
    "            output[m] = keys[np.nanargmin(lvec)]\n",
    "        # if len(np.where(np.isnan(np.nanmean(losses[sid][:,:], axis = 1)))[0]) < losses[sid].shape[0]:\n",
    "        #    output[m] = np.nanargmin(np.nanmean(losses[sid][:,:], axis = 1))\n",
    "\n",
    "    output = np.asarray(output, dtype=int)\n",
    "    return(output)\n",
    "\n",
    "#get where we actually modelled\n",
    "def get_eval_indices(eval_index_matrices):\n",
    "    eval_indices = {}\n",
    "    major_structure_ids = np.asarray(list(eval_index_matrices.keys()))\n",
    "    for sid in major_structure_ids:\n",
    "        eval_indices[sid] = np.where(eval_index_matrices[sid].sum(axis = 0) > 0)[0]\n",
    "    return(eval_indices)    \n",
    "\n",
    "\n",
    "\n",
    "def get_weights(eval_centroids, model_centroids, gamma):\n",
    "    weights = pairwise_kernels(X=eval_centroids, Y=model_centroids, metric='rbf', gamma=gamma, filter_params=True)\n",
    "    return (weights)\n",
    "\n",
    "\n",
    "def get_indices(ids):\n",
    "\n",
    "    ids_unique = np.unique(ids)\n",
    "    output = np.zeros((len(ids_unique), len(ids)), dtype = int)\n",
    "    for i in range(len(ids_unique)):\n",
    "        output[i,np.where(ids == ids_unique[i])[0] ] = 1\n",
    "    return(output)\n",
    "\n",
    "#get indices of firstlist in firstlisttest in categories defined by secondlist\n",
    "def get_indices2(firstlist, firstlisttest, secondlist):\n",
    "    \n",
    "    sl_unique = np.unique(secondlist)\n",
    "    output = np.zeros((len(sl_unique), len(secondlist)), dtype = int)\n",
    "    for i in range(len(sl_unique)):\n",
    "        output[i,np.intersect1d(np.where(np.isin(firstlist,firstlisttest))[0], np.where(secondlist == sl_unique[i])[0])] = 1\n",
    "    return(output)\n",
    "\n",
    "#nmodels = nleafs\n",
    "#populate each with experiments that share summary structure\n",
    "def get_indices_summaryinleaf(summarylist , leaflist):\n",
    "    \n",
    "    nexp = len(leaflist)\n",
    "    leaf_unique = np.unique(leaflist)\n",
    "    output = np.zeros((len(leaf_unique), nexp), dtype = int)\n",
    "    \n",
    "    for i in range(len(leaf_unique)):\n",
    "        \n",
    "        summary = summarylist[np.where(leaflist == leaf_unique[i])[0]][0]\n",
    "        output[i,np.where(summarylist == summary)[0]] = 1\n",
    "        \n",
    "        \n",
    "    return(output)\n",
    "\n",
    "#get predictions at all eval_indices using model_indices\n",
    "#if an eval_indices is also a model indice, leave it out of the model\n",
    "#if a model index is not an eval index, it never gets left out\n",
    "def get_nwloocv_predictions_singlemodel(projections, centroids, gamma, model_indices, eval_indices):\n",
    "  \n",
    "    eval_index_val = np.where(eval_indices == 1)[0]\n",
    "    model_index_val = np.where(model_indices == 1)[0]\n",
    "    \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    \n",
    "    nmod_ind = len(model_index_val)\n",
    "    neval = len(eval_index_val)\n",
    "    #nexp = centroids.shape[0]\n",
    "    predictions = np.empty(projections.shape)\n",
    "    #print(model_index_val.shape, eval_index_val.shape)\n",
    "\n",
    "    if len(model_index_val) > 0 and  len(eval_index_val) > 0:\n",
    "        weights = pairwise_kernels(centroids[model_index_val], centroids[eval_index_val], metric='rbf', gamma=gamma, filter_params=True) #get_weights(centroids, gamma)\n",
    "        for i in range(neval):\n",
    "            matchindex = np.where(model_index_val == eval_index_val[i])[0]\n",
    "            otherindices = np.setdiff1d(np.asarray(list(range(nmod_ind))), matchindex)         \n",
    "            #this order of operations is the fastest I found\n",
    "            weights_i = weights[:,i] / weights[:,i][otherindices].sum()\n",
    "            weights_i[matchindex] = 0\n",
    "            weights_i = np.asarray(weights_i, dtype=np.float32)\n",
    "            pred = np.dot(weights_i, projections[model_index_val])\n",
    "            predictions[eval_index_val[i]] = pred\n",
    "\n",
    "        \n",
    "    return(predictions)    \n",
    "\n",
    "def get_nwloocv_predictions_multimodel(projections, centroids, gammas, model_index_matrix, eval_index_matrix):\n",
    "    \n",
    "\n",
    "    \n",
    "    ntargets = projections.shape[1]\n",
    "    nexp = projections.shape[0]\n",
    "    nmodels = model_index_matrix.shape[0]\n",
    "    ngammas = len(gammas)\n",
    "    \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    predictions = np.empty((nmodels, ngammas, nexp, ntargets))\n",
    "    \n",
    "    \n",
    "    for m in range(nmodels):\n",
    "        #print('m', m, len(np.where(model_index_matrix[m] ==1)[0]))\n",
    "        predictions[m] = np.asarray([get_nwloocv_predictions_singlemodel(projections, centroids, gammas[g], model_index_matrix[m], eval_index_matrix[m]) for g in range(ngammas)])\n",
    "    \n",
    "    return(predictions)  \n",
    "\n",
    "def combine_predictions(predictions, eval_index_matrix):\n",
    "    \n",
    "    nmodels, ngammas, nexp, ntargets = predictions.shape\n",
    "    combined_predictions = np.empty((ngammas, nexp, ntargets))\n",
    "    for m in range(nmodels):\n",
    "        combined_predictions[:,np.where(eval_index_matrix[m] == 1)[0]] = predictions[m][:,np.where(eval_index_matrix[m] == 1)[0]]\n",
    "        \n",
    "    return(combined_predictions)\n",
    "\n",
    "def get_nwloocv_predictions_multimodel_merge(projections, centroids, gammas, model_index_matrix, eval_index_matrix):\n",
    "    \n",
    "    predictions_unmerged = get_nwloocv_predictions_multimodel(projections, centroids, gammas, model_index_matrix, eval_index_matrix)\n",
    "    print(predictions_unmerged.shape)\n",
    "    predictions_merged = combine_predictions(predictions_unmerged, eval_index_matrix)\n",
    "    \n",
    "    return(predictions_merged)\n",
    "\n",
    "#we should not pass model_index_matrices that are identical to eval_index_matrices and have only 1 element per model\n",
    "#although in principal we could do automatically in the cross validation code \n",
    "#we would rather do it explicitly to ensure identical indexing b/w experiments\n",
    "#if we only have 1 model index we will remove the model index from eval indices\n",
    "def screen_indices(model_indices, eval_indices):\n",
    "    \n",
    "    eval_indices2 = eval_indices.copy()\n",
    "    mod_loc = np.where(model_indices == 1)[0]\n",
    "    if len(mod_loc) == 1:\n",
    "        eval_indices2[mod_loc] = 0\n",
    "    return(eval_indices2)\n",
    "\n",
    "#this could result in an empty eval index i.e. certain indices having no prediction.  catch later\n",
    "#can merge (sum) the index matrix to see where predictions are actually generated\n",
    "def screen_index_matrices(model_index_matrices, eval_index_matrices):\n",
    "    \n",
    "    #alter eval_indices to remove model index in cases where there is only one experiment in the model\n",
    "    \n",
    "    nmodels = model_index_matrices.shape[0]\n",
    "    eval_index_matrices2 = eval_index_matrices.copy()\n",
    "    for m in range(nmodels):\n",
    "        eval_index_matrices2[m] = screen_indices(model_index_matrices[m], eval_index_matrices[m])\n",
    "    \n",
    "    return(eval_index_matrices2)\n",
    "\n",
    "#need code for removing experiments that have no model\n",
    "#this can happen when the model set is a subset of the evaluation set.\n",
    "#we will therefore generate predictions for a subset\n",
    "#given a leaf is included, the eval set is the same\n",
    "#however, we want to remove evals in leaves we don't have a wt for... of course one could say we are doing worse...\n",
    "#but we also have a fewer number of models\n",
    "#model_index_matrices are the indices of the leafs'\n",
    "#indices_wtinleaf are the wild types\n",
    "#need to make sure we dont have leafs with only 1 experiment\n",
    "\n",
    "\n",
    "def screen_index_matrices2(model_index_matrices, eval_index_matrices):\n",
    "    \n",
    "    #alter model and eval matrices to be nonzero only when there are at least two experiments in the model\n",
    "    #it can be useful for when model_index_matrices is a subset of eval_index_matrices\n",
    "    #nmodels = model_index_matrices.shape[0]\n",
    "    include_per_model = model_index_matrices.sum(axis= 1)\n",
    "    to_exclude = np.where(include_per_model <= 1)[0]\n",
    "    #to_include = np.where(include_per_model > 0)[0]\n",
    "    \n",
    "    model_index_matrices2 = model_index_matrices.copy()\n",
    "    eval_index_matrices2 = eval_index_matrices.copy()\n",
    "    model_index_matrices2[to_exclude] = 0\n",
    "    eval_index_matrices2[to_exclude] = 0\n",
    "    \n",
    "    return(model_index_matrices2, eval_index_matrices2)\n",
    "\n",
    "def screen_index_matrices3(model_index_matrices, eval_index_matrices):\n",
    "    \n",
    "    #alter model and eval matrices to be nonzero only when there are at least one experiments in the model\n",
    "    #it can be useful for when model_index_matrices is a subset of eval_index_matrices\n",
    "    #nmodels = model_index_matrices.shape[0]\n",
    "    include_per_model = model_index_matrices.sum(axis= 1)\n",
    "    to_exclude = np.where(include_per_model < 1)[0]\n",
    "    #to_include = np.where(include_per_model > 0)[0]\n",
    "    \n",
    "    model_index_matrices2 = model_index_matrices.copy()\n",
    "    eval_index_matrices2 = eval_index_matrices.copy()\n",
    "    model_index_matrices2[to_exclude] = 0\n",
    "    eval_index_matrices2[to_exclude] = 0\n",
    "    \n",
    "    to_remove = np.where(include_per_model == 1)[0]\n",
    "    eval_index_matrices2[to_remove] = 0\n",
    "    \n",
    "    return(model_index_matrices2, eval_index_matrices2)\n",
    "\n",
    "def get_cre_similarity(proj, cres,eminors,colnames):\n",
    "\n",
    "    ss = np.append(['cre','injsum'],colnames)\n",
    "    ns = np.concatenate([np.empty(2, dtype = str), np.repeat('ipsi',291), np.repeat('contra',286)])\n",
    "    colns = np.vstack([ns,ss]).transpose()\n",
    "    clns = list(zip(*colns.transpose()))\n",
    "    clnsmi = pd.MultiIndex.from_tuples(clns, names=['first', 'second'])\n",
    "    data_merged = pd.DataFrame(np.hstack([np.expand_dims(cres,1),np.expand_dims(eminors,1),proj ]), columns= clnsmi)\n",
    "    data_melted = pd.melt(data_merged, id_vars=[('','cre'),('','injsum')])\n",
    "    data_melted = data_melted.astype({'value': np.float64})\n",
    "    means = data_melted.groupby(by=['first', 'second',('','cre'),('','injsum')])['value'].mean().reset_index()\n",
    "    means = means.astype({'value': np.float64})\n",
    "    means_cast = pd.pivot_table(means, index = [('','cre'),('','injsum')], columns = ['first', 'second'])\n",
    "    cs = np.asarray(list(means_cast.index))    \n",
    "    return(means_cast,cs)\n",
    "\n",
    "def get_cre_dist_cv(proj, means_cast, eminors,cres):\n",
    "    nsamp = cres.shape[0]\n",
    "    credist = np.empty((nsamp,nsamp))\n",
    "    credist[:] = np.nan\n",
    "    for i in range(nsamp):\n",
    "        print(i)\n",
    "        #get mean of all points sharing cre line and minor structure with this one (but not including)\n",
    "        meani = means_cast.loc[tuple([cres[i], eminors[i]])]\n",
    "#         ncr = len(np.where(cres == cres[i])[0])\n",
    "        ls = np.where(leafs[sid] == leafs[sid][i])[0]\n",
    "        crs = np.where(cres == cres[i])[0]\n",
    "        ncr = len(np.intersect1d(ls, crs))\n",
    "        \n",
    "        meanloocvi = meani\n",
    "        if ncr > 1:\n",
    "            meanloocvi = (ncr * meani ) / (ncr - 1) -   (1/ ncr)* proj[i] #results[reg[i]][tuple([cs[reg_cre_ind[j],1], cs[reg_cre_ind[k],1]])]\n",
    "        else:\n",
    "            meanloocvi = np.zeros(proj[i].shape[0])\n",
    "            meanloocvi[:] = np.nan\n",
    "            \n",
    "        #this was originally\n",
    "        #print(meanloocvi.max(), 'max', ncr)\n",
    "\n",
    "        #meanloocvi = meani\n",
    "        #meanloocvi = (ncr * meani ) / (ncr - 1) -   (1/ ncr)* msvd.reg_proj_vcount_norm_renorm[i] #results[reg[i]][tuple([cs[reg_cre_ind[j],1], cs[reg_cre_ind[k],1]])]\n",
    "        #rkeys = list(results[eminors[i]].keys())#np.asarray(list(results[experiments_minor_structures[sid][i]].keys()))\n",
    "        for j in range(nsamp):\n",
    "            meanj = means_cast.loc[tuple([cres[j], eminors[j]])]\n",
    "            #t= tuple([cres[i], cres[j]])\n",
    "            #if np.isin(t, rkeys).all():\n",
    "            if eminors[j] == eminors[i]:\n",
    "                credist[i,j]  = np.linalg.norm(meanloocvi - meanj)\n",
    "\n",
    "    return(credist)\n",
    "\n",
    "# def get_nwloocv_predictions_singlemodel_dists(projections, dists, gamma, model_indices, eval_indices):\n",
    "  \n",
    "#     eval_index_val = np.where(eval_indices == 1)[0]\n",
    "#     model_index_val = np.where(model_indices == 1)[0]\n",
    "    \n",
    "#     projections = np.asarray(projections, dtype=np.float32)\n",
    "    \n",
    "#     nmod_ind = len(model_index_val)\n",
    "#     neval = len(eval_index_val)\n",
    "#     #nexp = centroids.shape[0]\n",
    "#     predictions = np.empty(projections.shape)\n",
    "#     #print(model_index_val.shape, eval_index_val.shape)\n",
    "\n",
    "#     if len(model_index_val) > 0 and  len(eval_index_val) > 0:\n",
    "#         weights = np.exp(-dists[model_index_val][:, eval_index_val] / gamma)#np.exp(-dists[model_index_val] / gamma) #get_weights(centroids, gamma)\n",
    "#         for i in range(neval):\n",
    "#             matchindex = np.where(model_index_val == eval_index_val[i])[0]\n",
    "#             otherindices = np.setdiff1d(np.asarray(list(range(nmod_ind))), matchindex)         \n",
    "#             #this order of operations is the fastest I found\n",
    "#             weights_i = weights[i,:] / np.nansum(weights[i,:][otherindices])\n",
    "#             #print(np.nansum(weights[:,i][otherindices]))\n",
    "#             weights_i[matchindex] = 0\n",
    "#             weights_i = np.asarray(weights_i, dtype=np.float32)\n",
    "#             weights_i[np.isnan(weights_i)] = 0.\n",
    "#             pred = np.dot(weights_i, projections[model_index_val])\n",
    "#             predictions[eval_index_val[i]] = pred\n",
    "\n",
    "        \n",
    "#     return(predictions) \n",
    "\n",
    "def get_nwloocv_predictions_multimodel_merge_dists(projections, dists, gammas, model_index_matrix, eval_index_matrix):\n",
    "    \n",
    "    predictions_unmerged = get_nwloocv_predictions_multimodel_dists(projections, dists, gammas, model_index_matrix, eval_index_matrix)\n",
    "    print(predictions_unmerged.shape)\n",
    "    predictions_merged = combine_predictions(predictions_unmerged, eval_index_matrix)\n",
    "    \n",
    "    return(predictions_merged)\n",
    "    \n",
    "    \n",
    "def get_nwloocv_predictions_multimodel_dists(projections, dists, gammas, model_index_matrix, eval_index_matrix):\n",
    "    \n",
    "\n",
    "    \n",
    "    ntargets = projections.shape[1]\n",
    "    nexp = projections.shape[0]\n",
    "    nmodels = model_index_matrix.shape[0]\n",
    "    ngammas = len(gammas)\n",
    "    \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    predictions = np.empty((nmodels, ngammas, nexp, ntargets))\n",
    "    \n",
    "    \n",
    "    for m in range(nmodels):\n",
    "        #print('m', m, len(np.where(model_index_matrix[m] ==1)[0]))\n",
    "        predictions[m] = np.asarray([get_nwloocv_predictions_singlemodel_dists(projections, dists, gammas[g], model_index_matrix[m], eval_index_matrix[m]) for g in range(ngammas)])\n",
    "    \n",
    "    return(predictions)  \n",
    "\n",
    "\n",
    "class ConnectivityData():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.structure_datas = {}    \n",
    "    \n",
    "    def get_injection_hemisphere_ids(self):\n",
    "        \n",
    "        structure_datas = self.structure_datas\n",
    "        \n",
    "        \n",
    "        for sid in list(structure_datas.keys()):\n",
    "            structure_datas[sid].get_injection_hemisphere_ids()\n",
    "            \n",
    "        self.structure_datas = structure_datas\n",
    "        \n",
    "        \n",
    "    def align(self):\n",
    "        \n",
    "        structure_datas= self.structure_datas\n",
    "        \n",
    "        for sid in list(structure_datas.keys()):\n",
    "            structure_datas[sid].align()\n",
    "        self.structure_datas = structure_datas\n",
    "\n",
    "    def get_centroids(self):\n",
    "        \n",
    "        structure_datas = self.structure_datas\n",
    "        \n",
    "        for sid in list(structure_datas.keys()):\n",
    "            structure_datas[sid].get_centroids()\n",
    "            \n",
    "        self.structure_datas = structure_datas\n",
    "        \n",
    "class StructureData():\n",
    "    \n",
    "    def __init__(self, sid):\n",
    "        self.experiment_datas = {}\n",
    "        \n",
    "    def get_injection_hemisphere_ids(self):   \n",
    "        \n",
    "        experiment_datas = self.experiment_datas\n",
    "        \n",
    "        for eid in list(experiment_datas.keys()):\n",
    "             experiment_datas[eid].injection_hemisphere_id = get_injection_hemisphere_id(experiment_datas[eid].injection_qmasked, majority=True)\n",
    "        self.experiment_datas = experiment_datas\n",
    "        \n",
    "    def align(self):\n",
    "        \n",
    "        experiment_datas = self.experiment_datas\n",
    "        \n",
    "        for eid in list(experiment_datas.keys()):\n",
    "            if experiment_datas[eid].injection_hemisphere_id == 1:\n",
    "                experiment_datas[eid].flip()\n",
    "        self.experiment_datas = experiment_datas\n",
    "\n",
    "    def get_centroids(self):\n",
    "        \n",
    "        experiment_datas = self.experiment_datas\n",
    "        \n",
    "        for eid in list(experiment_datas.keys()):\n",
    "            experiment_datas[eid].centroid = compute_centroid(experiment_datas[eid].injection_qmasked)\n",
    "        self.experiment_datas = experiment_datas        \n",
    "        \n",
    "class ExperimentData():\n",
    "    \n",
    "    def __init__(self, eid):\n",
    "        2+2\n",
    "    \n",
    "    #def get_injection_hemisphere():\n",
    "        \n",
    "        \n",
    "    def flip(self):\n",
    "        \"\"\"Reflects experiment along midline.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self - flipped experiment\n",
    "        \"\"\"\n",
    "\n",
    "        self.injection_qmasked = self.injection_qmasked[..., ::-1]\n",
    "        self.projection_qmasked = self.projection_qmasked[..., ::-1]\n",
    "        self.injection_signal = self.injection_signal[..., ::-1]\n",
    "        self.projection_signal = self.projection_signal[..., ::-1]\n",
    "        self.injection_signal_true = self.injection_signal_true[..., ::-1]\n",
    "        #self.projection_signal_true = self.projection_signal_true[..., ::-1]\n",
    "        self.injection_fraction = self.injection_fraction[..., ::-1]\n",
    "        self.data_quality_mask = self.data_quality_mask[...,::-1]\n",
    "        \n",
    "        #return self        \n",
    "\n",
    "def _mask_data_volume(data_volume, data_mask, tolerance=0.0):\n",
    "    \"\"\"Masks a given data volume in place.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_volume : array, shape (x_ccf, y_ccf, z_ccf)\n",
    "        Data volume to be masked.\n",
    "\n",
    "    data_mask : array, shape (x_ccf, y_ccf, z_ccf)\n",
    "        data_mask for given experiment (values in [0,1])\n",
    "        See allensdk.core.mouse_connectivity_cache for more info.\n",
    "\n",
    "    tolerance : float, optional (default=0.0)\n",
    "        tolerance with which to define bad voxels in data_mask.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_volume\n",
    "        data_volume parameter masked in place.\n",
    "\n",
    "    \"\"\"\n",
    "    if data_volume.shape != data_mask.shape:\n",
    "        raise ValueError(\"data_volume (%s) and data_mask (%s) must be the same \"\n",
    "                         \"shape!\" % (data_volume.shape, data_mask.shape))\n",
    "\n",
    "    # mask data volume\n",
    "    data_volume[data_mask < tolerance] = 0.0\n",
    "\n",
    "    return data_volume\n",
    "\n",
    "\n",
    "def get_summarystructure_dictionary(connectivity_data, data_info):\n",
    "    summarystructure_dictionary = {}\n",
    "    major_structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "    for sid in major_structure_ids:\n",
    "        eids = np.asarray(list(connectivity_data.structure_datas[sid].experiment_datas.keys()))\n",
    "        summarystructure_dictionary[sid] = get_minorstructures(eids, data_info)\n",
    "    return (summarystructure_dictionary)\n",
    "\n",
    "\n",
    "def get_leaves_ontologicalorder(connectivity_data, ontological_order):\n",
    "    '''\n",
    "\n",
    "    :param msvd:\n",
    "    :param ontological_order:\n",
    "    :return: The leaf order associated with the 'ontological_order' of summary structures\n",
    "    '''\n",
    "    sid0 = list(connectivity_data.structure_datas.keys())[0]\n",
    "    #eid0 = list(connectivity_data.structure_datas[sid0].experiment_datas.keys())[0]\n",
    "    levs = connectivity_data.structure_datas[sid0].projection_mask.reference_space.structure_tree.child_ids(\n",
    "        ontological_order)\n",
    "    flat_list = np.asarray([item for sublist in levs for item in sublist])\n",
    "\n",
    "    nss = len(levs)\n",
    "    leavves = np.asarray([])\n",
    "    for i in range(nss):\n",
    "        if len(levs[i]) > 0:\n",
    "            leavves = np.append(leavves, levs[i])\n",
    "        else:\n",
    "            leavves = np.append(leavves, ontological_order[i])\n",
    "    return (leavves)\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from mcmodels.regressors.nonparametric.nadaraya_watson import get_weights\n",
    "\n",
    "def get_best_hyperparameters(losses, keys):\n",
    "    \n",
    "    major_structure_ids = np.asarray(list(losses.keys()))\n",
    "    nms = len(major_structure_ids)\n",
    "    nkey = keys.shape[1]\n",
    "    output = np.empty((nms, nkey))\n",
    "    for m in range(nms):\n",
    "        print(m)\n",
    "        sid = major_structure_ids[m]\n",
    "        lvec = np.asarray([np.nanmean(losses[sid][key]) for key in keys])\n",
    "        if np.any(~np.isnan(lvec)):\n",
    "            output[m] = keys[np.nanargmin(lvec)]\n",
    "        # if len(np.where(np.isnan(np.nanmean(losses[sid][:,:], axis = 1)))[0]) < losses[sid].shape[0]:\n",
    "        #    output[m] = np.nanargmin(np.nanmean(losses[sid][:,:], axis = 1))\n",
    "\n",
    "    output = np.asarray(output, dtype=int)\n",
    "    return(output)\n",
    "\n",
    "#get where we actually modelled\n",
    "def get_eval_indices(eval_index_matrices):\n",
    "    eval_indices = {}\n",
    "    major_structure_ids = np.asarray(list(eval_index_matrices.keys()))\n",
    "    for sid in major_structure_ids:\n",
    "        eval_indices[sid] = np.where(eval_index_matrices[sid].sum(axis = 0) > 0)[0]\n",
    "    return(eval_indices)    \n",
    "\n",
    "\n",
    "\n",
    "def get_weights(eval_centroids, model_centroids, gamma):\n",
    "    weights = pairwise_kernels(X=eval_centroids, Y=model_centroids, metric='rbf', gamma=gamma, filter_params=True)\n",
    "    return (weights)\n",
    "\n",
    "\n",
    "def get_indices(ids):\n",
    "\n",
    "    ids_unique = np.unique(ids)\n",
    "    output = np.zeros((len(ids_unique), len(ids)), dtype = int)\n",
    "    for i in range(len(ids_unique)):\n",
    "        output[i,np.where(ids == ids_unique[i])[0] ] = 1\n",
    "    return(output)\n",
    "\n",
    "#get indices of firstlist in firstlisttest in categories defined by secondlist\n",
    "def get_indices2(firstlist, firstlisttest, secondlist):\n",
    "    \n",
    "    sl_unique = np.unique(secondlist)\n",
    "    output = np.zeros((len(sl_unique), len(secondlist)), dtype = int)\n",
    "    for i in range(len(sl_unique)):\n",
    "        output[i,np.intersect1d(np.where(np.isin(firstlist,firstlisttest))[0], np.where(secondlist == sl_unique[i])[0])] = 1\n",
    "    return(output)\n",
    "\n",
    "#nmodels = nleafs\n",
    "#populate each with experiments that share summary structure\n",
    "def get_indices_summaryinleaf(summarylist , leaflist):\n",
    "    \n",
    "    nexp = len(leaflist)\n",
    "    leaf_unique = np.unique(leaflist)\n",
    "    output = np.zeros((len(leaf_unique), nexp), dtype = int)\n",
    "    \n",
    "    for i in range(len(leaf_unique)):\n",
    "        \n",
    "        summary = summarylist[np.where(leaflist == leaf_unique[i])[0]][0]\n",
    "        output[i,np.where(summarylist == summary)[0]] = 1\n",
    "        \n",
    "        \n",
    "    return(output)\n",
    "\n",
    "#get predictions at all eval_indices using model_indices\n",
    "#if an eval_indices is also a model indice, leave it out of the model\n",
    "#if a model index is not an eval index, it never gets left out\n",
    "def get_nwloocv_predictions_singlemodel(projections, centroids, gamma, model_indices, eval_indices):\n",
    "  \n",
    "    eval_index_val = np.where(eval_indices == 1)[0]\n",
    "    model_index_val = np.where(model_indices == 1)[0]\n",
    "    \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    \n",
    "    nmod_ind = len(model_index_val)\n",
    "    neval = len(eval_index_val)\n",
    "    #nexp = centroids.shape[0]\n",
    "    predictions = np.empty(projections.shape)\n",
    "    #print(model_index_val.shape, eval_index_val.shape)\n",
    "\n",
    "    if len(model_index_val) > 0 and  len(eval_index_val) > 0:\n",
    "        weights = pairwise_kernels(centroids[model_index_val], centroids[eval_index_val], metric='rbf', gamma=gamma, filter_params=True) #get_weights(centroids, gamma)\n",
    "        for i in range(neval):\n",
    "            matchindex = np.where(model_index_val == eval_index_val[i])[0]\n",
    "            otherindices = np.setdiff1d(np.asarray(list(range(nmod_ind))), matchindex)         \n",
    "            #this order of operations is the fastest I found\n",
    "            weights_i = weights[:,i] / weights[:,i][otherindices].sum()\n",
    "            weights_i[matchindex] = 0\n",
    "            weights_i = np.asarray(weights_i, dtype=np.float32)\n",
    "            pred = np.dot(weights_i, projections[model_index_val])\n",
    "            predictions[eval_index_val[i]] = pred\n",
    "\n",
    "        \n",
    "    return(predictions)    \n",
    "\n",
    "def get_nwloocv_predictions_multimodel(projections, centroids, gammas, model_index_matrix, eval_index_matrix):\n",
    "    \n",
    "\n",
    "    \n",
    "    ntargets = projections.shape[1]\n",
    "    nexp = projections.shape[0]\n",
    "    nmodels = model_index_matrix.shape[0]\n",
    "    ngammas = len(gammas)\n",
    "    \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    predictions = np.empty((nmodels, ngammas, nexp, ntargets))\n",
    "    \n",
    "    \n",
    "    for m in range(nmodels):\n",
    "        #print('m', m, len(np.where(model_index_matrix[m] ==1)[0]))\n",
    "        predictions[m] = np.asarray([get_nwloocv_predictions_singlemodel(projections, centroids, gammas[g], model_index_matrix[m], eval_index_matrix[m]) for g in range(ngammas)])\n",
    "    \n",
    "    return(predictions)  \n",
    "\n",
    "def combine_predictions(predictions, eval_index_matrix):\n",
    "    \n",
    "    nmodels, ngammas, nexp, ntargets = predictions.shape\n",
    "    combined_predictions = np.empty((ngammas, nexp, ntargets))\n",
    "    for m in range(nmodels):\n",
    "        combined_predictions[:,np.where(eval_index_matrix[m] == 1)[0]] = predictions[m][:,np.where(eval_index_matrix[m] == 1)[0]]\n",
    "        \n",
    "    return(combined_predictions)\n",
    "\n",
    "def get_nwloocv_predictions_multimodel_merge(projections, centroids, gammas, model_index_matrix, eval_index_matrix):\n",
    "    \n",
    "    predictions_unmerged = get_nwloocv_predictions_multimodel(projections, centroids, gammas, model_index_matrix, eval_index_matrix)\n",
    "    print(predictions_unmerged.shape)\n",
    "    predictions_merged = combine_predictions(predictions_unmerged, eval_index_matrix)\n",
    "    \n",
    "    return(predictions_merged)\n",
    "\n",
    "#we should not pass model_index_matrices that are identical to eval_index_matrices and have only 1 element per model\n",
    "#although in principal we could do automatically in the cross validation code \n",
    "#we would rather do it explicitly to ensure identical indexing b/w experiments\n",
    "#if we only have 1 model index we will remove the model index from eval indices\n",
    "def screen_indices(model_indices, eval_indices):\n",
    "    \n",
    "    eval_indices2 = eval_indices.copy()\n",
    "    mod_loc = np.where(model_indices == 1)[0]\n",
    "    if len(mod_loc) == 1:\n",
    "        eval_indices2[mod_loc] = 0\n",
    "    return(eval_indices2)\n",
    "\n",
    "#this could result in an empty eval index i.e. certain indices having no prediction.  catch later\n",
    "#can merge (sum) the index matrix to see where predictions are actually generated\n",
    "def screen_index_matrices(model_index_matrices, eval_index_matrices):\n",
    "    \n",
    "    #alter eval_indices to remove model index in cases where there is only one experiment in the model\n",
    "    \n",
    "    nmodels = model_index_matrices.shape[0]\n",
    "    eval_index_matrices2 = eval_index_matrices.copy()\n",
    "    for m in range(nmodels):\n",
    "        eval_index_matrices2[m] = screen_indices(model_index_matrices[m], eval_index_matrices[m])\n",
    "    \n",
    "    return(eval_index_matrices2)\n",
    "\n",
    "#need code for removing experiments that have no model\n",
    "#this can happen when the model set is a subset of the evaluation set.\n",
    "#we will therefore generate predictions for a subset\n",
    "#given a leaf is included, the eval set is the same\n",
    "#however, we want to remove evals in leaves we don't have a wt for... of course one could say we are doing worse...\n",
    "#but we also have a fewer number of models\n",
    "#model_index_matrices are the indices of the leafs'\n",
    "#indices_wtinleaf are the wild types\n",
    "#need to make sure we dont have leafs with only 1 experiment\n",
    "\n",
    "def screen_index_matrices2(model_index_matrices, eval_index_matrices):\n",
    "    \n",
    "    #alter model and eval matrices to be nonzero only when there are at least two experiments in the model\n",
    "    #it can be useful for when model_index_matrices is a subset of eval_index_matrices\n",
    "    #nmodels = model_index_matrices.shape[0]\n",
    "    include_per_model = model_index_matrices.sum(axis= 1)\n",
    "    to_exclude = np.where(include_per_model <= 1)[0]\n",
    "    #to_include = np.where(include_per_model > 0)[0]\n",
    "    \n",
    "    model_index_matrices2 = model_index_matrices.copy()\n",
    "    eval_index_matrices2 = eval_index_matrices.copy()\n",
    "    model_index_matrices2[to_exclude] = 0\n",
    "    eval_index_matrices2[to_exclude] = 0\n",
    "    \n",
    "    return(model_index_matrices2, eval_index_matrices2)\n",
    "  \n",
    "    \n",
    "def screen_index_matrices3(model_index_matrices, eval_index_matrices):\n",
    "    \n",
    "    #alter model and eval matrices to be nonzero only when there are at least one experiments in the model\n",
    "    #it can be useful for when model_index_matrices is a subset of eval_index_matrices\n",
    "    #nmodels = model_index_matrices.shape[0]\n",
    "    include_per_model = model_index_matrices.sum(axis= 1)\n",
    "    to_exclude = np.where(include_per_model < 1)[0]\n",
    "    #to_include = np.where(include_per_model > 0)[0]\n",
    "    \n",
    "    model_index_matrices2 = model_index_matrices.copy()\n",
    "    eval_index_matrices2 = eval_index_matrices.copy()\n",
    "    model_index_matrices2[to_exclude] = 0\n",
    "    eval_index_matrices2[to_exclude] = 0\n",
    "    \n",
    "    to_remove = np.where(include_per_model == 1)[0]\n",
    "    eval_index_matrices2[to_remove] = 0\n",
    "    \n",
    "    return(model_index_matrices2, eval_index_matrices2)\n",
    "\n",
    "def get_cre_similarity(proj, cres,eminors,colnames):\n",
    "\n",
    "    ss = np.append(['cre','injsum'],colnames)\n",
    "    ns = np.concatenate([np.empty(2, dtype = str), np.repeat('ipsi',291), np.repeat('contra',286)])\n",
    "    colns = np.vstack([ns,ss]).transpose()\n",
    "    clns = list(zip(*colns.transpose()))\n",
    "    clnsmi = pd.MultiIndex.from_tuples(clns, names=['first', 'second'])\n",
    "    data_merged = pd.DataFrame(np.hstack([np.expand_dims(cres,1),np.expand_dims(eminors,1),proj ]), columns= clnsmi)\n",
    "    data_melted = pd.melt(data_merged, id_vars=[('','cre'),('','injsum')])\n",
    "    data_melted = data_melted.astype({'value': np.float64})\n",
    "    means = data_melted.groupby(by=['first', 'second',('','cre'),('','injsum')])['value'].mean().reset_index()\n",
    "    means = means.astype({'value': np.float64})\n",
    "    means_cast = pd.pivot_table(means, index = [('','cre'),('','injsum')], columns = ['first', 'second'])\n",
    "    cs = np.asarray(list(means_cast.index))    \n",
    "    return(means_cast,cs)\n",
    "\n",
    "def get_cre_dist_cv(proj, means_cast, eminors,cres):\n",
    "    nsamp = cres.shape[0]\n",
    "    credist = np.empty((nsamp,nsamp))\n",
    "    credist[:] = np.nan\n",
    "    for i in range(nsamp):\n",
    "        print(i)\n",
    "        #get mean of all points sharing cre line and minor structure with this one (but not including)\n",
    "        meani = means_cast.loc[tuple([cres[i], eminors[i]])]\n",
    "        ls = np.where(leafs[sid] == leafs[sid][i])[0]\n",
    "        crs = np.where(cres == cres[i])[0]\n",
    "        ncr = len(np.intersect1d(ls, crs))\n",
    "        \n",
    "        meanloocvi = meani\n",
    "        if ncr > 1:\n",
    "            meanloocvi = (ncr * meani ) / (ncr - 1) -   (1/ ncr)* proj[i] #results[reg[i]][tuple([cs[reg_cre_ind[j],1], cs[reg_cre_ind[k],1]])]\n",
    "        else:\n",
    "            meanloocvi = np.zeros(proj[i].shape[0])\n",
    "            meanloocvi[:] = np.nan\n",
    "        for j in range(nsamp):\n",
    "            meanj = means_cast.loc[tuple([cres[j], eminors[j]])]\n",
    "            if eminors[j] == eminors[i]:\n",
    "                credist[i,j]  = np.linalg.norm(meanloocvi - meanj)\n",
    "\n",
    "    return(credist)\n",
    "\n",
    "\n",
    "def get_ccf_data(cache, experiment_id):\n",
    "\n",
    "    eid_data = ExperimentData(experiment_id)\n",
    "    eid_data.data_quality_mask = cache.get_data_mask(experiment_id)[0]\n",
    "    eid_data.injection_signal = cache.get_injection_density(experiment_id)[0]\n",
    "    eid_data.injection_fraction = cache.get_injection_fraction(experiment_id)[0]\n",
    "    eid_data.projection_signal = cache.get_projection_density(experiment_id)[0]\n",
    "    return(eid_data)\n",
    "#     return {\n",
    "#         \"data_quality_mask\" : cache.get_data_mask(experiment_id)[0],\n",
    "#         \"injection_signal\" : cache.get_injection_density(experiment_id)[0],\n",
    "#         \"injection_fraction\" : cache.get_injection_fraction(experiment_id)[0],\n",
    "#         \"projection_signal\" : cache.get_projection_density(experiment_id)[0]\n",
    "#     }\n",
    "\n",
    "def get_connectivity_data(cache, structure_ids, experiments_exclude, remove_injection = False):\n",
    "\n",
    "    connectivity_data = ConnectivityData()\n",
    "    for sid in structure_ids:\n",
    "        print(sid)\n",
    "        sid_data = StructureData(sid)\n",
    "        #deprecated language\n",
    "        model_data = ModelData(cache, sid)\n",
    "        sid_data.eids = model_data.get_experiment_ids(experiments_exclude=experiments_exclude, cre=None)\n",
    "        for eid in sid_data.eids:\n",
    "            \n",
    "            eid_data = get_ccf_data(cache, eid)#ExperimentData(eid)\n",
    "            eid_data.data_mask_tolerance = .5\n",
    "            #ccf_data = get_ccf_data(cache, eid)\n",
    "            eid_data.injection_signal_true = eid_data.injection_signal * eid_data.injection_fraction\n",
    "            if remove_injection == True:\n",
    "                pass #remove injection fraction from projection\n",
    "            #injection_signal should = projection_signal in some locations (nonzero)\n",
    "            #why do we use partial?\n",
    "            #mask_func = partial(_mask_data_volume,data_mask=eid_data.data_mask,tolerance=eid_data.data_mask_tolerance)\n",
    "            eid_data.injection_qmasked  = _mask_data_volume(eid_data.injection_signal_true,eid_data.data_quality_mask,eid_data.data_mask_tolerance)\n",
    "            eid_data.projection_qmasked  = _mask_data_volume(eid_data.projection_signal,eid_data.data_quality_mask,eid_data.data_mask_tolerance) #mask_func(eid_data.projection_signal) \n",
    "            #eid_data.centroid = compute_centroid(eid_data.injection_qmasked)\n",
    "            sid_data.experiment_datas[eid] = eid_data\n",
    "        connectivity_data.structure_datas[sid] = sid_data\n",
    "    return(connectivity_data)\n",
    "\n",
    "def get_data_matrices(connectivity_data):\n",
    "    \n",
    "    structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "    for sid in structure_ids:\n",
    "        experiment_ids = np.asarray(list(connectivity_data.structure_datas[sid].experiment_datas.keys()))\n",
    "        connectivity_data.structure_datas[sid].injection_mask = Mask.from_cache(cache,structure_ids=[sid],hemisphere_id=2)\n",
    "        connectivity_data.structure_datas[sid].projection_mask = Mask.from_cache(cache,structure_ids=default_structure_ids, hemisphere_id=3)\n",
    "        for eid in experiment_ids:\n",
    "            connectivity_data.structure_datas[sid].experiment_datas[eid].injection_vec = connectivity_data.structure_datas[sid].injection_mask.mask_volume(connectivity_data.structure_datas[sid].experiment_datas[eid].injection_qmasked)\n",
    "            connectivity_data.structure_datas[sid].experiment_datas[eid].projection_vec = connectivity_data.structure_datas[sid].projection_mask.mask_volume(connectivity_data.structure_datas[sid].experiment_datas[eid].projection_qmasked)\n",
    "        connectivity_data.structure_datas[sid].injections = np.asarray([connectivity_data.structure_datas[sid].experiment_datas[eid].injection_vec for eid in connectivity_data.structure_datas[sid].eids])\n",
    "        connectivity_data.structure_datas[sid].projections = np.asarray([connectivity_data.structure_datas[sid].experiment_datas[eid].projection_vec for eid in connectivity_data.structure_datas[sid].eids])\n",
    "        connectivity_data.structure_datas[sid].centroids = np.asarray([connectivity_data.structure_datas[sid].experiment_datas[eid].centroid for eid in connectivity_data.structure_datas[sid].eids])\n",
    "        \n",
    "        \n",
    "    return(connectivity_data)\n",
    "\n",
    "\n",
    "# def get_regionalized_normalized_data(connectivity_data, cache, source_order, ipsi_key, contra_key): #experiments_minor_structures):\n",
    "#     '''\n",
    "#     :param msvds: Class dictionary holding data\n",
    "#     :param cache: AllenSDK cache\n",
    "#     :param source_order: Source key (tautologically ipsilateral due to hemisphere mirroring)\n",
    "#     :param ipsi_key: Ipsilateral target key\n",
    "#     :param contra_key:  Contralateral target key\n",
    "#     :return: msvds: Class dictionary holding average data\n",
    "#     '''\n",
    "#     major_structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "#     for sid in major_structure_ids:\n",
    "#         # print()\n",
    "#         structure_data = connectivity_data.structure_datas[sid]\n",
    "#         #nexp = msvd.projections.shape[0]\n",
    "\n",
    "#         #minor_structures = np.unique(experiments_minor_structures[sid])\n",
    "#         #nmins = len(minor_structures)\n",
    "\n",
    "#         projections = structure_data.projections\n",
    "#         ipsi_proj = unionize(projections, ipsi_key)\n",
    "#         contra_proj = unionize(projections, contra_key)\n",
    "#         reg_proj = np.hstack([ipsi_proj, contra_proj])\n",
    "#         structure_data.reg_proj = reg_proj\n",
    "\n",
    "#         ipsi_target_regions, ipsi_target_counts = nonzero_unique(ipsi_key, return_counts=True)\n",
    "#         contra_target_regions, contra_target_counts = nonzero_unique(contra_key, return_counts=True)\n",
    "#         target_counts = np.concatenate([ipsi_target_counts, contra_target_counts])\n",
    "#         reg_proj_vcount_norm = np.divide(reg_proj, target_counts[np.newaxis, :])\n",
    "#         structure_data.reg_proj_vcount_norm = reg_proj_vcount_norm\n",
    "#         structure_data.reg_proj_vcount_norm_renorm = reg_proj_vcount_norm / np.expand_dims(np.linalg.norm(reg_proj_vcount_norm, axis=1), 1)\n",
    "        \n",
    "#         source_mask = Mask.from_cache(cache, structure_ids=[sid], hemisphere_id=2)\n",
    "#         source_key = source_mask.get_key(structure_ids=source_order)\n",
    "#         source_regions, source_counts = nonzero_unique(source_key, return_counts=True)\n",
    "\n",
    "#         injections = structure_data.injections\n",
    "#         reg_ipsi_inj = unionize(injections, source_key)\n",
    "#         structure_data.reg_inj = reg_ipsi_inj\n",
    "#         reg_inj_vcount_norm = np.divide(reg_ipsi_inj, source_counts[np.newaxis, :])\n",
    "#         structure_data.reg_inj_vcount_norm = reg_inj_vcount_norm\n",
    "        \n",
    "#         structure_data.reg_proj_vcount_norm_injnorm = reg_proj_vcount_norm / np.expand_dims(np.linalg.norm(reg_inj_vcount_norm, axis=1), 1)\n",
    "#         connectivity_data.structure_datas[sid] = structure_data\n",
    "#         #msvd.reg_proj_vcountnorm_totalnorm =\n",
    "#     connectivity_data.ipsi_target_regions = ipsi_target_regions\n",
    "#     connectivity_data.contra_target_regions = contra_target_regions        \n",
    "#     connectivity_data.target_regions = np.concatenate([ipsi_target_regions, contra_target_regions])\n",
    "#     return (connectivity_data)\n",
    "\n",
    "# def get_normalized_data(connectivity_data, normalization = None):\n",
    "\n",
    "#     structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "#     for sid in structure_ids:\n",
    "#         if normalization == None:\n",
    "#             pass\n",
    "#         if normalization == 'injection':\n",
    "#             nc = np.sum(connectivity_data.structure_datas[sid].injections, axis = 1)\n",
    "#             connectivity_data.structure_datas[sid].projection_normed = connectivity_data.structure_datas[sid].projections / nc\n",
    "#         if normalization == 'total':\n",
    "#             nc = np.sum(conn_data[sid].projections, axis = 1)\n",
    "#             connectivity_data.structure_datas[sid].projection_normed = connectivity_data.structure_datas[sid].projections / nc\n",
    "\n",
    "#     return(connectivity_data)\n",
    "\n",
    "\n",
    "def compute_centroid(injection_density):\n",
    "    \"\"\"Computes centroid in index coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    injection_density : array, shape (x_ccf, y_ccf, z_ccf)\n",
    "        injection_density data volume.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        centroid of injection_density in index coordinates.\n",
    "    \"\"\"\n",
    "    nnz = injection_density.nonzero()\n",
    "    coords = np.vstack(nnz)\n",
    "\n",
    "    return np.dot(coords, injection_density[nnz]) / injection_density.sum()\n",
    "\n",
    "def get_cre_status(data_info, connectivity_data):\n",
    "    major_structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "    exps = np.asarray(data_info.index.values , dtype = np.int)\n",
    "    creline = {}\n",
    "    for sid in major_structure_ids:\n",
    "        experiment_ids = np.asarray(list(connectivity_data.structure_datas[sid].experiment_datas.keys()))\n",
    "        nexp = len(experiment_ids)\n",
    "        creline[sid] = np.zeros(nexp, dtype = object)\n",
    "        for i in range(len(experiment_ids)):\n",
    "            index = np.where(exps == experiment_ids[i])[0][0]\n",
    "            creline[sid][i] = data_info['transgenic-line'].iloc[index]\n",
    "    return(creline)\n",
    "\n",
    "def get_minorstructures(eids, data_info):\n",
    "    \n",
    "    experiments_minors = np.zeros(len(eids), dtype=object)\n",
    "\n",
    "    for i in range(len(eids)):\n",
    "        experiment_id = eids[i]\n",
    "        experiments_minors[i] = ai_map[data_info['primary-injection-structure'].loc[experiment_id]]\n",
    "    \n",
    "    return(experiments_minors)\n",
    "\n",
    "def get_summarystructures(connectivity_data, data_info):\n",
    "    summarystructure_dictionary = {}\n",
    "    major_structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "    for sid in major_structure_ids:\n",
    "        eids = np.asarray(list(connectivity_data.structure_datas[sid].experiment_datas.keys()))\n",
    "        connectivity_data.structure_datas[sid].summary_structures = get_minorstructures(eids, data_info)\n",
    "    return (connectivity_data)\n",
    "\n",
    "def get_leafs(connectivity_data, data_info,leafs):\n",
    "    summarystructure_dictionary = {}\n",
    "    major_structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "    for sid in major_structure_ids:\n",
    "        eids = np.asarray(list(connectivity_data.structure_datas[sid].experiment_datas.keys()))\n",
    "        connectivity_data.structure_datas[sid].leafs = leafs[sid]\n",
    "    return (connectivity_data)\n",
    "\n",
    "\n",
    "def screen_index_matrices4(eval_index_matrices, screen):\n",
    "\n",
    "    eval_index_matrices2 = eval_index_matrices.copy()\n",
    "    \n",
    "    #remove values only occuring once in screenset from eval_index_matrices\n",
    "    #shouldn't necessarily remove from model... they can still be used for other points.\n",
    "    nmodels = eval_index_matrices.shape[0]\n",
    "    #output = sceens\n",
    "    for m in range(nmodels):\n",
    "        #print(m)\n",
    "        eval_indices = np.where(eval_index_matrices[m] == 1)[0]\n",
    "        screen_occurences = np.asarray(Counter(screen[eval_indices]).most_common())\n",
    "        if len(screen_occurences)>1:\n",
    "            screens_eliminate = screen_occurences[:,0][np.where(np.asarray(screen_occurences[:,1], dtype = int) == 1)[0]]\n",
    "            eval_index_matrices2[m,eval_indices[np.where(np.isin(screen[eval_indices],screens_eliminate))[0]]] = 0\n",
    "    return(eval_index_matrices2)\n",
    "\n",
    "\n",
    "def get_nw_predictions(projections, dists, gamma):\n",
    "  \n",
    "     \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    neval = dists.shape[1]\n",
    "    #nexp = centroids.shape[0]\n",
    "    predictions = np.zeros((neval, projections.shape[1]))\n",
    "    predictions[:] = np.nan\n",
    "    \n",
    "    #print(model_index_val.shape, eval_index_val.shape)\n",
    "    #weights = np.exp(- dists / gamma)#np.exp(-dists[model_index_val] / gamma) #get_weights(centroids, gamma)\n",
    "    for i in range(neval):\n",
    "        dists_i = dists[:,i] - np.min(dists[:,i])\n",
    "        #dists_i = dists[i,:] - np.min(dists[i,:])\n",
    "        weights_i = np.exp(- dists_i * gamma)\n",
    "        weights_i = np.asarray(weights_i, dtype=np.float32)\n",
    "        weights_i[np.isnan(weights_i)] = 0.\n",
    "        weights_i = weights_i / np.sum(weights_i)\n",
    "        predictions[i] = np.dot(weights_i, projections)\n",
    "        \n",
    "    return(predictions) \n",
    "\n",
    "\n",
    "def get_nwloocv_predictions_singlemodel_dists(projections, dists, gamma, model_indices, eval_indices):\n",
    "  \n",
    "    eval_index_val = np.where(eval_indices == 1)[0]\n",
    "    model_index_val = np.where(model_indices == 1)[0]\n",
    "    \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    \n",
    "    nmod_ind = len(model_index_val)\n",
    "    neval = len(eval_index_val)\n",
    "    #nexp = centroids.shape[0]\n",
    "    predictions = np.empty(projections.shape)\n",
    "\n",
    "    if len(model_index_val) > 0 and  len(eval_index_val) > 0:\n",
    "        #weights = np.exp(-dists[model_index_val][:, eval_index_val] / gamma)#np.exp(-dists[model_index_val] / gamma) #get_weights(centroids, gamma)\n",
    "        for i in range(neval):\n",
    "            matchindex = np.where(model_index_val == eval_index_val[i])[0]\n",
    "            otherindices = np.setdiff1d(np.asarray(list(range(nmod_ind))), matchindex)         \n",
    "            #this order of operations is the fastest I found\n",
    "            dists_i = dists[model_index_val][:, eval_index_val[i]] - np.min(dists[model_index_val[otherindices]][:, eval_index_val[i]])\n",
    "            weights_i = np.exp(-dists_i * gamma)#weights[i,:] / np.nansum(weights[i,:][otherindices])\n",
    "            #print(np.nansum(weights[:,i][otherindices]))\n",
    "            weights_i[matchindex] = 0\n",
    "            weights_i = np.asarray(weights_i, dtype=np.float32)\n",
    "            weights_i = weights_i / np.sum(weights_i)\n",
    "            #weights_i[np.isnan(weights_i)] = 0.\n",
    "            pred = np.dot(weights_i, projections[model_index_val])\n",
    "            predictions[eval_index_val[i]] = pred\n",
    "        \n",
    "    return(predictions) \n",
    "\n",
    "# def get_nwloocv_predictions_multimodel_merge_dists(projections, dists, gammas, model_index_matrix, eval_index_matrix):\n",
    "    \n",
    "#     predictions_unmerged = get_nwloocv_predictions_multimodel_dists(projections, dists, gammas, model_index_matrix, eval_index_matrix)\n",
    "#     print(predictions_unmerged.shape)\n",
    "#     predictions_merged = combine_predictions(predictions_unmerged, eval_index_matrix)\n",
    "    \n",
    "#     return(predictions_merged)\n",
    "    \n",
    "    \n",
    "def get_nwloocv_predictions_multimodel_dists(projections, dists, gammas, model_index_matrix, eval_index_matrix):\n",
    "    \n",
    "    ntargets = projections.shape[1]\n",
    "    nexp = projections.shape[0]\n",
    "    nmodels = model_index_matrix.shape[0]\n",
    "    ngammas = len(gammas)\n",
    "    \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    predictions = np.empty((nmodels, ngammas, nexp, ntargets))\n",
    "    \n",
    "    \n",
    "    for m in range(nmodels):\n",
    "        #print('m', m, len(np.where(model_index_matrix[m] ==1)[0]))\n",
    "        predictions[m] = np.asarray([get_nwloocv_predictions_singlemodel_dists(projections, dists, gammas[g], model_index_matrix[m], eval_index_matrix[m]) for g in range(ngammas)])\n",
    "    \n",
    "    return(predictions)  \n",
    "\n",
    "def get_cre_distances(projections, means_cast, sids, cres):\n",
    "    nsamp = cres.shape[0]\n",
    "    credist = np.empty((nsamp,nsamp))\n",
    "    credist[:] = np.nan\n",
    "    for i in range(nsamp):\n",
    "        print(i)\n",
    "        meani = means_cast[sids[i]][cres[i]]#means_cast.loc[tuple([cres[i], sids[i]])]\n",
    "        for j in range(nsamp):\n",
    "            meanj = means_cast[sids[j]][cres[j]]\n",
    "            if sids[j] == sids[i]:\n",
    "                credist[i,j]  = np.linalg.norm(meani - meanj)#**2\n",
    "    return(credist)\n",
    "def get_loss_surface_cv(projections, centroids, cres, sids,fraction, gamma = 100000):\n",
    "    means_cast = get_means(projections, cres, sids)\n",
    "    cre_distances_cv = get_cre_distances_cv(projections, means_cast, sids, cres)\n",
    "    surface = get_surface_from_distances(projections,centroids,cre_distances_cv, fraction, gamma)\n",
    "    surface.cre_distances_cv = cre_distances_cv\n",
    "    return(surface)\n",
    "\n",
    "# def get_cre_distances_cv(proj, means_cast, sids,cres):\n",
    "#     nsamp = cres.shape[0]\n",
    "#     credist = np.empty((nsamp,nsamp))\n",
    "#     credist[:] = np.nan\n",
    "#     for i in range(nsamp):\n",
    "#         print(i)\n",
    "#         meani = meani = means_cast[sids[i]][cres[i]]\n",
    "#         ls = np.where(leafs[sid] == leafs[sid][i])[0]\n",
    "#         crs = np.where(cres == cres[i])[0]\n",
    "#         ncr = len(np.intersect1d(ls, crs))\n",
    "        \n",
    "#         meanloocvi = meani\n",
    "#         if ncr > 1:\n",
    "#             meanloocvi = (ncr * meani ) / (ncr - 1) -   (1/ ncr)* proj[i] #results[reg[i]][tuple([cs[reg_cre_ind[j],1], cs[reg_cre_ind[k],1]])]\n",
    "#         else:\n",
    "#             meanloocvi = np.zeros(proj[i].shape[0])\n",
    "#             meanloocvi[:] = np.nan\n",
    "            \n",
    "#         for j in range(nsamp):\n",
    "#             meanj = means_cast[sids[j]][cres[j]]\n",
    "#             if sids[j] == sids[i]:\n",
    "#                 credist[i,j]  = np.linalg.norm(meanloocvi - meanj)\n",
    "\n",
    "#     return(credist)\n",
    "\n",
    "def get_cre_distances_cv(proj, means_cast, sids,cres):\n",
    "    nsamp = cres.shape[0]\n",
    "    credist = np.empty((nsamp,nsamp))\n",
    "    credist[:] = np.nan\n",
    "    for i in range(nsamp):\n",
    "        print(i)\n",
    "        meani = meani = means_cast[sids[i]][cres[i]]\n",
    "        ls = np.where(leafs[sid] == leafs[sid][i])[0]\n",
    "        crs = np.where(cres == cres[i])[0]\n",
    "        ncr = len(np.intersect1d(ls, crs))\n",
    "        \n",
    "        meanloocvi = meani\n",
    "        if ncr > 1:\n",
    "            meanloocvi = (ncr * meani ) / (ncr - 1) -   (1/ ncr)* proj[i] #results[reg[i]][tuple([cs[reg_cre_ind[j],1], cs[reg_cre_ind[k],1]])]\n",
    "        else:\n",
    "            meanloocvi = np.zeros(proj[i].shape[0])\n",
    "            meanloocvi[:] = np.nan\n",
    "            \n",
    "        for j in range(nsamp):\n",
    "            meanj = means_cast[sids[j]][cres[j]]\n",
    "            if sids[j] == sids[i]:\n",
    "                credist[j,i]  = np.linalg.norm(meanloocvi - meanj)\n",
    "\n",
    "    return(credist)\n",
    "def get_means(projections, cres, sids):\n",
    "    cre_means = {}\n",
    "    cre_types = np.unique(cres)\n",
    "    sid_types = np.unique(sids)\n",
    "    for i in range(len(sid_types)):\n",
    "        cre_means[sid_types[i]] = {}\n",
    "        sid_inds = np.where(sids == sid_types[i])[0]\n",
    "        for j in range(len(cre_types)): \n",
    "            cre_inds = np.where(cres == cre_types[j])[0]\n",
    "            cre_means[sid_types[i]][cre_types[j]] = np.mean(projections[np.intersect1d(sid_inds, cre_inds)], axis = 0 )\n",
    "    return(cre_means)\n",
    "\n",
    "def get_surface_from_distances(projections,centroids,cre_distances, fraction, gamma = 100000):\n",
    "    \n",
    "    nsamp = centroids.shape[0]\n",
    "    pairs = np.asarray(np.where(~np.isnan(cre_distances))).transpose() #not all cres will have distances, e.g. if not in same leaf\n",
    "    ngp = pairs.shape[0]\n",
    "    \n",
    "    coordinates = np.zeros((ngp,2))\n",
    "    projection_distances = np.zeros((ngp,1))\n",
    "    for i in range(ngp):\n",
    "        coordinates[i,0] = np.linalg.norm(centroids[pairs[i][0]] - centroids[pairs[i][1]])**2\n",
    "        coordinates[i,1] = cre_distances[pairs[i][0]][pairs[i][1]]\n",
    "        projection_distances[i] = np.linalg.norm(projections[pairs[i][0]] - projections[pairs[i][1]])**2\n",
    "    coordinates_normed = coordinates / np.linalg.norm(coordinates, axis = 0)#**2\n",
    "\n",
    "    surface = NadarayaWatson(kernel='rbf',  gamma  = gamma)\n",
    "    randos = random.sample(list(range(ngp)), math.floor(ngp * fraction))\n",
    "    surface.fit(coordinates_normed[randos], projection_distances[randos])\n",
    "    surface.coordinates_normed = coordinates_normed\n",
    "    surface.norms = np.linalg.norm(coordinates, axis = 0)\n",
    "    return(surface)\n",
    "\n",
    "def get_loss_surface(projections, centroids, cres, sids,fraction, gamma = 100000):\n",
    "    means_cast = get_means(projections, cres, sids)\n",
    "    cre_distances = get_cre_distances(projections, means_cast, sids, cres)\n",
    "    surface = get_surface_from_distances(projections,centroids,cre_distances, fraction, gamma)\n",
    "    return(surface)\n",
    "\n",
    "def plot_loss_surface(surface):\n",
    "    xs = np.linspace(0, surface.coordinates_normed[:,0].max(), 100)\n",
    "    ys = np.linspace(0, surface.coordinates_normed[:,1].max(), 100)\n",
    "    preds = np.empty((100,100))\n",
    "    for x in range(100):\n",
    "        for y in range(100):\n",
    "            preds[x,y] = surface.predict(np.asarray([[xs[x], ys[y]]]))#asdf.predict(np.asarray([[xs[x], ys[y]]])) #qqq.predict(np.asarray([[xs[x], ys[y]]]))\n",
    "\n",
    "    mxy = np.asarray(np.meshgrid(xs,ys)).transpose()\n",
    "\n",
    "    #%matplotlib inline\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "\n",
    "    ax.scatter(mxy[:,:,0], \n",
    "               mxy[:,:,1], \n",
    "               preds, s= .1)\n",
    "\n",
    "    #ax.set_axis_off()\n",
    "    ax.set_xlabel('Centroid distance', fontsize=20, rotation=150)\n",
    "    ax.set_ylabel('Cre-distance', fontsize=20, rotation=150)\n",
    "    ax.set_zlabel('Projection distance predicted', fontsize=20, rotation=150)  \n",
    "    \n",
    "def get_embedding_cv(surface, dists, cre_distances_cv):\n",
    "    \n",
    "    ntrain = dists.shape[0]\n",
    "    norms = surface.norms\n",
    "    leaf_pairs = np.asarray(np.where(~np.isnan(cre_distances_cv))).transpose()\n",
    "    nlp = leaf_pairs.shape[0]\n",
    "    \n",
    "    losses = np.zeros((ntrain, ntrain))\n",
    "    for i in range(nlp):\n",
    "        d_ij = dists[leaf_pairs[i][0]][leaf_pairs[i][1]] / norms[0]\n",
    "        p_ij = cre_distances_cv[leaf_pairs[i][0]][leaf_pairs[i][1]] / norms[1]\n",
    "        losses[leaf_pairs[i][0]][leaf_pairs[i][1]] = surface.predict(np.asarray([[d_ij, p_ij]]))\n",
    "      \n",
    "    losses[np.where(losses == 0)] = np.nan\n",
    "    \n",
    "    return(losses)\n",
    "\n",
    "def get_region_prediction(cache, structure_data,  structures, prediction_region, cre, gamma, surface = None, cre_model = False):\n",
    "    \n",
    "    leaf_experiments = np.where(structures == prediction_region)[0]\n",
    "    nexp = len(leaf_experiments)\n",
    "    centroids = structure_data.centroids[leaf_experiments]\n",
    "    cres = structure_data.crelines[leaf_experiments]\n",
    "    projections = structure_data.reg_proj_norm[leaf_experiments]\n",
    "    mask = Mask.from_cache(cache,structure_ids=[prediction_region],hemisphere_id=2)\n",
    "\n",
    "    if surface != None and cre_model != True:\n",
    "        means = get_means(projections,cres, np.repeat(prediction_region,nexp))\n",
    "        \n",
    "        losses = get_embedding(surface, pairwise_distances(centroids, mask.coordinates)**2, cres, cre, means[prediction_region])\n",
    "        predictions = get_nw_predictions(projections, losses, gamma)\n",
    "        output = np.mean(predictions, axis = 0)\n",
    "        \n",
    "    if surface != None and cre_model == True:\n",
    "        means = get_means(projections, cres, np.repeat(prediction_region,nexp))\n",
    "        #print(means)\n",
    "        if np.isin(cre, np.asarray(list(means[prediction_region].keys()))):\n",
    "            losses = get_embedding(surface, pairwise_distances(centroids, mask.coordinates)**2, cres, cre, means[prediction_region])\n",
    "            predictions = get_nw_predictions(projections, losses, gamma)\n",
    "            output = np.mean(predictions, axis = 0)\n",
    "        else:\n",
    "            output = np.zeros(projections.shape[1])\n",
    "            output[:] = np.nan\n",
    "            \n",
    "    if surface == None and cre_model != True:\n",
    "        means = get_means(projections, cres, [prediction_region])\n",
    "        predictions = means[cres]\n",
    "        output = np.mean(predictions, axis = 0)\n",
    "        \n",
    "    if surface == None and cre_model == True:\n",
    "        predictions = get_nw_predictions(projections, pairwise_distances(centroids, mask.coordinates)**2, gamma)\n",
    "        output = np.mean(predictions, axis = 0)\n",
    "        \n",
    "    \n",
    "    return(output)\n",
    "\n",
    "def get_connectivity_matrices(connectivity_data, cres, structure_dict, source_ordering, target_ordering, structure_major_dictionary,gamma_dict = None):\n",
    "    \n",
    "    nsource = len(source_ordering)\n",
    "    #n#target = len(target_ordering)\n",
    "    ncre = len(cres)\n",
    "    \n",
    "    ipsi_target_regions = connectivity_data.ipsi_target_regions\n",
    "    contra_target_regions = connectivity_data.contra_target_regions     \n",
    "                                                      \n",
    "    ipsi_indices= np.asarray([])\n",
    "    contra_indices = np.asarray([])\n",
    "    for iy in target_ordering: \n",
    "        #print(iy)\n",
    "        ipsi_indices = np.concatenate([ipsi_indices, np.where(ipsi_target_regions==iy)[0]] )\n",
    "        contra_indices = np.concatenate([contra_indices, np.where(contra_target_regions==iy)[0]] )\n",
    "    ipsi_indices = np.asarray(ipsi_indices, dtype = int)   \n",
    "    contra_indices = np.asarray(contra_indices, dtype = int)   \n",
    "    \n",
    "    #reorder = np.concatenate([ipsi_indices, contra_indices])  \n",
    "    reorder = np.concatenate([ipsi_indices, len(ipsi_indices) + contra_indices])  \n",
    "    \n",
    "    ntarget = len(reorder)\n",
    "      \n",
    "    connectivity = np.zeros((ncre, nsource, ntarget))\n",
    "    connectivity[:] = np.nan\n",
    "    #structure_major_dictionary = connectivity_data.structure_major_dictionary\n",
    "    for c in range(ncre):\n",
    "        for i in range(nsource):\n",
    "            print(i)\n",
    "            sid = structure_major_dictionary[source_ordering[i]]\n",
    "            gamma = gamma_dict[sid]\n",
    "            connectivity[c,i] = get_region_prediction(cache, connectivity_data.structure_datas[sid], structures = structure_dict[sid], prediction_region=source_ordering[i], cre = cres[c], gamma = gamma, surface = connectivity_data.structure_datas[sid].loss_surface, cre_model = True)\n",
    "                                                      \n",
    "    connectivity = connectivity[:,:,reorder]                                                  \n",
    "                                                      \n",
    "    return(connectivity)\n",
    "\n",
    "def get_summarystructure_major_dictionary(connectivity_data):\n",
    "    \n",
    "    structure_major_dictionary = {}\n",
    "    keys = np.asarray(list(connectivity_data.structure_datas))\n",
    "    for sid in keys:\n",
    "        strs_sid = np.unique(connectivity_data.structure_datas[sid].summary_structures)\n",
    "        nstrs = len(strs_sid)\n",
    "        for s in range(nstrs):\n",
    "            structure_major_dictionary[strs_sid[s]] = sid\n",
    "        \n",
    "    return(structure_major_dictionary)\n",
    "\n",
    "def get_major_summarystructure_dictionary(connectivity_data):\n",
    "    \n",
    "    major_structure_dictionary = {}\n",
    "    keys = np.asarray(list(connectivity_data.structure_datas))\n",
    "    for sid in keys:\n",
    "        strs_sid = np.unique(connectivity_data.structure_datas[sid].summary_structures)\n",
    "        major_structure_dictionary[sid] = np.asarray(strs_sid, dtype = int)\n",
    "        \n",
    "    return(major_structure_dictionary)\n",
    "\n",
    "def get_leaf_major_dictionary(connectivity_data):\n",
    "    \n",
    "    structure_major_dictionary = {}\n",
    "    keys = np.asarray(list(connectivity_data.structure_datas))\n",
    "    for sid in keys:\n",
    "        strs_sid = np.unique(connectivity_data.structure_datas[sid].leafs)\n",
    "        nstrs = len(strs_sid)\n",
    "        for s in range(nstrs):\n",
    "            structure_major_dictionary[strs_sid[s]] = sid\n",
    "        \n",
    "    return(structure_major_dictionary)\n",
    "\n",
    "def get_major_leaf_dictionary(connectivity_data):\n",
    "    \n",
    "    major_structure_dictionary = {}\n",
    "    keys = np.asarray(list(connectivity_data.structure_datas))\n",
    "    for sid in keys:\n",
    "        strs_sid = np.unique(connectivity_data.structure_datas[sid].leafs)\n",
    "        major_structure_dictionary[sid] = np.asarray(strs_sid, dtype = int)\n",
    "        \n",
    "    return(major_structure_dictionary)\n",
    "\n",
    "def get_embedding(surface, dists, cres = None, cre = None, means = None):\n",
    "    \n",
    "    ntrain = dists.shape[0]\n",
    "    neval = dists.shape[1]\n",
    "    norms = surface.norms\n",
    "    #cnorm = surface.cnorm\n",
    "    \n",
    "    cre_deezy = np.zeros((ntrain))\n",
    "    \n",
    "    for i in range(ntrain):\n",
    "        cre_deezy[i] = np.linalg.norm(means[cres[i]] - means[cre])\n",
    "    \n",
    "    losses = np.zeros((ntrain, neval))\n",
    "    for i in range(ntrain):\n",
    "        for j in range(neval):\n",
    "            d_ij = dists[i,j] / norms[0]\n",
    "            p_i = cre_deezy[i] / norms[1]\n",
    "            losses[i,j] = surface.predict(np.asarray([[d_ij, p_i]]))\n",
    "            \n",
    "    return(losses)\n",
    "def get_crelines(connectivity_data, creline):\n",
    "\n",
    "    major_structure_ids = np.asarray(list(connectivity_data.structure_datas.keys()))\n",
    "    for sid in major_structure_ids:\n",
    "        connectivity_data.structure_datas[sid].crelines = creline[sid]\n",
    "        \n",
    "    return(connectivity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "703\n",
      "1089\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-889eb2ab72e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconnectivity_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_connectivity_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajor_structure_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiments_exclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_injection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconnectivity_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_injection_hemisphere_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconnectivity_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconnectivity_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_centroids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a353ed5bb337>\u001b[0m in \u001b[0;36mget_connectivity_data\u001b[0;34m(cache, structure_ids, experiments_exclude, remove_injection)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0meid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msid_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m             \u001b[0meid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ccf_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#ExperimentData(eid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0meid_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_mask_tolerance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;31m#ccf_data = get_ccf_data(cache, eid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a353ed5bb337>\u001b[0m in \u001b[0;36mget_ccf_data\u001b[0;34m(cache, experiment_id)\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0meid_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_quality_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0meid_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minjection_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_injection_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m     \u001b[0meid_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minjection_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_injection_fraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m     \u001b[0meid_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_projection_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/allensdk/core/mouse_connectivity_cache.py\u001b[0m in \u001b[0;36mget_injection_fraction\u001b[0;34m(self, experiment_id, file_name)\u001b[0m\n\u001b[1;32m    239\u001b[0m             file_name, experiment_id, self.resolution, strategy='lazy')\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnrrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_data_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/nrrd/reader.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(filename, custom_field_map, index_order)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_field_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/nrrd/reader.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(header, fh, filename, index_order)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Decompress and append data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mdecompressed_data\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdecompobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressed_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;31m# Update start index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "connectivity_data = get_connectivity_data(cache, major_structure_ids, experiments_exclude, remove_injection = False)\n",
    "connectivity_data.get_injection_hemisphere_ids()\n",
    "connectivity_data.align()\n",
    "connectivity_data.get_centroids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_data = get_data_matrices(connectivity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "creline = get_cre_status(data_info, connectivity_data)\n",
    "with open('data/info/leafs.pickle', 'rb') as handle:\n",
    "    leafs = pickle.load(handle)\n",
    "    \n",
    "connectivity_data = get_summarystructures(connectivity_data, data_info)\n",
    "connectivity_data = get_leafs(connectivity_data, data_info,leafs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_data = get_crelines(connectivity_data, creline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#major division segregation is legacy code but convenient for fast cross validation in major division model\n",
    "#experiments_minor_structures = get_summarystructure_dictionary(connectivity_data, data_info)\n",
    "#get leaves in ontological order.  Where leafs don't exist, uses summary structure\n",
    "ontological_order_leaves = get_leaves_ontologicalorder(connectivity_data, ontological_order)\n",
    "#Key isn't affected by which experiment we choose. This allows default masking to be inherited from the AllenSDK.\n",
    "sid0 = list(connectivity_data.structure_datas.keys())[0]\n",
    "eid0 = list(connectivity_data.structure_datas[sid0].experiment_datas.keys())[0]\n",
    "#Identify keys denoting which voxels correspond to which structure in the ipsi and contra targets.\n",
    "#contra_targetkey = msvd.experiments[list(msvd.experiments.keys())[0]].projection_mask.get_key(structure_ids=ontological_order_leaves, hemisphere_id=1)\n",
    "#ipsi_targetkey = msvd.experiments[list(msvd.experiments.keys())[0]].projection_mask.get_key(structure_ids=ontological_order_leaves, hemisphere_id=2)\n",
    "contra_targetkey = connectivity_data.structure_datas[sid0].projection_mask.get_key(structure_ids=ontological_order, hemisphere_id=1)\n",
    "ipsi_targetkey = connectivity_data.structure_datas[sid0].projection_mask.get_key(structure_ids=ontological_order, hemisphere_id=2)\n",
    "#get average intensities of projection structures given ipsi and contra keys\n",
    "#source_key = ontological_order #only relevant here when injection needs to be unionized, but currently a required argument\n",
    "ipsi_target_regions, ipsi_target_counts = nonzero_unique(ipsi_targetkey, return_counts=True)\n",
    "contra_target_regions, contra_target_counts = nonzero_unique(contra_targetkey, return_counts=True)\n",
    "\n",
    "target_order = lambda x: np.array(ontological_order)[np.isin(ontological_order, x)]\n",
    "permutation = lambda x: np.argsort(np.argsort(target_order(x)))\n",
    "targ_ids = np.concatenate([ipsi_target_regions[permutation(ipsi_target_regions)],\n",
    "                           contra_target_regions[permutation(contra_target_regions)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_data = get_regionalized_normalized_data(connectivity_data, cache, ontological_order, ipsi_targetkey, contra_targetkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_leaf = {}\n",
    "indices_wtinleaf = {}\n",
    "indices_wtleaf = {}\n",
    "indices_summary = {}\n",
    "indices_summaryinleaf = {}\n",
    "indices_major = {}\n",
    "indices_majorinleaf = {}\n",
    "indices_leaf2ormore = {}\n",
    "indices_wtinleaf2ormore = {}\n",
    "indices_leaf_reduced = {}\n",
    "indices_wtinleaf_reduced = {}\n",
    "indices_wt_leaf2ormore = {}\n",
    "indices_leaf2ormore_wt = {}\n",
    "indices_wt1ormore_leaf2ormore = {}\n",
    "indices_leaf2ormore_wt1ormore = {}\n",
    "indices_creleaf = {}\n",
    "indices_creleaf2ormore = {}\n",
    "creleafs = {}\n",
    "creleafs_merged = {}\n",
    "for sid in major_structure_ids:\n",
    "    \n",
    "    #wt_leaf on leaf\n",
    "\n",
    "    #get the indices of experiments sharing leafs (nmodels is number of leafs)\n",
    "    indices_leaf[sid] = get_indices(leafs[sid]) #eval_indices\n",
    "    #indices_creleaf = get_indices(leafs[sid])\n",
    "    \n",
    "    #get the indices of the wts in the leaf (nmodels is number of leafs)\n",
    "    indices_wtinleaf[sid] = get_indices2(creline[sid], np.asarray(['C57BL/6J']),leafs[sid]) #model_indices\n",
    "    \n",
    "    #get indices of experiments sharing summary structure x cre combination (nmodel is number of cre x leaf combinations)\n",
    "    #indices_wtleaf[sid] = get_indices(creleafs_merged[sid])\n",
    "    \n",
    "    #get indices of experiments sharing summary structure(nmodel is number of summary structures)\n",
    "    #indices_summary[sid] = get_indices(experiments_minor_structures[sid])\n",
    "    \n",
    "    #get indices of experiments sharing major structure(nmodel is number of summary structures)\n",
    "    #indices_major[sid] = np.ones((1,experiments_minor_structures[sid].shape[0]))\n",
    "    \n",
    "    #get indices of experiments sharing same major structure as a leaf (nmodel is number of leafs)\n",
    "    \n",
    "    #get indices of experiments in same summary structure as a leaf (nmodel is number of leafs)\n",
    "    #indices_summaryinleaf[sid] = get_indices_summaryinleaf(experiments_minor_structures[sid], leafs[sid])\n",
    "    \n",
    "    #evaluate models on leafs\n",
    "    indices_leaf2ormore[sid] = screen_index_matrices(indices_leaf[sid], indices_leaf[sid])\n",
    "    \n",
    "    indices_wt1ormore_leaf2ormore[sid], indices_leaf2ormore_wt1ormore[sid] = screen_index_matrices3(indices_wtinleaf[sid], indices_leaf2ormore[sid])\n",
    "    \n",
    "    indices_wtinleaf2ormore[sid] = screen_index_matrices(indices_wtinleaf[sid], indices_wtinleaf[sid])\n",
    "    indices_leaf_reduced[sid], indices_wtinleaf_reduced[sid]  = screen_index_matrices2( indices_leaf2ormore[sid],indices_wtinleaf[sid])\n",
    "\n",
    "    creleafs[sid] = np.asarray(np.vstack([leafs[sid], creline[sid]]), dtype = str).transpose()\n",
    "    creleafs_merged[sid] = [creleafs[sid][:,0][i]  + creleafs[sid][:,1][i] for i in range(creleafs[sid].shape[0])]\n",
    "\n",
    "    indices_creleaf[sid] = get_indices(np.asarray(creleafs_merged[sid]))\n",
    "    indices_creleaf2ormore[sid] = screen_index_matrices(indices_creleaf[sid], indices_creleaf[sid])\n",
    "    indices_majorinleaf[sid] = np.ones(indices_creleaf2ormore[sid].shape)#np.ones((len(np.unique(leafs[sid])),leafs[sid].shape[0]))#get_indices2(np.ones(len(leafs[sid])), np.asarray([1]),leafs[sid]) #model_indices\n",
    "\n",
    "    \n",
    "eval_indices_creleaf2ormore = get_eval_indices(indices_creleaf2ormore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(indices_creleaf2ormore[315], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 7, 36, 577)\n",
      "(6, 7, 7, 577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 7, 122, 577)\n",
      "(62, 7, 85, 577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "gammas = np.asarray([0.1,.5,1,2,10,20,40])\n",
    "for sid in major_structure_ids[:12]:\n",
    "    pds = pairwise_distances(connectivity_data.structure_datas[sid].centroids)**2\n",
    "    connectivity_data.structure_datas[sid].loocv_predictions_leaf_leaf2 = get_nwloocv_predictions_multimodel_merge_dists(connectivity_data.structure_datas[sid].reg_proj_norm, \n",
    "                                                                                       pds,#np.ones(losses.shape),#(losses - np.nanmin(losses))**2,#**6, \n",
    "                                                                                       gammas, \n",
    "                                                                                       indices_majorinleaf[sid], \n",
    "                                                                                       indices_creleaf2ormore[sid])                                  \n",
    "\n",
    "a= [list(range(7))]\n",
    "keys = np.asarray(list(itertools.product(*a)))\n",
    "\n",
    "reg_proj_norm= {}\n",
    "nwloocv_leaf_leaf2 = {}\n",
    "for sid in major_structure_ids[:12]:\n",
    "    reg_proj_norm[sid ] = connectivity_data.structure_datas[sid].reg_proj_norm\n",
    "    nwloocv_leaf_leaf2[sid] = connectivity_data.structure_datas[sid].loocv_predictions_leaf_leaf2\n",
    "\n",
    "eval_indices_creleaf2ormore = get_eval_indices(indices_creleaf2ormore)\n",
    "\n",
    "losses_leaf_leaf2 = get_loss(reg_proj_norm, nwloocv_leaf_leaf2,pred_ind = eval_indices_creleaf2ormore, true_ind = eval_indices_creleaf2ormore,keys = keys)\n",
    "best_gamma_leaf_leaf2 = get_best_hyperparameters(losses_leaf_leaf2,keys)\n",
    "meanloss_nw_leaf_leaf2distscreleaf = get_loss_best_hyp(losses_leaf_leaf2, best_gamma_leaf_leaf2)\n",
    "\n",
    "#meanloss_nw_leaf_leaf2wt\n",
    "#meanloss_nw_wtleaf_leaf2wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meanloss_nw_leaf_leaf2distscreleaf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4f4df9d78854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeanloss_nw_leaf_leaf2distscreleaf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'meanloss_nw_leaf_leaf2distscreleaf' is not defined"
     ]
    }
   ],
   "source": [
    "meanloss_nw_leaf_leaf2distscreleaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = np.asarray([0.1,.5,1,2,10,20,40])\n",
    "sid = 315\n",
    "pds = pairwise_distances(connectivity_data.structure_datas[sid].centroids)**2\n",
    "# connectivity_data.structure_datas[sid].loocv_predictions_leaf_leaf2 = get_nwloocv_predictions_multimodel_merge_dists(connectivity_data.structure_datas[sid].reg_proj_norm, \n",
    "#                                                                                    pds,#np.ones(losses.shape),#(losses - np.nanmin(losses))**2,#**6, \n",
    "#                                                                                    gammas, \n",
    "#                                                                                    indices_majorinleaf[sid], \n",
    "#                                                                                    indices_creleaf2ormore[sid])                                  \n",
    "model_index_matrix = indices_majorinleaf[sid]\n",
    "eval_index_matrix = indices_creleaf2ormore[sid]\n",
    "dists = pds\n",
    "gamma = gammas[0]\n",
    "projections = connectivity_data.structure_datas[sid].reg_proj_norm\n",
    "#get_nwloocv_predictions_singlemodel_dists(projections, dists, gammas[g], model_index_matrix[m], eval_index_matrix[m]) for g in range(ngammas)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_indices = eval_index_matrix[3]\n",
    "model_indices = model_index_matrix[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "    eval_index_val = np.where(eval_indices == 1)[0]\n",
    "    model_index_val = np.where(model_indices == 1)[0]\n",
    "    \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    \n",
    "    nmod_ind = len(model_index_val)\n",
    "    neval = len(eval_index_val)\n",
    "    #nexp = centroids.shape[0]\n",
    "    predictions = np.empty(projections.shape)\n",
    "\n",
    "    if len(model_index_val) > 0 and  len(eval_index_val) > 0:\n",
    "        #weights = np.exp(-dists[model_index_val][:, eval_index_val] / gamma)#np.exp(-dists[model_index_val] / gamma) #get_weights(centroids, gamma)\n",
    "        for i in range(neval):\n",
    "            print(i)\n",
    "            matchindex = np.where(model_index_val == eval_index_val[i])[0]\n",
    "            otherindices = np.setdiff1d(np.asarray(list(range(nmod_ind))), matchindex)         \n",
    "            #this order of operations is the fastest I found\n",
    "            dists_i = dists[model_index_val][:, eval_index_val[i]] - np.min(dists[model_index_val[otherindices]][:, eval_index_val[i]])\n",
    "            weights_i = np.exp(-dists_i * gamma)#weights[i,:] / np.nansum(weights[i,:][otherindices])\n",
    "            #print(np.nansum(weights[:,i][otherindices]))\n",
    "            weights_i[matchindex] = 0\n",
    "            weights_i = np.asarray(weights_i, dtype=np.float32)\n",
    "            weights_i = weights_i / np.sum(weights_i)\n",
    "            #weights_i[np.isnan(weights_i)] = 0.\n",
    "            pred = np.dot(weights_i, projections[model_index_val])\n",
    "            predictions[eval_index_val[i]] = pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nwloocv_predictions_multimodel_dists(projections, dists, gammas, model_index_matrix, eval_index_matrix):\n",
    "    \n",
    "\n",
    "    \n",
    "    ntargets = projections.shape[1]\n",
    "    nexp = projections.shape[0]\n",
    "    nmodels = model_index_matrix.shape[0]\n",
    "    ngammas = len(gammas)\n",
    "    \n",
    "    projections = np.asarray(projections, dtype=np.float32)\n",
    "    predictions = np.empty((nmodels, ngammas, nexp, ntargets))\n",
    "    \n",
    "    \n",
    "    for m in range(nmodels):\n",
    "        print('m', m, len(np.where(model_index_matrix[m] ==1)[0]))\n",
    "        predictions[m] = np.asarray([get_nwloocv_predictions_singlemodel_dists(projections, dists, gammas[g], model_index_matrix[m], eval_index_matrix[m]) for g in range(ngammas)])\n",
    "    \n",
    "    return(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m 0 1128\n",
      "m 1 1128\n",
      "m 2 1128\n",
      "m 3 1128\n",
      "m 4 1128\n",
      "m 5 1128\n",
      "m 6 1128\n",
      "m 7 1128\n",
      "m 8 1128\n",
      "m 9 1128\n",
      "m 10 1128\n",
      "m 11 1128\n",
      "m 12 1128\n",
      "m 13 1128\n",
      "m 14 1128\n",
      "m 15 1128\n",
      "m 16 1128\n",
      "m 17 1128\n",
      "m 18 1128\n",
      "m 19 1128\n",
      "m 20 1128\n",
      "m 21 1128\n",
      "m 22 1128\n",
      "m 23 1128\n",
      "m 24 1128\n",
      "m 25 1128\n",
      "m 26 1128\n",
      "m 27 1128\n",
      "m 28 1128\n",
      "m 29 1128\n",
      "m 30 1128\n",
      "m 31 1128\n",
      "m 32 1128\n",
      "m 33 1128\n",
      "m 34 1128\n",
      "m 35 1128\n",
      "m 36 1128\n",
      "m 37 1128\n",
      "m 38 1128\n",
      "m 39 1128\n",
      "m 40 1128\n",
      "m 41 1128\n",
      "m 42 1128\n",
      "m 43 1128\n",
      "m 44 1128\n",
      "m 45 1128\n",
      "m 46 1128\n",
      "m 47 1128\n",
      "m 48 1128\n",
      "m 49 1128\n",
      "m 50 1128\n",
      "m 51 1128\n",
      "m 52 1128\n",
      "m 53 1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m 54 1128\n",
      "m 55 1128\n",
      "m 56 1128\n",
      "m 57 1128\n",
      "m 58 1128\n",
      "m 59 1128\n",
      "m 60 1128\n",
      "m 61 1128\n",
      "m 62 1128\n",
      "m 63 1128\n",
      "m 64 1128\n",
      "m 65 1128\n",
      "m 66 1128\n",
      "m 67 1128\n",
      "m 68 1128\n",
      "m 69 1128\n",
      "m 70 1128\n",
      "m 71 1128\n",
      "m 72 1128\n",
      "m 73 1128\n",
      "m 74 1128\n",
      "m 75 1128\n",
      "m 76 1128\n",
      "m 77 1128\n",
      "m 78 1128\n",
      "m 79 1128\n",
      "m 80 1128\n",
      "m 81 1128\n",
      "m 82 1128\n",
      "m 83 1128\n",
      "m 84 1128\n",
      "m 85 1128\n",
      "m 86 1128\n",
      "m 87 1128\n",
      "m 88 1128\n",
      "m 89 1128\n",
      "m 90 1128\n",
      "m 91 1128\n",
      "m 92 1128\n",
      "m 93 1128\n",
      "m 94 1128\n",
      "m 95 1128\n",
      "m 96 1128\n",
      "m 97 1128\n",
      "m 98 1128\n",
      "m 99 1128\n",
      "m 100 1128\n",
      "m 101 1128\n",
      "m 102 1128\n",
      "m 103 1128\n",
      "m 104 1128\n",
      "m 105 1128\n",
      "m 106 1128\n",
      "m 107 1128\n",
      "m 108 1128\n",
      "m 109 1128\n",
      "m 110 1128\n",
      "m 111 1128\n",
      "m 112 1128\n",
      "m 113 1128\n",
      "m 114 1128\n",
      "m 115 1128\n",
      "m 116 1128\n",
      "m 117 1128\n",
      "m 118 1128\n",
      "m 119 1128\n",
      "m 120 1128\n",
      "m 121 1128\n",
      "m 122 1128\n",
      "m 123 1128\n",
      "m 124 1128\n",
      "m 125 1128\n",
      "m 126 1128\n",
      "m 127 1128\n",
      "m 128 1128\n",
      "m 129 1128\n",
      "m 130 1128\n",
      "m 131 1128\n",
      "m 132 1128\n",
      "m 133 1128\n",
      "m 134 1128\n",
      "m 135 1128\n",
      "m 136 1128\n",
      "m 137 1128\n",
      "m 138 1128\n",
      "m 139 1128\n",
      "m 140 1128\n",
      "m 141 1128\n",
      "m 142 1128\n",
      "m 143 1128\n",
      "m 144 1128\n",
      "m 145 1128\n",
      "m 146 1128\n",
      "m 147 1128\n",
      "m 148 1128\n",
      "m 149 1128\n",
      "m 150 1128\n",
      "m 151 1128\n",
      "m 152 1128\n",
      "m 153 1128\n",
      "m 154 1128\n",
      "m 155 1128\n",
      "m 156 1128\n",
      "m 157 1128\n",
      "m 158 1128\n",
      "m 159 1128\n",
      "m 160 1128\n",
      "m 161 1128\n",
      "m 162 1128\n",
      "m 163 1128\n",
      "m 164 1128\n",
      "m 165 1128\n",
      "m 166 1128\n",
      "m 167 1128\n",
      "m 168 1128\n",
      "m 169 1128\n",
      "m 170 1128\n",
      "m 171 1128\n",
      "m 172 1128\n",
      "m 173 1128\n",
      "m 174 1128\n",
      "m 175 1128\n",
      "m 176 1128\n",
      "m 177 1128\n",
      "m 178 1128\n",
      "m 179 1128\n",
      "m 180 1128\n",
      "m 181 1128\n",
      "m 182 1128\n",
      "m 183 1128\n",
      "m 184 1128\n",
      "m 185 1128\n",
      "m 186 1128\n",
      "m 187 1128\n",
      "m 188 1128\n",
      "m 189 1128\n",
      "m 190 1128\n",
      "m 191 1128\n",
      "m 192 1128\n",
      "m 193 1128\n",
      "m 194 1128\n",
      "m 195 1128\n",
      "m 196 1128\n",
      "m 197 1128\n",
      "m 198 1128\n",
      "m 199 1128\n",
      "m 200 1128\n",
      "m 201 1128\n",
      "m 202 1128\n",
      "m 203 1128\n",
      "m 204 1128\n",
      "m 205 1128\n",
      "m 206 1128\n",
      "m 207 1128\n",
      "m 208 1128\n",
      "m 209 1128\n",
      "m 210 1128\n",
      "m 211 1128\n",
      "m 212 1128\n",
      "m 213 1128\n",
      "m 214 1128\n",
      "m 215 1128\n",
      "m 216 1128\n",
      "m 217 1128\n",
      "m 218 1128\n",
      "m 219 1128\n",
      "m 220 1128\n",
      "m 221 1128\n",
      "m 222 1128\n",
      "m 223 1128\n",
      "m 224 1128\n",
      "m 225 1128\n",
      "m 226 1128\n",
      "m 227 1128\n",
      "m 228 1128\n",
      "m 229 1128\n",
      "m 230 1128\n",
      "m 231 1128\n",
      "m 232 1128\n",
      "m 233 1128\n",
      "m 234 1128\n",
      "m 235 1128\n",
      "m 236 1128\n",
      "m 237 1128\n",
      "m 238 1128\n",
      "m 239 1128\n",
      "m 240 1128\n",
      "m 241 1128\n",
      "m 242 1128\n",
      "m 243 1128\n",
      "m 244 1128\n",
      "m 245 1128\n",
      "m 246 1128\n",
      "m 247 1128\n",
      "m 248 1128\n",
      "m 249 1128\n",
      "m 250 1128\n",
      "m 251 1128\n",
      "m 252 1128\n",
      "m 253 1128\n",
      "m 254 1128\n",
      "m 255 1128\n",
      "m 256 1128\n",
      "m 257 1128\n",
      "m 258 1128\n",
      "m 259 1128\n",
      "m 260 1128\n",
      "m 261 1128\n",
      "m 262 1128\n",
      "m 263 1128\n",
      "m 264 1128\n",
      "m 265 1128\n",
      "m 266 1128\n",
      "m 267 1128\n",
      "m 268 1128\n",
      "m 269 1128\n",
      "m 270 1128\n",
      "m 271 1128\n",
      "m 272 1128\n",
      "m 273 1128\n",
      "m 274 1128\n",
      "m 275 1128\n",
      "m 276 1128\n",
      "m 277 1128\n",
      "m 278 1128\n",
      "m 279 1128\n",
      "m 280 1128\n",
      "m 281 1128\n",
      "m 282 1128\n",
      "m 283 1128\n",
      "m 284 1128\n",
      "m 285 1128\n",
      "m 286 1128\n",
      "m 287 1128\n",
      "m 288 1128\n",
      "m 289 1128\n",
      "m 290 1128\n",
      "m 291 1128\n",
      "m 292 1128\n",
      "m 293 1128\n",
      "m 294 1128\n",
      "m 295 1128\n",
      "m 296 1128\n",
      "m 297 1128\n",
      "m 298 1128\n",
      "m 299 1128\n",
      "m 300 1128\n",
      "m 301 1128\n",
      "m 302 1128\n",
      "m 303 1128\n",
      "m 304 1128\n",
      "m 305 1128\n",
      "m 306 1128\n",
      "m 307 1128\n",
      "m 308 1128\n",
      "m 309 1128\n",
      "m 310 1128\n",
      "m 311 1128\n",
      "m 312 1128\n",
      "m 313 1128\n",
      "m 314 1128\n",
      "m 315 1128\n",
      "m 316 1128\n",
      "m 317 1128\n",
      "m 318 1128\n",
      "m 319 1128\n",
      "m 320 1128\n",
      "m 321 1128\n",
      "m 322 1128\n",
      "m 323 1128\n",
      "m 324 1128\n",
      "m 325 1128\n",
      "m 326 1128\n",
      "m 327 1128\n",
      "m 328 1128\n",
      "m 329 1128\n",
      "m 330 1128\n",
      "m 331 1128\n",
      "m 332 1128\n",
      "m 333 1128\n",
      "m 334 1128\n",
      "m 335 1128\n",
      "m 336 1128\n",
      "m 337 1128\n",
      "m 338 1128\n",
      "m 339 1128\n",
      "m 340 1128\n",
      "m 341 1128\n",
      "m 342 1128\n",
      "m 343 1128\n",
      "m 344 1128\n",
      "m 345 1128\n",
      "m 346 1128\n",
      "m 347 1128\n",
      "m 348 1128\n",
      "m 349 1128\n",
      "m 350 1128\n",
      "m 351 1128\n",
      "m 352 1128\n",
      "m 353 1128\n",
      "m 354 1128\n",
      "m 355 1128\n",
      "m 356 1128\n",
      "m 357 1128\n",
      "m 358 1128\n",
      "m 359 1128\n",
      "m 360 1128\n",
      "m 361 1128\n",
      "m 362 1128\n",
      "m 363 1128\n",
      "m 364 1128\n",
      "m 365 1128\n",
      "m 366 1128\n",
      "m 367 1128\n",
      "m 368 1128\n",
      "m 369 1128\n",
      "m 370 1128\n",
      "m 371 1128\n",
      "m 372 1128\n",
      "m 373 1128\n",
      "m 374 1128\n",
      "m 375 1128\n",
      "m 376 1128\n",
      "m 377 1128\n",
      "m 378 1128\n",
      "m 379 1128\n",
      "m 380 1128\n",
      "m 381 1128\n",
      "m 382 1128\n",
      "m 383 1128\n",
      "m 384 1128\n",
      "m 385 1128\n",
      "m 386 1128\n",
      "m 387 1128\n",
      "m 388 1128\n",
      "m 389 1128\n",
      "m 390 1128\n",
      "m 391 1128\n",
      "m 392 1128\n",
      "m 393 1128\n",
      "m 394 1128\n",
      "m 395 1128\n",
      "m 396 1128\n",
      "m 397 1128\n",
      "m 398 1128\n",
      "m 399 1128\n",
      "m 400 1128\n",
      "m 401 1128\n",
      "m 402 1128\n",
      "m 403 1128\n",
      "m 404 1128\n",
      "m 405 1128\n",
      "m 406 1128\n",
      "m 407 1128\n",
      "m 408 1128\n",
      "m 409 1128\n",
      "m 410 1128\n",
      "m 411 1128\n",
      "m 412 1128\n",
      "m 413 1128\n",
      "m 414 1128\n",
      "m 415 1128\n",
      "m 416 1128\n",
      "m 417 1128\n",
      "m 418 1128\n",
      "m 419 1128\n",
      "m 420 1128\n",
      "m 421 1128\n",
      "m 422 1128\n",
      "m 423 1128\n",
      "m 424 1128\n",
      "m 425 1128\n",
      "m 426 1128\n",
      "m 427 1128\n",
      "m 428 1128\n",
      "m 429 1128\n",
      "m 430 1128\n",
      "m 431 1128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3318\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3319\u001b[0;31m                     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3320\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-857c1fddf3e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                                                        \u001b[0mindices_majorinleaf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                                                        indices_creleaf2ormore[sid])                                  \n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-656a497dbdff>\u001b[0m in \u001b[0;36mget_nwloocv_predictions_multimodel_merge_dists\u001b[0;34m(projections, dists, gammas, model_index_matrix, eval_index_matrix)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m     \u001b[0mpredictions_unmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nwloocv_predictions_multimodel_dists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_index_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_index_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_unmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-3f015a4e5771>\u001b[0m in \u001b[0;36mget_nwloocv_predictions_multimodel_dists\u001b[0;34m(projections, dists, gammas, model_index_matrix, eval_index_matrix)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_index_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_nwloocv_predictions_singlemodel_dists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_index_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_index_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngammas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-3f015a4e5771>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_index_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_nwloocv_predictions_singlemodel_dists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_index_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_index_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngammas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-656a497dbdff>\u001b[0m in \u001b[0;36mget_nwloocv_predictions_singlemodel_dists\u001b[0;34m(projections, dists, gamma, model_indices, eval_indices)\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;31m#this order of operations is the fastest I found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m             \u001b[0mdists_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_index_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_index_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_index_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0motherindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_index_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0mweights_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdists_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#weights[i,:] / np.nansum(weights[i,:][otherindices])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3335\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3336\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   1995\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m     def showtraceback(self, exc_tuple=None, filename=None, tb_offset=None,\n\u001b[0m\u001b[1;32m   1998\u001b[0m                       exception_only=False, running_compiled_code=False):\n\u001b[1;32m   1999\u001b[0m         \"\"\"Display the exception that just occurred.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gammas = np.asarray([0.1,.5,1,2,10,20,40])\n",
    "for sid in major_structure_ids[4:5]:\n",
    "    pds = pairwise_distances(connectivity_data.structure_datas[sid].centroids)**2\n",
    "    connectivity_data.structure_datas[sid].loocv_predictions_leaf_leaf2 = get_nwloocv_predictions_multimodel_merge_dists(connectivity_data.structure_datas[sid].reg_proj_norm, \n",
    "                                                                                       pds,#np.ones(losses.shape),#(losses - np.nanmin(losses))**2,#**6, \n",
    "                                                                                       gammas, \n",
    "                                                                                       indices_majorinleaf[sid], \n",
    "                                                                                       indices_creleaf2ormore[sid])                                  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620, 1128)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_creleaf2ormore[sid].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanloss_nw_leaf_leaf2distscreleaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 7, 36, 577)\n",
      "(6, 7, 7, 577)\n",
      "(82, 7, 122, 577)\n",
      "(62, 7, 85, 577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620, 7, 1128, 577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 7, 68, 577)\n",
      "(42, 7, 46, 577)\n",
      "(24, 7, 35, 577)\n",
      "(29, 7, 33, 577)\n",
      "(24, 7, 30, 577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n",
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 7, 78, 577)\n",
      "(67, 7, 83, 577)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/allen_010719_5/lib/python3.7/site-packages/ipykernel_launcher.py:1273: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "gammas = np.asarray([0.1,.5,1,2,10,20,40])\n",
    "for sid in major_structure_ids:\n",
    "    pds = pairwise_distances(connectivity_data.structure_datas[sid].centroids)**2\n",
    "    connectivity_data.structure_datas[sid].loocv_predictions_leaf_leaf2 = get_nwloocv_predictions_multimodel_merge_dists(connectivity_data.structure_datas[sid].reg_proj_norm, \n",
    "                                                                                       pds,#np.ones(losses.shape),#(losses - np.nanmin(losses))**2,#**6, \n",
    "                                                                                       gammas, \n",
    "                                                                                       indices_creleaf2ormore[sid], \n",
    "                                                                                       indices_creleaf2ormore[sid])                                  \n",
    "\n",
    "a= [list(range(7))]\n",
    "keys = np.asarray(list(itertools.product(*a)))\n",
    "\n",
    "reg_proj_norm= {}\n",
    "nwloocv_leaf_leaf2 = {}\n",
    "for sid in major_structure_ids:\n",
    "    reg_proj_norm[sid ] = connectivity_data.structure_datas[sid].reg_proj_norm\n",
    "    nwloocv_leaf_leaf2[sid] = connectivity_data.structure_datas[sid].loocv_predictions_leaf_leaf2\n",
    "\n",
    "eval_indices_creleaf2ormore = get_eval_indices(indices_creleaf2ormore)\n",
    "\n",
    "losses_leaf_leaf2 = get_loss(reg_proj_norm, nwloocv_leaf_leaf2,pred_ind = eval_indices_creleaf2ormore, true_ind = eval_indices_creleaf2ormore,keys = keys)\n",
    "best_gamma_leaf_leaf2 = get_best_hyperparameters(losses_leaf_leaf2,keys)\n",
    "meanloss_nw_leaf_leaf2distscreleaf = get_loss_best_hyp(losses_leaf_leaf2, best_gamma_leaf_leaf2)\n",
    "\n",
    "#meanloss_nw_leaf_leaf2wt\n",
    "#meanloss_nw_wtleaf_leaf2wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04951634, 0.4966726 , 0.10827024, 0.25932422, 0.15073582,\n",
       "       0.11942033, 0.23823622, 0.0798464 , 0.2385843 , 0.19568441,\n",
       "       0.06782042, 0.57384959])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanloss_nw_leaf_leaf2distscreleaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen_010719_5",
   "language": "python",
   "name": "allen_010719_5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
